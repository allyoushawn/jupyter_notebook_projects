{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "587b627a-c885-47a9-991a-7dc506d42199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CovariateEmbedding(nn.Module):\n",
    "    def __init__(self, num_covariates, embedding_dim):\n",
    "        super(CovariateEmbedding, self).__init__()\n",
    "        self.embedding_layers = nn.ModuleList([\n",
    "            nn.Linear(1, embedding_dim) for _ in range(num_covariates)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, num_covariates)\n",
    "        Returns: Tensor of shape (batch_size, num_covariates, embedding_dim)\n",
    "        \"\"\"\n",
    "        embedded = [layer(x[:, i:i+1]) for i, layer in enumerate(self.embedding_layers)]\n",
    "        return torch.stack(embedded, dim=1)  # Shape: (batch, num_covariates, embedding_dim)\n",
    "\n",
    "\n",
    "class TreatmentEmbedding(nn.Module):\n",
    "    def __init__(self, num_treatments, embedding_dim):\n",
    "        super(TreatmentEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_treatments, embedding_dim)\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t: Tensor of shape (batch_size,)\n",
    "        Returns: Tensor of shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.embedding(t)\n",
    "\n",
    "\n",
    "class TransformerCovariateEncoder(nn.Module):\n",
    "    def __init__(self, num_covariates, embedding_dim, num_heads=4, num_layers=2):\n",
    "        super(TransformerCovariateEncoder, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, num_covariates, embedding_dim)\n",
    "        Returns: Tensor of shape (batch_size, num_covariates, embedding_dim)\n",
    "        \"\"\"\n",
    "        x = x.permute(1, 0, 2)  # Transformers expect (seq_len, batch, dim)\n",
    "        x = self.transformer(x)\n",
    "        return x.permute(1, 0, 2)  # Convert back to (batch_size, num_covariates, embedding_dim)\n",
    "\n",
    "\n",
    "class TreatmentCovariateCrossAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads=4, num_layers=1):\n",
    "        super(TreatmentCovariateCrossAttention, self).__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=num_heads)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, covariate_embeddings, treatment_embeddings):\n",
    "        \"\"\"\n",
    "        covariate_embeddings: (batch_size, num_covariates, embedding_dim) -> Acts as \"memory\" (key & value)\n",
    "        treatment_embeddings: (batch_size, embedding_dim) -> Acts as \"query\"\n",
    "        \n",
    "        Returns: (batch_size, num_covariates, embedding_dim) - Updated covariate representation\n",
    "        \"\"\"\n",
    "        # Expand treatment embeddings to match covariates\n",
    "        treatment_embeddings = treatment_embeddings.unsqueeze(1)  # Shape (batch_size, 1, embedding_dim)\n",
    "\n",
    "        # TransformerDecoder requires (seq_len, batch, dim) format\n",
    "        memory = covariate_embeddings.permute(1, 0, 2)  # (num_covariates, batch, embedding_dim)\n",
    "        query = treatment_embeddings.permute(1, 0, 2)  # (1, batch, embedding_dim)\n",
    "\n",
    "        # Apply Transformer Decoder (cross-attention)\n",
    "        updated_covariates = self.transformer_decoder(query, memory)  # Shape: (1, batch, embedding_dim)\n",
    "\n",
    "        return updated_covariates.permute(1, 0, 2)  # Convert back to (batch_size, 1, embedding_dim)\n",
    "\n",
    "\n",
    "class OutcomePrediction(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(OutcomePrediction, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, 1)  # Final regression layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, 1, embedding_dim)\n",
    "        Returns: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten before prediction\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TransTEE(nn.Module):\n",
    "    def __init__(self, num_covariates, embedding_dim, num_treatments):\n",
    "        super(TransTEE, self).__init__()\n",
    "        self.covariate_embedding = CovariateEmbedding(num_covariates, embedding_dim)\n",
    "        self.treatment_embedding = TreatmentEmbedding(num_treatments, embedding_dim)\n",
    "        self.covariate_encoder = TransformerCovariateEncoder(num_covariates, embedding_dim)\n",
    "        self.cross_attention = TreatmentCovariateCrossAttention(embedding_dim)\n",
    "        self.outcome_predictor = OutcomePrediction(embedding_dim)\n",
    "\n",
    "        # Treatment prediction head (Propensity Score)\n",
    "        self.propensity_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: Covariates (batch_size, num_covariates)\n",
    "        t: Treatments (batch_size,)\n",
    "\n",
    "        Returns: Estimated outcome (batch_size, 1)\n",
    "        \"\"\"\n",
    "        x = self.covariate_embedding(x)  # Encode covariates\n",
    "        e_x = self.propensity_head(torch.mean(x, dim=1))  # Propensity scores\n",
    "\n",
    "        \n",
    "        t = self.treatment_embedding(t)  # Encode treatment\n",
    "        x = self.covariate_encoder(x)  # Self-attention on covariates\n",
    "        x = self.cross_attention(x, t)  # Treatment-covariate interactions\n",
    "        y_pred = self.outcome_predictor(x)  # Final outcome prediction\n",
    "\n",
    "        \n",
    "        return y_pred, e_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "596e082b-9f8e-40f8-a020-7091ebfe7e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  blood_pressure  cholesterol  treatment  outcome\n",
      "0   55             140          200          1      120\n",
      "1   40             130          180          2      125\n",
      "2   60             150          220          3      145\n",
      "Covariates shape: torch.Size([3, 3])\n",
      "Treatment shape: torch.Size([3])\n",
      "Outcome shape: torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'age': [55, 40, 60],\n",
    "    'blood_pressure': [140, 130, 150],\n",
    "    'cholesterol': [200, 180, 220],\n",
    "    'treatment': [1, 2, 3],  # Treatment as categorical variable\n",
    "    'outcome': [120, 125, 145]  # Observed outcome (only needed for training)\n",
    "})\n",
    "\n",
    "print(data)\n",
    "\n",
    "import torch\n",
    "\n",
    "def dataframe_to_tensors(df):\n",
    "    \"\"\"\n",
    "    Convert a Pandas DataFrame into PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Input DataFrame with covariates, treatments, and optionally outcomes.\n",
    "\n",
    "    Returns:\n",
    "    covariates_tensor (torch.Tensor): Shape (batch_size, num_covariates)\n",
    "    treatment_tensor (torch.Tensor): Shape (batch_size,)\n",
    "    outcome_tensor (torch.Tensor or None): Shape (batch_size, 1) if available, else None\n",
    "    \"\"\"\n",
    "    # Convert continuous covariates to float tensor\n",
    "    covariates = torch.tensor(df.iloc[:, :-2].values, dtype=torch.float32)  # All except last 2 cols\n",
    "    # Convert treatment to integer tensor\n",
    "    treatment = torch.tensor(df['treatment'].values, dtype=torch.long)  # Long tensor for embedding lookup\n",
    "    # Convert outcome if available\n",
    "    outcome = torch.tensor(df['outcome'].values, dtype=torch.float32).unsqueeze(1) if 'outcome' in df else None\n",
    "    \n",
    "    return covariates, treatment, outcome\n",
    "\n",
    "# Convert DataFrame\n",
    "covariates_tensor, treatment_tensor, outcome_tensor = dataframe_to_tensors(data)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Covariates shape:\", covariates_tensor.shape)  # Expected: (batch_size, num_covariates)\n",
    "print(\"Treatment shape:\", treatment_tensor.shape)  # Expected: (batch_size,)\n",
    "print(\"Outcome shape:\", outcome_tensor.shape)  # Expected: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d0a0c5f0-2b0f-43b7-9f93-9f9a8a560bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted outcome: tensor([[ 0.2193],\n",
      "        [ 0.2970],\n",
      "        [-0.4642]], grad_fn=<AddmmBackward0>)\n",
      "Predicted e_x: tensor([[0.0504],\n",
      "        [0.0319],\n",
      "        [0.0689]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "num_covariates = 3  # Age, BP, Cholesterol\n",
    "embedding_dim = 8   # Embedding dimension for both covariates & treatments\n",
    "num_treatments = 5  # Assume 5 possible treatments\n",
    "\n",
    "# Initialize the model\n",
    "model = TransTEE(num_covariates, embedding_dim, num_treatments)\n",
    "\n",
    "# Perform forward pass (inference)\n",
    "predicted_outcome, e_x = model(covariates_tensor, treatment_tensor)\n",
    "\n",
    "print(\"Predicted outcome:\", predicted_outcome)\n",
    "print(\"Predicted e_x:\", e_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1bc6fc0d-2e9c-4638-b2da-ba3a7df4e66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 17047.5039\n",
      "Epoch 10, Loss: 16614.7246\n",
      "Epoch 20, Loss: 16596.6367\n",
      "Epoch 30, Loss: 16568.8438\n",
      "Epoch 40, Loss: 16508.1270\n",
      "Epoch 50, Loss: 16507.9863\n",
      "Epoch 60, Loss: 16463.8730\n",
      "Epoch 70, Loss: 16421.8418\n",
      "Epoch 80, Loss: 16422.6426\n",
      "Epoch 90, Loss: 16358.5273\n"
     ]
    }
   ],
   "source": [
    "# Define loss function (Mean Squared Error for regression)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (one epoch for example)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred, e_x = model(covariates_tensor, treatment_tensor)  # Forward pass\n",
    "    loss = loss_function(y_pred, outcome_tensor)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "572f28fa-b0f8-4d3d-be91-b5aa7aeff4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted outcome: (tensor([[2.5095],\n",
      "        [2.8087],\n",
      "        [2.6630]], grad_fn=<AddmmBackward0>), tensor([[0.1285],\n",
      "        [0.0761],\n",
      "        [0.1836]], grad_fn=<SigmoidBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# Perform forward pass (inference)\n",
    "predicted_outcome = model(covariates_tensor, treatment_tensor)\n",
    "\n",
    "print(\"Predicted outcome:\", predicted_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "98830dff-3f1d-4cc8-8052-865ca126cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([800, 5]), y_train shape: torch.Size([800, 1]), w_train shape: torch.Size([800, 1])\n",
      "tau_train shape: torch.Size([800])\n"
     ]
    }
   ],
   "source": [
    "# Use Dragon Net input data frame\n",
    "from causalml.dataset import synthetic_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load synthetic dataset using updated API\n",
    "y, X, w, tau, b, e = synthetic_data(mode=1, n=1000, p=5, sigma=1.0, adj=0.0)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test, w_train, w_test, tau_train, tau_test = train_test_split(\n",
    "    X, y, w, tau, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train, y_test = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1), torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "w_train, w_test = torch.tensor(w_train, dtype=torch.float32).unsqueeze(1), torch.tensor(w_test, dtype=torch.float32).unsqueeze(1)\n",
    "tau_train, tau_test = torch.tensor(tau_train, dtype=torch.float32), torch.tensor(tau_test, dtype=torch.float32)\n",
    "\n",
    "# Print dataset shapes to verify\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, w_train shape: {w_train.shape}\")\n",
    "print(f\"tau_train shape: {tau_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "88891558-45e0-4877-abc2-5f404ba59581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted outcome: tensor([[-0.9378],\n",
      "        [-0.5109],\n",
      "        [-0.4597],\n",
      "        [-0.2885],\n",
      "        [-0.5436],\n",
      "        [-0.7282],\n",
      "        [-0.8049],\n",
      "        [-0.4678],\n",
      "        [-0.5155],\n",
      "        [-0.2668],\n",
      "        [-0.9839],\n",
      "        [-1.0729],\n",
      "        [-0.9636],\n",
      "        [-0.5570],\n",
      "        [-1.1488],\n",
      "        [-0.8911],\n",
      "        [-0.6083],\n",
      "        [-0.5857],\n",
      "        [-1.0442],\n",
      "        [-0.6843],\n",
      "        [-0.9571],\n",
      "        [-1.1847],\n",
      "        [-0.6073],\n",
      "        [-0.8753],\n",
      "        [-0.8945],\n",
      "        [-0.6331],\n",
      "        [-1.0048],\n",
      "        [-0.6052],\n",
      "        [-1.0634],\n",
      "        [-1.0284],\n",
      "        [-0.8800],\n",
      "        [-0.6870],\n",
      "        [-0.9811],\n",
      "        [-0.4885],\n",
      "        [-0.7844],\n",
      "        [-1.0704],\n",
      "        [-0.3534],\n",
      "        [-1.0672],\n",
      "        [-0.9561],\n",
      "        [-1.1046],\n",
      "        [-0.5518],\n",
      "        [-0.7643],\n",
      "        [-0.1884],\n",
      "        [-0.5126],\n",
      "        [-0.4640],\n",
      "        [-1.0495],\n",
      "        [-0.6077],\n",
      "        [-0.9586],\n",
      "        [-1.2012],\n",
      "        [-0.9535],\n",
      "        [-0.5179],\n",
      "        [-0.3524],\n",
      "        [-0.5072],\n",
      "        [-0.6275],\n",
      "        [-0.4523],\n",
      "        [-1.0574],\n",
      "        [-0.8733],\n",
      "        [-0.6345],\n",
      "        [-1.0625],\n",
      "        [-0.5557],\n",
      "        [-0.4857],\n",
      "        [-0.5584],\n",
      "        [-1.1018],\n",
      "        [-1.2030],\n",
      "        [-0.5403],\n",
      "        [-1.0120],\n",
      "        [-0.6479],\n",
      "        [-0.5672],\n",
      "        [-0.9012],\n",
      "        [-1.0790],\n",
      "        [-0.4107],\n",
      "        [-0.9950],\n",
      "        [-1.1130],\n",
      "        [-0.6417],\n",
      "        [-1.0965],\n",
      "        [-0.4753],\n",
      "        [-0.7654],\n",
      "        [-0.9656],\n",
      "        [-1.0578],\n",
      "        [-1.0115],\n",
      "        [-0.5383],\n",
      "        [-0.5720],\n",
      "        [-0.8542],\n",
      "        [-0.4273],\n",
      "        [-0.9456],\n",
      "        [-0.8912],\n",
      "        [-0.6794],\n",
      "        [-0.5196],\n",
      "        [-0.3814],\n",
      "        [-0.7270],\n",
      "        [-0.2837],\n",
      "        [-1.2676],\n",
      "        [-0.5777],\n",
      "        [-0.9559],\n",
      "        [-0.2215],\n",
      "        [-0.8549],\n",
      "        [-0.5769],\n",
      "        [-0.9368],\n",
      "        [-0.4712],\n",
      "        [-0.8936],\n",
      "        [-0.7610],\n",
      "        [-1.0612],\n",
      "        [-0.3413],\n",
      "        [-1.0313],\n",
      "        [-1.1118],\n",
      "        [-1.1073],\n",
      "        [-0.7621],\n",
      "        [-0.4163],\n",
      "        [-0.7391],\n",
      "        [-0.3981],\n",
      "        [-0.6803],\n",
      "        [-1.0447],\n",
      "        [-0.9399],\n",
      "        [-1.1167],\n",
      "        [-0.5464],\n",
      "        [-0.5821],\n",
      "        [-0.5207],\n",
      "        [-0.8561],\n",
      "        [-0.9556],\n",
      "        [-0.4580],\n",
      "        [-1.1929],\n",
      "        [-0.6213],\n",
      "        [-0.6778],\n",
      "        [-0.5577],\n",
      "        [-1.1915],\n",
      "        [-0.5866],\n",
      "        [-0.8093],\n",
      "        [-0.6290],\n",
      "        [-0.2729],\n",
      "        [-0.9575],\n",
      "        [-0.8990],\n",
      "        [-0.9157],\n",
      "        [-1.0354],\n",
      "        [-1.0700],\n",
      "        [-0.4954],\n",
      "        [-0.6017],\n",
      "        [-1.0953],\n",
      "        [-1.2563],\n",
      "        [-0.5787],\n",
      "        [-0.8854],\n",
      "        [-0.4988],\n",
      "        [-0.7142],\n",
      "        [-0.4999],\n",
      "        [-0.6281],\n",
      "        [-0.4284],\n",
      "        [-1.0356],\n",
      "        [-0.0802],\n",
      "        [-0.9930],\n",
      "        [-0.3603],\n",
      "        [-0.8465],\n",
      "        [-1.1148],\n",
      "        [-0.5660],\n",
      "        [-0.9442],\n",
      "        [-0.2555],\n",
      "        [-0.4545],\n",
      "        [-0.9323],\n",
      "        [-0.8037],\n",
      "        [-0.8219],\n",
      "        [-0.4690],\n",
      "        [-0.4813],\n",
      "        [-1.0580],\n",
      "        [-1.1687],\n",
      "        [-0.9918],\n",
      "        [-0.8633],\n",
      "        [-0.3846],\n",
      "        [-0.4310],\n",
      "        [-0.9617],\n",
      "        [-0.6034],\n",
      "        [-0.9249],\n",
      "        [-0.9203],\n",
      "        [-0.9153],\n",
      "        [-1.0325],\n",
      "        [-0.7347],\n",
      "        [-0.4798],\n",
      "        [-1.0682],\n",
      "        [-0.8630],\n",
      "        [-1.0859],\n",
      "        [-0.7896],\n",
      "        [-0.5634],\n",
      "        [-1.0459],\n",
      "        [-0.6789],\n",
      "        [-0.5378],\n",
      "        [-0.9778],\n",
      "        [-0.9113],\n",
      "        [-0.9636],\n",
      "        [-0.9379],\n",
      "        [-0.3221],\n",
      "        [-0.5713],\n",
      "        [-0.7950],\n",
      "        [-0.6499],\n",
      "        [-0.8668],\n",
      "        [-0.5384],\n",
      "        [-0.7931],\n",
      "        [-0.9430],\n",
      "        [-0.8634],\n",
      "        [-0.8079],\n",
      "        [-0.4316],\n",
      "        [-0.6070],\n",
      "        [-0.8565],\n",
      "        [-0.9391],\n",
      "        [-0.8174],\n",
      "        [-1.0523],\n",
      "        [-0.8723],\n",
      "        [-0.6909],\n",
      "        [-0.4597],\n",
      "        [-0.7156],\n",
      "        [-0.6969],\n",
      "        [-0.9740],\n",
      "        [-0.9445],\n",
      "        [-0.6862],\n",
      "        [-0.3713],\n",
      "        [-0.7275],\n",
      "        [-0.9892],\n",
      "        [-0.4505],\n",
      "        [-0.6279],\n",
      "        [-0.4639],\n",
      "        [-0.9496],\n",
      "        [-0.7327],\n",
      "        [-0.5849],\n",
      "        [-0.9643],\n",
      "        [-0.3693],\n",
      "        [-0.9656],\n",
      "        [-0.3335],\n",
      "        [-1.0140],\n",
      "        [-0.7006],\n",
      "        [-0.7395],\n",
      "        [-0.6515],\n",
      "        [-0.8616],\n",
      "        [-0.9172],\n",
      "        [-0.8129],\n",
      "        [-0.5780],\n",
      "        [-0.9049],\n",
      "        [-0.9738],\n",
      "        [-1.0163],\n",
      "        [-0.3868],\n",
      "        [-0.5842],\n",
      "        [-1.0028],\n",
      "        [-0.4609],\n",
      "        [-1.0287],\n",
      "        [-1.0018],\n",
      "        [-1.1436],\n",
      "        [-0.4708],\n",
      "        [-0.5214],\n",
      "        [-0.5922],\n",
      "        [-0.7991],\n",
      "        [-1.0744],\n",
      "        [-0.4369],\n",
      "        [-1.0687],\n",
      "        [-0.4905],\n",
      "        [-0.4183],\n",
      "        [-0.7577],\n",
      "        [-0.7264],\n",
      "        [-0.4359],\n",
      "        [-0.3629],\n",
      "        [-0.6577],\n",
      "        [-0.6902],\n",
      "        [-0.4494],\n",
      "        [-0.6015],\n",
      "        [-0.8780],\n",
      "        [-0.9429],\n",
      "        [-0.8460],\n",
      "        [-1.0582],\n",
      "        [-0.9773],\n",
      "        [-0.3190],\n",
      "        [-0.6332],\n",
      "        [-0.5341],\n",
      "        [-0.7036],\n",
      "        [-0.4101],\n",
      "        [-0.7281],\n",
      "        [-0.5632],\n",
      "        [-1.1541],\n",
      "        [-1.0029],\n",
      "        [-0.7026],\n",
      "        [-0.8303],\n",
      "        [-1.1088],\n",
      "        [-0.3393],\n",
      "        [-0.5872],\n",
      "        [-0.4764],\n",
      "        [-0.8026],\n",
      "        [-0.4763],\n",
      "        [-0.4735],\n",
      "        [-1.0957],\n",
      "        [-0.6449],\n",
      "        [-0.9597],\n",
      "        [-0.5356],\n",
      "        [-0.9968],\n",
      "        [-0.8957],\n",
      "        [-0.3899],\n",
      "        [-0.4331],\n",
      "        [-0.3606],\n",
      "        [-0.4324],\n",
      "        [-0.8918],\n",
      "        [-0.9757],\n",
      "        [-1.1014],\n",
      "        [-1.0114],\n",
      "        [-0.5419],\n",
      "        [-0.8609],\n",
      "        [-0.4801],\n",
      "        [-0.8267],\n",
      "        [-0.6533],\n",
      "        [-0.9013],\n",
      "        [-0.7748],\n",
      "        [-0.4882],\n",
      "        [-1.1184],\n",
      "        [-1.1877],\n",
      "        [-0.2679],\n",
      "        [-0.5692],\n",
      "        [-0.5855],\n",
      "        [-0.2493],\n",
      "        [-1.0921],\n",
      "        [-0.5383],\n",
      "        [-0.9017],\n",
      "        [-0.3719],\n",
      "        [-0.9543],\n",
      "        [-0.8041],\n",
      "        [-0.4257],\n",
      "        [-1.1880],\n",
      "        [-0.5546],\n",
      "        [-0.8779],\n",
      "        [-1.0186],\n",
      "        [-0.2618],\n",
      "        [-1.0438],\n",
      "        [-0.5685],\n",
      "        [-0.4671],\n",
      "        [-0.3939],\n",
      "        [-0.9100],\n",
      "        [-1.0163],\n",
      "        [-0.9921],\n",
      "        [-0.9045],\n",
      "        [-1.1025],\n",
      "        [-0.4688],\n",
      "        [-0.8399],\n",
      "        [-0.6002],\n",
      "        [-0.5453],\n",
      "        [-0.8678],\n",
      "        [-0.9361],\n",
      "        [-0.9189],\n",
      "        [-0.7825],\n",
      "        [-0.5610],\n",
      "        [-0.2953],\n",
      "        [-0.5428],\n",
      "        [-0.4120],\n",
      "        [-0.9878],\n",
      "        [-1.0326],\n",
      "        [-0.7663],\n",
      "        [-0.4899],\n",
      "        [-0.9716],\n",
      "        [-0.9070],\n",
      "        [-0.5955],\n",
      "        [-0.9883],\n",
      "        [-0.6728],\n",
      "        [-0.5861],\n",
      "        [-0.9014],\n",
      "        [-0.4978],\n",
      "        [-1.0053],\n",
      "        [-0.5343],\n",
      "        [-0.8882],\n",
      "        [-0.9443],\n",
      "        [-0.9188],\n",
      "        [-0.9453],\n",
      "        [-0.9795],\n",
      "        [-0.9287],\n",
      "        [-0.6651],\n",
      "        [-1.2023],\n",
      "        [-1.0409],\n",
      "        [-0.6694],\n",
      "        [-0.9989],\n",
      "        [-0.3889],\n",
      "        [-0.8509],\n",
      "        [-0.9952],\n",
      "        [-0.6479],\n",
      "        [-0.9707],\n",
      "        [-0.7203],\n",
      "        [-1.0699],\n",
      "        [-0.6684],\n",
      "        [-0.4326],\n",
      "        [-0.9827],\n",
      "        [-1.0639],\n",
      "        [-0.9214],\n",
      "        [-0.7003],\n",
      "        [-0.4394],\n",
      "        [-0.9593],\n",
      "        [-1.0454],\n",
      "        [-1.0021],\n",
      "        [-0.9971],\n",
      "        [-0.4640],\n",
      "        [-0.3562],\n",
      "        [-0.9037],\n",
      "        [-1.0868],\n",
      "        [-0.6377],\n",
      "        [-0.8163],\n",
      "        [-0.4578],\n",
      "        [-0.6710],\n",
      "        [-0.7618],\n",
      "        [-0.4152],\n",
      "        [-0.5716],\n",
      "        [-1.0369],\n",
      "        [-1.0693],\n",
      "        [-0.6936],\n",
      "        [-0.4365],\n",
      "        [-1.0623],\n",
      "        [-1.0608],\n",
      "        [-0.6038],\n",
      "        [-0.9186],\n",
      "        [-0.6437],\n",
      "        [-0.3728],\n",
      "        [-0.7570],\n",
      "        [-0.9741],\n",
      "        [-0.9004],\n",
      "        [-1.0311],\n",
      "        [-0.7163],\n",
      "        [-0.9573],\n",
      "        [-0.8186],\n",
      "        [-0.8904],\n",
      "        [-0.8844],\n",
      "        [-0.4087],\n",
      "        [-0.6745],\n",
      "        [-0.5251],\n",
      "        [-0.8790],\n",
      "        [-0.9177],\n",
      "        [-0.3730],\n",
      "        [-1.0056],\n",
      "        [-0.7067],\n",
      "        [-0.4242],\n",
      "        [-0.4900],\n",
      "        [-0.9501],\n",
      "        [-0.6213],\n",
      "        [-0.6440],\n",
      "        [-1.0671],\n",
      "        [-0.9490],\n",
      "        [-0.9670],\n",
      "        [-0.3557],\n",
      "        [-0.5393],\n",
      "        [-0.6464],\n",
      "        [-0.7171],\n",
      "        [-0.3236],\n",
      "        [-0.8472],\n",
      "        [-0.5784],\n",
      "        [-0.9672],\n",
      "        [-0.4391],\n",
      "        [-0.9257],\n",
      "        [-0.2106],\n",
      "        [-1.0624],\n",
      "        [-0.9802],\n",
      "        [-0.5269],\n",
      "        [-0.8524],\n",
      "        [-0.7848],\n",
      "        [-1.1361],\n",
      "        [-0.5278],\n",
      "        [-0.8591],\n",
      "        [-0.2723],\n",
      "        [-0.7667],\n",
      "        [-0.5886],\n",
      "        [-0.6727],\n",
      "        [-0.7863],\n",
      "        [-0.8434],\n",
      "        [-1.0342],\n",
      "        [-1.0630],\n",
      "        [-0.9842],\n",
      "        [-0.7303],\n",
      "        [-0.6044],\n",
      "        [-1.1522],\n",
      "        [-0.3974],\n",
      "        [-0.9682],\n",
      "        [-0.5397],\n",
      "        [-0.8547],\n",
      "        [-1.0701],\n",
      "        [-0.9872],\n",
      "        [-0.4462],\n",
      "        [-1.1016],\n",
      "        [-0.5117],\n",
      "        [-0.9585],\n",
      "        [-0.6412],\n",
      "        [-0.9236],\n",
      "        [-0.4220],\n",
      "        [-0.5755],\n",
      "        [-0.7587],\n",
      "        [-0.3778],\n",
      "        [-0.5683],\n",
      "        [-0.5328],\n",
      "        [-0.8925],\n",
      "        [-0.3428],\n",
      "        [-0.6063],\n",
      "        [-0.8392],\n",
      "        [-0.6782],\n",
      "        [-0.5906],\n",
      "        [-0.4280],\n",
      "        [-0.4900],\n",
      "        [-1.0725],\n",
      "        [-1.0876],\n",
      "        [-0.4939],\n",
      "        [-0.7936],\n",
      "        [-0.8930],\n",
      "        [-0.7218],\n",
      "        [-0.9411],\n",
      "        [-1.1312],\n",
      "        [-0.9647],\n",
      "        [-1.1760],\n",
      "        [-0.6058],\n",
      "        [-0.0820],\n",
      "        [-1.0669],\n",
      "        [-0.9511],\n",
      "        [-0.3754],\n",
      "        [-0.9089],\n",
      "        [-0.9878],\n",
      "        [-0.5841],\n",
      "        [-1.1057],\n",
      "        [-0.9120],\n",
      "        [-0.9345],\n",
      "        [-0.7764],\n",
      "        [-0.5323],\n",
      "        [-0.4222],\n",
      "        [-1.0873],\n",
      "        [-1.0829],\n",
      "        [-0.9332],\n",
      "        [-0.9278],\n",
      "        [-1.2221],\n",
      "        [-0.7276],\n",
      "        [-0.5991],\n",
      "        [-0.6535],\n",
      "        [-0.4052],\n",
      "        [-0.3656],\n",
      "        [-0.9270],\n",
      "        [-0.6319],\n",
      "        [-0.4844],\n",
      "        [-0.6346],\n",
      "        [-0.8380],\n",
      "        [-0.7494],\n",
      "        [-0.5425],\n",
      "        [-0.7481],\n",
      "        [-0.8893],\n",
      "        [-0.9215],\n",
      "        [-0.6046],\n",
      "        [-0.9704],\n",
      "        [-1.0921],\n",
      "        [-0.5513],\n",
      "        [-0.8952],\n",
      "        [-0.6705],\n",
      "        [-0.3097],\n",
      "        [-1.0076],\n",
      "        [-0.8851],\n",
      "        [-0.9286],\n",
      "        [-0.4702],\n",
      "        [-0.9704],\n",
      "        [-0.9931],\n",
      "        [-0.5731],\n",
      "        [-0.4900],\n",
      "        [-0.6044],\n",
      "        [-0.3977],\n",
      "        [-0.8645],\n",
      "        [-0.5237],\n",
      "        [-1.1130],\n",
      "        [-1.0130],\n",
      "        [-0.5819],\n",
      "        [-1.1192],\n",
      "        [-0.8332],\n",
      "        [-0.8759],\n",
      "        [-0.2509],\n",
      "        [-0.8428],\n",
      "        [-1.0524],\n",
      "        [-1.0597],\n",
      "        [-0.5916],\n",
      "        [-1.0168],\n",
      "        [-1.0478],\n",
      "        [-0.5396],\n",
      "        [-0.8570],\n",
      "        [-0.6193],\n",
      "        [-0.7175],\n",
      "        [-1.0134],\n",
      "        [-0.4602],\n",
      "        [-0.7080],\n",
      "        [-0.5820],\n",
      "        [-0.8828],\n",
      "        [-0.8871],\n",
      "        [-0.7833],\n",
      "        [-0.6061],\n",
      "        [-0.6650],\n",
      "        [-1.1109],\n",
      "        [-0.4539],\n",
      "        [-0.9527],\n",
      "        [-0.4721],\n",
      "        [-0.9524],\n",
      "        [-0.8380],\n",
      "        [-0.7254],\n",
      "        [-0.5850],\n",
      "        [-1.0824],\n",
      "        [-0.3509],\n",
      "        [-0.7023],\n",
      "        [-0.5437],\n",
      "        [-0.6779],\n",
      "        [-1.1417],\n",
      "        [-0.3820],\n",
      "        [-0.9445],\n",
      "        [-1.0946],\n",
      "        [-1.1211],\n",
      "        [-0.4408],\n",
      "        [-0.7315],\n",
      "        [-1.0712],\n",
      "        [-0.9849],\n",
      "        [-0.4126],\n",
      "        [-1.0071],\n",
      "        [-0.3503],\n",
      "        [-1.0951],\n",
      "        [-0.8496],\n",
      "        [-0.6289],\n",
      "        [-0.4379],\n",
      "        [-1.2473],\n",
      "        [-0.5784],\n",
      "        [-0.9468],\n",
      "        [-0.5865],\n",
      "        [-0.5852],\n",
      "        [-0.5853],\n",
      "        [-1.0348],\n",
      "        [-1.0216],\n",
      "        [-1.0117],\n",
      "        [-0.2379],\n",
      "        [-0.9201],\n",
      "        [-0.9662],\n",
      "        [-0.7508],\n",
      "        [-0.8701],\n",
      "        [-0.9338],\n",
      "        [-1.0972],\n",
      "        [-1.1248],\n",
      "        [-0.6644],\n",
      "        [-1.0796],\n",
      "        [-0.9215],\n",
      "        [-0.3755],\n",
      "        [-1.1480],\n",
      "        [-0.4868],\n",
      "        [-1.0067],\n",
      "        [-0.7099],\n",
      "        [-0.2873],\n",
      "        [-0.3619],\n",
      "        [-1.0739],\n",
      "        [-0.8988],\n",
      "        [-0.9106],\n",
      "        [-0.5471],\n",
      "        [-0.4890],\n",
      "        [-0.5745],\n",
      "        [-0.7870],\n",
      "        [-1.0378],\n",
      "        [-0.3823],\n",
      "        [-1.4285],\n",
      "        [-0.6202],\n",
      "        [-0.9706],\n",
      "        [-0.8720],\n",
      "        [-1.0460],\n",
      "        [-0.7017],\n",
      "        [-0.5936],\n",
      "        [-0.7724],\n",
      "        [-1.0824],\n",
      "        [-0.9835],\n",
      "        [-1.1383],\n",
      "        [-1.0081],\n",
      "        [-1.0954],\n",
      "        [-0.8584],\n",
      "        [-1.0419],\n",
      "        [-1.1010],\n",
      "        [-0.9094],\n",
      "        [-0.4706],\n",
      "        [-1.0977],\n",
      "        [-0.9046],\n",
      "        [-0.9823],\n",
      "        [-0.8926],\n",
      "        [-0.5231],\n",
      "        [-0.3402],\n",
      "        [-0.9687],\n",
      "        [-0.7809],\n",
      "        [-0.8146],\n",
      "        [-0.9731],\n",
      "        [-0.3675],\n",
      "        [-0.8816],\n",
      "        [-1.2720],\n",
      "        [-0.3590],\n",
      "        [-1.0704],\n",
      "        [-0.8538],\n",
      "        [-0.8328],\n",
      "        [-1.0029],\n",
      "        [-0.9445],\n",
      "        [-0.9039],\n",
      "        [-0.9630],\n",
      "        [-0.8914],\n",
      "        [-0.6240],\n",
      "        [-0.8977],\n",
      "        [-0.5622],\n",
      "        [-0.6129],\n",
      "        [-1.2543],\n",
      "        [-0.8312],\n",
      "        [-0.8283],\n",
      "        [-0.1571],\n",
      "        [-0.4874],\n",
      "        [-0.8902],\n",
      "        [-0.9163],\n",
      "        [-1.1684],\n",
      "        [-0.9970],\n",
      "        [-0.9968],\n",
      "        [-0.8408],\n",
      "        [-0.7702],\n",
      "        [-0.8335],\n",
      "        [-0.9654],\n",
      "        [-0.3851],\n",
      "        [-0.9336],\n",
      "        [-0.8803],\n",
      "        [-0.7868],\n",
      "        [-0.5649],\n",
      "        [-0.5490],\n",
      "        [-0.6131],\n",
      "        [-0.9145],\n",
      "        [-0.9654],\n",
      "        [-1.1513],\n",
      "        [-0.9825],\n",
      "        [-0.5147],\n",
      "        [-0.5484],\n",
      "        [-0.7211],\n",
      "        [-0.6832],\n",
      "        [-0.7940],\n",
      "        [-0.9490],\n",
      "        [-1.0203],\n",
      "        [-0.9625],\n",
      "        [-0.3433],\n",
      "        [-0.4373],\n",
      "        [-0.7691],\n",
      "        [-0.9739],\n",
      "        [-0.6832],\n",
      "        [-1.0394],\n",
      "        [-1.1543],\n",
      "        [-0.4415],\n",
      "        [-0.5207],\n",
      "        [-0.2945],\n",
      "        [-0.6190],\n",
      "        [-0.3303],\n",
      "        [-1.2101],\n",
      "        [-0.5799],\n",
      "        [-0.7165],\n",
      "        [-0.6598],\n",
      "        [-1.1786],\n",
      "        [-0.8242],\n",
      "        [-0.6191],\n",
      "        [-0.9874],\n",
      "        [-0.7021],\n",
      "        [-1.0407],\n",
      "        [-1.0218],\n",
      "        [-1.1265],\n",
      "        [-1.2377],\n",
      "        [-0.8547],\n",
      "        [-0.3632],\n",
      "        [-0.5488],\n",
      "        [-0.6460],\n",
      "        [-0.5108],\n",
      "        [-0.6508],\n",
      "        [-0.9533],\n",
      "        [-0.4011],\n",
      "        [-0.5947],\n",
      "        [-0.8975],\n",
      "        [-1.0420],\n",
      "        [-0.5041],\n",
      "        [-0.2500],\n",
      "        [-0.6232],\n",
      "        [-0.5021],\n",
      "        [-0.5158],\n",
      "        [-0.9536],\n",
      "        [-0.8924],\n",
      "        [-0.4817],\n",
      "        [-0.4927],\n",
      "        [-0.8623],\n",
      "        [-1.0525],\n",
      "        [-0.9013],\n",
      "        [-0.7826],\n",
      "        [-0.8931],\n",
      "        [-0.9257],\n",
      "        [-0.6532],\n",
      "        [-0.5403],\n",
      "        [-1.0093],\n",
      "        [-0.1160],\n",
      "        [-0.9259],\n",
      "        [-0.8671],\n",
      "        [-0.6299],\n",
      "        [-0.5453],\n",
      "        [-1.0183],\n",
      "        [-0.4406],\n",
      "        [-1.0666],\n",
      "        [-0.5733],\n",
      "        [-0.4636],\n",
      "        [-1.1318],\n",
      "        [-1.0273],\n",
      "        [-0.4020],\n",
      "        [-0.6543],\n",
      "        [-0.4748],\n",
      "        [-0.3677],\n",
      "        [-0.2753],\n",
      "        [-0.3840],\n",
      "        [-0.6142],\n",
      "        [-0.5869],\n",
      "        [-0.9086],\n",
      "        [-0.8079],\n",
      "        [-0.7119],\n",
      "        [-0.8750],\n",
      "        [-0.5918],\n",
      "        [-1.0646],\n",
      "        [-0.7002]], grad_fn=<AddmmBackward0>)\n",
      "Predicted e_x: tensor([[0.4699],\n",
      "        [0.4667],\n",
      "        [0.4593],\n",
      "        [0.4352],\n",
      "        [0.4994],\n",
      "        [0.4824],\n",
      "        [0.4875],\n",
      "        [0.3732],\n",
      "        [0.4513],\n",
      "        [0.4530],\n",
      "        [0.4476],\n",
      "        [0.5170],\n",
      "        [0.4692],\n",
      "        [0.4043],\n",
      "        [0.4686],\n",
      "        [0.4031],\n",
      "        [0.4651],\n",
      "        [0.5103],\n",
      "        [0.4410],\n",
      "        [0.5206],\n",
      "        [0.4846],\n",
      "        [0.4356],\n",
      "        [0.4844],\n",
      "        [0.4191],\n",
      "        [0.4707],\n",
      "        [0.4922],\n",
      "        [0.4805],\n",
      "        [0.4398],\n",
      "        [0.4382],\n",
      "        [0.5571],\n",
      "        [0.4529],\n",
      "        [0.4884],\n",
      "        [0.4131],\n",
      "        [0.4280],\n",
      "        [0.4294],\n",
      "        [0.5076],\n",
      "        [0.4481],\n",
      "        [0.4755],\n",
      "        [0.4585],\n",
      "        [0.5034],\n",
      "        [0.4348],\n",
      "        [0.4554],\n",
      "        [0.4968],\n",
      "        [0.5415],\n",
      "        [0.5341],\n",
      "        [0.4317],\n",
      "        [0.5043],\n",
      "        [0.5156],\n",
      "        [0.4831],\n",
      "        [0.4579],\n",
      "        [0.4657],\n",
      "        [0.4328],\n",
      "        [0.4866],\n",
      "        [0.5093],\n",
      "        [0.4094],\n",
      "        [0.5102],\n",
      "        [0.4570],\n",
      "        [0.4268],\n",
      "        [0.4320],\n",
      "        [0.4372],\n",
      "        [0.4056],\n",
      "        [0.4649],\n",
      "        [0.4527],\n",
      "        [0.4525],\n",
      "        [0.3796],\n",
      "        [0.4990],\n",
      "        [0.4279],\n",
      "        [0.5235],\n",
      "        [0.4555],\n",
      "        [0.4677],\n",
      "        [0.4343],\n",
      "        [0.4222],\n",
      "        [0.4398],\n",
      "        [0.4286],\n",
      "        [0.4868],\n",
      "        [0.5090],\n",
      "        [0.5115],\n",
      "        [0.4602],\n",
      "        [0.4259],\n",
      "        [0.4913],\n",
      "        [0.5054],\n",
      "        [0.5542],\n",
      "        [0.4774],\n",
      "        [0.5001],\n",
      "        [0.5408],\n",
      "        [0.4444],\n",
      "        [0.4243],\n",
      "        [0.4737],\n",
      "        [0.5609],\n",
      "        [0.4453],\n",
      "        [0.5364],\n",
      "        [0.4182],\n",
      "        [0.4825],\n",
      "        [0.4073],\n",
      "        [0.4134],\n",
      "        [0.4545],\n",
      "        [0.4855],\n",
      "        [0.4868],\n",
      "        [0.4276],\n",
      "        [0.4800],\n",
      "        [0.4416],\n",
      "        [0.4133],\n",
      "        [0.4178],\n",
      "        [0.4570],\n",
      "        [0.4494],\n",
      "        [0.5142],\n",
      "        [0.3953],\n",
      "        [0.5308],\n",
      "        [0.4462],\n",
      "        [0.4772],\n",
      "        [0.3639],\n",
      "        [0.4769],\n",
      "        [0.4275],\n",
      "        [0.4979],\n",
      "        [0.5300],\n",
      "        [0.4834],\n",
      "        [0.4861],\n",
      "        [0.4490],\n",
      "        [0.4309],\n",
      "        [0.4565],\n",
      "        [0.5082],\n",
      "        [0.4796],\n",
      "        [0.4298],\n",
      "        [0.5150],\n",
      "        [0.4395],\n",
      "        [0.4988],\n",
      "        [0.4862],\n",
      "        [0.4510],\n",
      "        [0.3771],\n",
      "        [0.4751],\n",
      "        [0.4726],\n",
      "        [0.5236],\n",
      "        [0.4880],\n",
      "        [0.5036],\n",
      "        [0.4272],\n",
      "        [0.4408],\n",
      "        [0.4400],\n",
      "        [0.5413],\n",
      "        [0.5541],\n",
      "        [0.4478],\n",
      "        [0.4539],\n",
      "        [0.5346],\n",
      "        [0.4322],\n",
      "        [0.4399],\n",
      "        [0.4445],\n",
      "        [0.5217],\n",
      "        [0.4532],\n",
      "        [0.4381],\n",
      "        [0.4175],\n",
      "        [0.3953],\n",
      "        [0.4223],\n",
      "        [0.4743],\n",
      "        [0.3782],\n",
      "        [0.4281],\n",
      "        [0.4490],\n",
      "        [0.4852],\n",
      "        [0.4479],\n",
      "        [0.4455],\n",
      "        [0.4675],\n",
      "        [0.4501],\n",
      "        [0.4714],\n",
      "        [0.4681],\n",
      "        [0.4271],\n",
      "        [0.4858],\n",
      "        [0.5376],\n",
      "        [0.4880],\n",
      "        [0.4786],\n",
      "        [0.5391],\n",
      "        [0.4566],\n",
      "        [0.4208],\n",
      "        [0.5100],\n",
      "        [0.4102],\n",
      "        [0.5153],\n",
      "        [0.5014],\n",
      "        [0.4995],\n",
      "        [0.4587],\n",
      "        [0.4347],\n",
      "        [0.4555],\n",
      "        [0.4888],\n",
      "        [0.4390],\n",
      "        [0.4298],\n",
      "        [0.4812],\n",
      "        [0.4338],\n",
      "        [0.4698],\n",
      "        [0.5302],\n",
      "        [0.4064],\n",
      "        [0.3655],\n",
      "        [0.4090],\n",
      "        [0.4562],\n",
      "        [0.4545],\n",
      "        [0.3862],\n",
      "        [0.4890],\n",
      "        [0.3598],\n",
      "        [0.4338],\n",
      "        [0.4684],\n",
      "        [0.4934],\n",
      "        [0.5150],\n",
      "        [0.4925],\n",
      "        [0.4499],\n",
      "        [0.3969],\n",
      "        [0.4499],\n",
      "        [0.5082],\n",
      "        [0.4333],\n",
      "        [0.4113],\n",
      "        [0.4749],\n",
      "        [0.4579],\n",
      "        [0.4154],\n",
      "        [0.4927],\n",
      "        [0.4117],\n",
      "        [0.5630],\n",
      "        [0.4656],\n",
      "        [0.4854],\n",
      "        [0.4778],\n",
      "        [0.5348],\n",
      "        [0.4575],\n",
      "        [0.4241],\n",
      "        [0.4723],\n",
      "        [0.3815],\n",
      "        [0.4670],\n",
      "        [0.3573],\n",
      "        [0.4737],\n",
      "        [0.4615],\n",
      "        [0.3867],\n",
      "        [0.5306],\n",
      "        [0.4645],\n",
      "        [0.4823],\n",
      "        [0.4501],\n",
      "        [0.5673],\n",
      "        [0.4412],\n",
      "        [0.5403],\n",
      "        [0.4938],\n",
      "        [0.4787],\n",
      "        [0.4807],\n",
      "        [0.4578],\n",
      "        [0.5207],\n",
      "        [0.5141],\n",
      "        [0.4620],\n",
      "        [0.4163],\n",
      "        [0.4882],\n",
      "        [0.4813],\n",
      "        [0.5053],\n",
      "        [0.5112],\n",
      "        [0.5235],\n",
      "        [0.4343],\n",
      "        [0.4722],\n",
      "        [0.5185],\n",
      "        [0.5126],\n",
      "        [0.4472],\n",
      "        [0.5070],\n",
      "        [0.4936],\n",
      "        [0.5186],\n",
      "        [0.5786],\n",
      "        [0.4913],\n",
      "        [0.5222],\n",
      "        [0.4432],\n",
      "        [0.5221],\n",
      "        [0.4144],\n",
      "        [0.4139],\n",
      "        [0.4432],\n",
      "        [0.4567],\n",
      "        [0.4431],\n",
      "        [0.4514],\n",
      "        [0.4414],\n",
      "        [0.5100],\n",
      "        [0.3887],\n",
      "        [0.5019],\n",
      "        [0.4008],\n",
      "        [0.5118],\n",
      "        [0.5171],\n",
      "        [0.4991],\n",
      "        [0.4842],\n",
      "        [0.4838],\n",
      "        [0.4765],\n",
      "        [0.4775],\n",
      "        [0.5125],\n",
      "        [0.4919],\n",
      "        [0.4945],\n",
      "        [0.4991],\n",
      "        [0.5108],\n",
      "        [0.4697],\n",
      "        [0.5338],\n",
      "        [0.5229],\n",
      "        [0.4605],\n",
      "        [0.5497],\n",
      "        [0.5643],\n",
      "        [0.4632],\n",
      "        [0.4865],\n",
      "        [0.4181],\n",
      "        [0.4603],\n",
      "        [0.4390],\n",
      "        [0.3840],\n",
      "        [0.3923],\n",
      "        [0.4275],\n",
      "        [0.5077],\n",
      "        [0.4994],\n",
      "        [0.4369],\n",
      "        [0.4713],\n",
      "        [0.4531],\n",
      "        [0.4750],\n",
      "        [0.5426],\n",
      "        [0.5214],\n",
      "        [0.4242],\n",
      "        [0.4861],\n",
      "        [0.4897],\n",
      "        [0.4844],\n",
      "        [0.5258],\n",
      "        [0.4632],\n",
      "        [0.4509],\n",
      "        [0.4171],\n",
      "        [0.5064],\n",
      "        [0.4708],\n",
      "        [0.3510],\n",
      "        [0.4706],\n",
      "        [0.4952],\n",
      "        [0.4758],\n",
      "        [0.3787],\n",
      "        [0.4674],\n",
      "        [0.5110],\n",
      "        [0.4529],\n",
      "        [0.4662],\n",
      "        [0.5257],\n",
      "        [0.4997],\n",
      "        [0.4979],\n",
      "        [0.4624],\n",
      "        [0.4447],\n",
      "        [0.3894],\n",
      "        [0.4324],\n",
      "        [0.4428],\n",
      "        [0.4861],\n",
      "        [0.5287],\n",
      "        [0.4478],\n",
      "        [0.5164],\n",
      "        [0.4154],\n",
      "        [0.4723],\n",
      "        [0.4179],\n",
      "        [0.4634],\n",
      "        [0.4738],\n",
      "        [0.4637],\n",
      "        [0.4353],\n",
      "        [0.4422],\n",
      "        [0.4960],\n",
      "        [0.4522],\n",
      "        [0.4157],\n",
      "        [0.5071],\n",
      "        [0.4009],\n",
      "        [0.5648],\n",
      "        [0.4952],\n",
      "        [0.4344],\n",
      "        [0.4210],\n",
      "        [0.5093],\n",
      "        [0.4728],\n",
      "        [0.4872],\n",
      "        [0.4283],\n",
      "        [0.4688],\n",
      "        [0.5389],\n",
      "        [0.4675],\n",
      "        [0.3988],\n",
      "        [0.5122],\n",
      "        [0.4006],\n",
      "        [0.3857],\n",
      "        [0.4091],\n",
      "        [0.3558],\n",
      "        [0.4796],\n",
      "        [0.4646],\n",
      "        [0.4488],\n",
      "        [0.5190],\n",
      "        [0.4736],\n",
      "        [0.5291],\n",
      "        [0.4318],\n",
      "        [0.4462],\n",
      "        [0.4454],\n",
      "        [0.3837],\n",
      "        [0.5261],\n",
      "        [0.4123],\n",
      "        [0.3918],\n",
      "        [0.4737],\n",
      "        [0.4746],\n",
      "        [0.5055],\n",
      "        [0.5118],\n",
      "        [0.4582],\n",
      "        [0.4943],\n",
      "        [0.5010],\n",
      "        [0.4631],\n",
      "        [0.4164],\n",
      "        [0.3954],\n",
      "        [0.4648],\n",
      "        [0.4919],\n",
      "        [0.3593],\n",
      "        [0.4881],\n",
      "        [0.4697],\n",
      "        [0.4179],\n",
      "        [0.4454],\n",
      "        [0.4973],\n",
      "        [0.4365],\n",
      "        [0.5163],\n",
      "        [0.4363],\n",
      "        [0.4267],\n",
      "        [0.4966],\n",
      "        [0.4534],\n",
      "        [0.4414],\n",
      "        [0.4001],\n",
      "        [0.4852],\n",
      "        [0.5126],\n",
      "        [0.5026],\n",
      "        [0.4972],\n",
      "        [0.4828],\n",
      "        [0.4439],\n",
      "        [0.4545],\n",
      "        [0.4570],\n",
      "        [0.4304],\n",
      "        [0.4749],\n",
      "        [0.4558],\n",
      "        [0.3894],\n",
      "        [0.4764],\n",
      "        [0.4318],\n",
      "        [0.4854],\n",
      "        [0.4645],\n",
      "        [0.4254],\n",
      "        [0.4669],\n",
      "        [0.5248],\n",
      "        [0.4383],\n",
      "        [0.4710],\n",
      "        [0.4199],\n",
      "        [0.4199],\n",
      "        [0.5098],\n",
      "        [0.5120],\n",
      "        [0.5506],\n",
      "        [0.4850],\n",
      "        [0.4239],\n",
      "        [0.5350],\n",
      "        [0.4696],\n",
      "        [0.5021],\n",
      "        [0.4768],\n",
      "        [0.3623],\n",
      "        [0.4380],\n",
      "        [0.4069],\n",
      "        [0.3753],\n",
      "        [0.5194],\n",
      "        [0.4735],\n",
      "        [0.4307],\n",
      "        [0.4230],\n",
      "        [0.4144],\n",
      "        [0.5231],\n",
      "        [0.4731],\n",
      "        [0.4590],\n",
      "        [0.4586],\n",
      "        [0.4385],\n",
      "        [0.4285],\n",
      "        [0.4882],\n",
      "        [0.5014],\n",
      "        [0.3709],\n",
      "        [0.4490],\n",
      "        [0.5212],\n",
      "        [0.4243],\n",
      "        [0.3577],\n",
      "        [0.4527],\n",
      "        [0.4274],\n",
      "        [0.4691],\n",
      "        [0.5149],\n",
      "        [0.4532],\n",
      "        [0.4167],\n",
      "        [0.4537],\n",
      "        [0.4242],\n",
      "        [0.4540],\n",
      "        [0.4736],\n",
      "        [0.4969],\n",
      "        [0.4609],\n",
      "        [0.4476],\n",
      "        [0.3933],\n",
      "        [0.4992],\n",
      "        [0.4635],\n",
      "        [0.4328],\n",
      "        [0.4485],\n",
      "        [0.4531],\n",
      "        [0.4444],\n",
      "        [0.4553],\n",
      "        [0.4019],\n",
      "        [0.4238],\n",
      "        [0.3981],\n",
      "        [0.4220],\n",
      "        [0.4579],\n",
      "        [0.4856],\n",
      "        [0.5276],\n",
      "        [0.4482],\n",
      "        [0.4506],\n",
      "        [0.4433],\n",
      "        [0.3787],\n",
      "        [0.4389],\n",
      "        [0.4919],\n",
      "        [0.5594],\n",
      "        [0.4644],\n",
      "        [0.4409],\n",
      "        [0.4492],\n",
      "        [0.3970],\n",
      "        [0.4166],\n",
      "        [0.4255],\n",
      "        [0.5102],\n",
      "        [0.5593],\n",
      "        [0.4628],\n",
      "        [0.4619],\n",
      "        [0.4614],\n",
      "        [0.4435],\n",
      "        [0.4516],\n",
      "        [0.4971],\n",
      "        [0.4567],\n",
      "        [0.5320],\n",
      "        [0.4934],\n",
      "        [0.4474],\n",
      "        [0.4697],\n",
      "        [0.5127],\n",
      "        [0.4899],\n",
      "        [0.4338],\n",
      "        [0.4814],\n",
      "        [0.4013],\n",
      "        [0.5357],\n",
      "        [0.4996],\n",
      "        [0.4874],\n",
      "        [0.5209],\n",
      "        [0.4302],\n",
      "        [0.5178],\n",
      "        [0.3929],\n",
      "        [0.4258],\n",
      "        [0.5117],\n",
      "        [0.5159],\n",
      "        [0.4334],\n",
      "        [0.4867],\n",
      "        [0.4629],\n",
      "        [0.5098],\n",
      "        [0.4851],\n",
      "        [0.3976],\n",
      "        [0.5161],\n",
      "        [0.5147],\n",
      "        [0.5103],\n",
      "        [0.4922],\n",
      "        [0.4995],\n",
      "        [0.4212],\n",
      "        [0.4234],\n",
      "        [0.4817],\n",
      "        [0.4221],\n",
      "        [0.4734],\n",
      "        [0.5008],\n",
      "        [0.4001],\n",
      "        [0.5025],\n",
      "        [0.4562],\n",
      "        [0.4126],\n",
      "        [0.4455],\n",
      "        [0.4486],\n",
      "        [0.4511],\n",
      "        [0.5306],\n",
      "        [0.5253],\n",
      "        [0.4873],\n",
      "        [0.4619],\n",
      "        [0.3911],\n",
      "        [0.4508],\n",
      "        [0.4556],\n",
      "        [0.3812],\n",
      "        [0.5453],\n",
      "        [0.4568],\n",
      "        [0.4626],\n",
      "        [0.4581],\n",
      "        [0.4646],\n",
      "        [0.4643],\n",
      "        [0.5077],\n",
      "        [0.4591],\n",
      "        [0.4447],\n",
      "        [0.4600],\n",
      "        [0.4989],\n",
      "        [0.4858],\n",
      "        [0.4517],\n",
      "        [0.4235],\n",
      "        [0.5209],\n",
      "        [0.4510],\n",
      "        [0.4909],\n",
      "        [0.4381],\n",
      "        [0.4915],\n",
      "        [0.5282],\n",
      "        [0.4677],\n",
      "        [0.4496],\n",
      "        [0.4944],\n",
      "        [0.4445],\n",
      "        [0.4560],\n",
      "        [0.4535],\n",
      "        [0.4956],\n",
      "        [0.5207],\n",
      "        [0.4575],\n",
      "        [0.4510],\n",
      "        [0.4119],\n",
      "        [0.4260],\n",
      "        [0.5026],\n",
      "        [0.5170],\n",
      "        [0.5240],\n",
      "        [0.4951],\n",
      "        [0.5475],\n",
      "        [0.4531],\n",
      "        [0.4812],\n",
      "        [0.4356],\n",
      "        [0.3937],\n",
      "        [0.4430],\n",
      "        [0.3754],\n",
      "        [0.4300],\n",
      "        [0.4183],\n",
      "        [0.4539],\n",
      "        [0.4592],\n",
      "        [0.4512],\n",
      "        [0.5000],\n",
      "        [0.5118],\n",
      "        [0.4810],\n",
      "        [0.4984],\n",
      "        [0.4419],\n",
      "        [0.4383],\n",
      "        [0.4154],\n",
      "        [0.5385],\n",
      "        [0.3752],\n",
      "        [0.4225],\n",
      "        [0.5247],\n",
      "        [0.3901],\n",
      "        [0.4496],\n",
      "        [0.4519],\n",
      "        [0.3995],\n",
      "        [0.4271],\n",
      "        [0.3589],\n",
      "        [0.4865],\n",
      "        [0.4720],\n",
      "        [0.4945],\n",
      "        [0.4018],\n",
      "        [0.4289],\n",
      "        [0.4916],\n",
      "        [0.4619],\n",
      "        [0.3944],\n",
      "        [0.4588],\n",
      "        [0.4289],\n",
      "        [0.4562],\n",
      "        [0.4593],\n",
      "        [0.5018],\n",
      "        [0.4196],\n",
      "        [0.4533],\n",
      "        [0.4462],\n",
      "        [0.4654],\n",
      "        [0.4604],\n",
      "        [0.4602],\n",
      "        [0.4311],\n",
      "        [0.4702],\n",
      "        [0.4778],\n",
      "        [0.4399],\n",
      "        [0.5159],\n",
      "        [0.4250],\n",
      "        [0.4825],\n",
      "        [0.4854],\n",
      "        [0.4604],\n",
      "        [0.4310],\n",
      "        [0.4516],\n",
      "        [0.4898],\n",
      "        [0.5225],\n",
      "        [0.4488],\n",
      "        [0.4809],\n",
      "        [0.4567],\n",
      "        [0.4045],\n",
      "        [0.5540],\n",
      "        [0.3844],\n",
      "        [0.4510],\n",
      "        [0.3714],\n",
      "        [0.3712],\n",
      "        [0.5028],\n",
      "        [0.4701],\n",
      "        [0.4401],\n",
      "        [0.4218],\n",
      "        [0.4700],\n",
      "        [0.4406],\n",
      "        [0.4209],\n",
      "        [0.4053],\n",
      "        [0.4208],\n",
      "        [0.4467],\n",
      "        [0.5217],\n",
      "        [0.4041],\n",
      "        [0.4447],\n",
      "        [0.4146],\n",
      "        [0.5141],\n",
      "        [0.4562],\n",
      "        [0.3937],\n",
      "        [0.5146],\n",
      "        [0.3805],\n",
      "        [0.4207],\n",
      "        [0.4946],\n",
      "        [0.4894],\n",
      "        [0.3984],\n",
      "        [0.4001],\n",
      "        [0.4340],\n",
      "        [0.4898],\n",
      "        [0.5080],\n",
      "        [0.4244],\n",
      "        [0.5330],\n",
      "        [0.4279],\n",
      "        [0.4599],\n",
      "        [0.4650],\n",
      "        [0.5777],\n",
      "        [0.5534],\n",
      "        [0.4187],\n",
      "        [0.4774],\n",
      "        [0.4078],\n",
      "        [0.4355],\n",
      "        [0.3745],\n",
      "        [0.4486],\n",
      "        [0.4707],\n",
      "        [0.5022],\n",
      "        [0.5033],\n",
      "        [0.5173],\n",
      "        [0.5169],\n",
      "        [0.5281],\n",
      "        [0.4542],\n",
      "        [0.4331],\n",
      "        [0.5192],\n",
      "        [0.4628],\n",
      "        [0.4426],\n",
      "        [0.5233],\n",
      "        [0.4515],\n",
      "        [0.5433],\n",
      "        [0.5111],\n",
      "        [0.4031],\n",
      "        [0.4698],\n",
      "        [0.4717],\n",
      "        [0.5495],\n",
      "        [0.4778],\n",
      "        [0.5400],\n",
      "        [0.4432],\n",
      "        [0.4492],\n",
      "        [0.4448],\n",
      "        [0.4460],\n",
      "        [0.5203],\n",
      "        [0.4683],\n",
      "        [0.4685],\n",
      "        [0.4297],\n",
      "        [0.5061],\n",
      "        [0.4904],\n",
      "        [0.4320],\n",
      "        [0.4800],\n",
      "        [0.4997],\n",
      "        [0.4165],\n",
      "        [0.4860],\n",
      "        [0.3920],\n",
      "        [0.4983],\n",
      "        [0.4384],\n",
      "        [0.5096],\n",
      "        [0.4718],\n",
      "        [0.4711],\n",
      "        [0.4562],\n",
      "        [0.5254],\n",
      "        [0.4532],\n",
      "        [0.4503],\n",
      "        [0.4384],\n",
      "        [0.5437],\n",
      "        [0.3762],\n",
      "        [0.4962],\n",
      "        [0.4555],\n",
      "        [0.4198],\n",
      "        [0.4749],\n",
      "        [0.3960],\n",
      "        [0.5082],\n",
      "        [0.3911],\n",
      "        [0.4219],\n",
      "        [0.4930],\n",
      "        [0.5378],\n",
      "        [0.4458],\n",
      "        [0.4610],\n",
      "        [0.4512],\n",
      "        [0.4815],\n",
      "        [0.4491],\n",
      "        [0.4073],\n",
      "        [0.5167],\n",
      "        [0.4248],\n",
      "        [0.4536],\n",
      "        [0.4976],\n",
      "        [0.4949],\n",
      "        [0.4673],\n",
      "        [0.4433],\n",
      "        [0.5282],\n",
      "        [0.4933],\n",
      "        [0.4872],\n",
      "        [0.5456],\n",
      "        [0.4521],\n",
      "        [0.4233],\n",
      "        [0.4696],\n",
      "        [0.4188],\n",
      "        [0.4989],\n",
      "        [0.4235],\n",
      "        [0.4658],\n",
      "        [0.3887],\n",
      "        [0.5042],\n",
      "        [0.4918],\n",
      "        [0.4993],\n",
      "        [0.4689],\n",
      "        [0.4483],\n",
      "        [0.5365],\n",
      "        [0.3763],\n",
      "        [0.4638],\n",
      "        [0.5306],\n",
      "        [0.4000],\n",
      "        [0.5033],\n",
      "        [0.4260],\n",
      "        [0.4956],\n",
      "        [0.4646]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "num_covariates = 5  # Age, BP, Cholesterol\n",
    "embedding_dim = 20   # Embedding dimension for both covariates & treatments\n",
    "num_treatments = 2  # Assume 2 possible treatments\n",
    "\n",
    "# Initialize the model\n",
    "model = TransTEE(num_covariates, embedding_dim, num_treatments)\n",
    "\n",
    "# Perform forward pass (inference)\n",
    "predicted_outcome, e_x = model(X_train, w_train.int().squeeze())\n",
    "\n",
    "print(\"Predicted outcome:\", predicted_outcome)\n",
    "print(\"Predicted e_x:\", e_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c35db85f-6ee7-4f5a-bd71-18174244ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 6.6300\n",
      "Epoch 10, Loss: 1.2132\n",
      "Epoch 20, Loss: 1.2073\n",
      "Epoch 30, Loss: 1.1940\n",
      "Epoch 40, Loss: 1.1806\n",
      "Epoch 50, Loss: 1.1961\n",
      "Epoch 60, Loss: 1.1813\n",
      "Epoch 70, Loss: 1.1759\n",
      "Epoch 80, Loss: 1.1859\n",
      "Epoch 90, Loss: 1.1734\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred, e_x = model(X_train, w_train.int().squeeze())  # Forward pass\n",
    "    loss = loss_function(y_pred, y_train)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d267d4a3-0fec-434f-a294-30c12cd36011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error in Treatment Effect Estimation: 0.3034\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y0_pred_test, e_x = model(X_test, torch.zeros(X_test.shape[0]).int())\n",
    "    y1_pred_test, e_x = model(X_test, torch.ones(X_test.shape[0]).int())\n",
    "\n",
    "    # Estimate Individual Treatment Effects (ITE)\n",
    "    tau_hat = (y1_pred_test - y0_pred_test).squeeze().numpy()\n",
    "\n",
    "    # Compute Mean Absolute Error\n",
    "    mae = np.mean(np.abs(tau_hat - tau_test.numpy()))\n",
    "    print(f\"Mean Absolute Error in Treatment Effect Estimation: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e89f7-3ed2-4799-b5d2-9120af70aa68",
   "metadata": {},
   "source": [
    "# Training transTEE on larger size of dataset leverage torch.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a8f6c43b-31d0-4549-8469-a7333e80a866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Step 50: Loss = 1.0395\n",
      "Step 100: Loss = 0.9936\n",
      "Step 150: Loss = 1.0315\n",
      "Step 200: Loss = 1.1180\n",
      "Step 250: Loss = 1.1121\n",
      "Step 300: Loss = 1.0653\n",
      "Step 350: Loss = 0.9752\n",
      "Step 400: Loss = 0.9917\n",
      "Step 450: Loss = 0.9489\n",
      "Step 500: Loss = 1.0928\n",
      "Step 550: Loss = 1.1608\n",
      "Step 600: Loss = 1.1006\n",
      "Step 650: Loss = 1.0792\n",
      "Step 700: Loss = 0.9973\n",
      "Step 750: Loss = 0.9906\n",
      "\n",
      "Mean Absolute Error for transTEE: 0.1142\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from causalml.dataset import synthetic_data\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load synthetic dataset with larger sample size\n",
    "y, X, w, tau, b, e = synthetic_data(mode=1, n=50000, p=10, sigma=1.0, adj=0.0)  # Increased dataset size\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test, w_train, w_test, tau_train, tau_test = train_test_split(\n",
    "    X, y, w, tau, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train, y_test = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1), torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "w_train, w_test = torch.tensor(w_train, dtype=torch.float32).unsqueeze(1), torch.tensor(w_test, dtype=torch.float32).unsqueeze(1)\n",
    "tau_train, tau_test = torch.tensor(tau_train, dtype=torch.float32), torch.tensor(tau_test, dtype=torch.float32)\n",
    "\n",
    "# --- PyTorch Dataset & DataLoader ---\n",
    "class CausalDataset(Dataset):\n",
    "    def __init__(self, X, y, w):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.w[idx]\n",
    "\n",
    "# Create data loaders for mini-batch training\n",
    "batch_size = 512\n",
    "train_dataset = CausalDataset(X_train, y_train, w_train)\n",
    "test_dataset = CausalDataset(X_test, y_test, w_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train.shape[1]\n",
    "model = TransTEE(num_covariates, embedding_dim, num_treatments).to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# --- Train DragonNet with Mini-batch Training ---\n",
    "num_epochs = 10\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_X, batch_y, batch_w in train_loader:\n",
    "        batch_X, batch_y, batch_w = batch_X.to(device), batch_y.to(device), batch_w.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        y_pred, e_x = model(batch_X, batch_w.int().squeeze())  # Forward pass\n",
    "        loss = loss_function(y_pred, batch_y)  # Compute loss\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# --- Evaluate transTEE ---\n",
    "model.eval()\n",
    "tau_hat = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, _, _ in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        y0_pred_test, e_x = model(batch_X, torch.zeros(batch_X.shape[0]).int())\n",
    "        y1_pred_test, e_x = model(batch_X, torch.ones(batch_X.shape[0]).int())\n",
    "        tau_hat.extend((y1_pred_test - y0_pred_test).cpu().numpy())\n",
    "\n",
    "tau_hat = np.array(tau_hat).flatten()\n",
    "\n",
    "# --- Compare with True Treatment Effects ---\n",
    "mae = mean_absolute_error(tau_test, tau_hat)\n",
    "print(f\"\\nMean Absolute Error for transTEE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0e0b2-5e62-4cb0-a6ea-78b001c54c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
