{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "587b627a-c885-47a9-991a-7dc506d42199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CovariateEmbedding(nn.Module):\n",
    "    def __init__(self, num_covariates, embedding_dim):\n",
    "        super(CovariateEmbedding, self).__init__()\n",
    "        self.embedding_layers = nn.ModuleList([\n",
    "            nn.Linear(1, embedding_dim) for _ in range(num_covariates)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, num_covariates)\n",
    "        Returns: Tensor of shape (batch_size, num_covariates, embedding_dim)\n",
    "        \"\"\"\n",
    "        embedded = [layer(x[:, i:i+1]) for i, layer in enumerate(self.embedding_layers)]\n",
    "        return torch.stack(embedded, dim=1)  # Shape: (batch, num_covariates, embedding_dim)\n",
    "\n",
    "\n",
    "class TreatmentEmbedding(nn.Module):\n",
    "    def __init__(self, num_treatments, embedding_dim):\n",
    "        super(TreatmentEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_treatments, embedding_dim)\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t: Tensor of shape (batch_size,)\n",
    "        Returns: Tensor of shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.embedding(t)\n",
    "\n",
    "\n",
    "class TransformerCovariateEncoder(nn.Module):\n",
    "    def __init__(self, num_covariates, embedding_dim, num_heads=4, num_layers=2):\n",
    "        super(TransformerCovariateEncoder, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, num_covariates, embedding_dim)\n",
    "        Returns: Tensor of shape (batch_size, num_covariates, embedding_dim)\n",
    "        \"\"\"\n",
    "        x = x.permute(1, 0, 2)  # Transformers expect (seq_len, batch, dim)\n",
    "        x = self.transformer(x)\n",
    "        return x.permute(1, 0, 2)  # Convert back to (batch_size, num_covariates, embedding_dim)\n",
    "\n",
    "\n",
    "class TreatmentCovariateCrossAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads=4, num_layers=1):\n",
    "        super(TreatmentCovariateCrossAttention, self).__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=num_heads)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, covariate_embeddings, treatment_embeddings):\n",
    "        \"\"\"\n",
    "        covariate_embeddings: (batch_size, num_covariates, embedding_dim) -> Acts as \"memory\" (key & value)\n",
    "        treatment_embeddings: (batch_size, embedding_dim) -> Acts as \"query\"\n",
    "        \n",
    "        Returns: (batch_size, num_covariates, embedding_dim) - Updated covariate representation\n",
    "        \"\"\"\n",
    "        # Expand treatment embeddings to match covariates\n",
    "        treatment_embeddings = treatment_embeddings.unsqueeze(1)  # Shape (batch_size, 1, embedding_dim)\n",
    "\n",
    "        # TransformerDecoder requires (seq_len, batch, dim) format\n",
    "        memory = covariate_embeddings.permute(1, 0, 2)  # (num_covariates, batch, embedding_dim)\n",
    "        query = treatment_embeddings.permute(1, 0, 2)  # (1, batch, embedding_dim)\n",
    "\n",
    "        # Apply Transformer Decoder (cross-attention)\n",
    "        updated_covariates = self.transformer_decoder(query, memory)  # Shape: (1, batch, embedding_dim)\n",
    "\n",
    "        return updated_covariates.permute(1, 0, 2)  # Convert back to (batch_size, 1, embedding_dim)\n",
    "\n",
    "\n",
    "class OutcomePrediction(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(OutcomePrediction, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, 1)  # Final regression layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, 1, embedding_dim)\n",
    "        Returns: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten before prediction\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TransTEE(nn.Module):\n",
    "    def __init__(self, num_covariates, embedding_dim, num_treatments):\n",
    "        super(TransTEE, self).__init__()\n",
    "        self.covariate_embedding = CovariateEmbedding(num_covariates, embedding_dim)\n",
    "        self.treatment_embedding = TreatmentEmbedding(num_treatments, embedding_dim)\n",
    "        self.covariate_encoder = TransformerCovariateEncoder(num_covariates, embedding_dim)\n",
    "        self.cross_attention = TreatmentCovariateCrossAttention(embedding_dim)\n",
    "        self.outcome_predictor = OutcomePrediction(embedding_dim)\n",
    "\n",
    "        # Treatment prediction head (Propensity Score)\n",
    "        self.propensity_head = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Learnable epsilon (initialized small)\n",
    "        self.epsilon = nn.Parameter(torch.tensor(1e-6))\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: Covariates (batch_size, num_covariates)\n",
    "        t: Treatments (batch_size,)\n",
    "\n",
    "        Returns: Estimated outcome (batch_size, 1)\n",
    "        \"\"\"\n",
    "        x = self.covariate_embedding(x)  # Encode covariates\n",
    "        e_x = self.propensity_head(torch.mean(x, dim=1))  # Propensity scores\n",
    "\n",
    "        \n",
    "        t = self.treatment_embedding(t)  # Encode treatment\n",
    "        x = self.covariate_encoder(x)  # Self-attention on covariates\n",
    "        x = self.cross_attention(x, t)  # Treatment-covariate interactions\n",
    "        y_pred = self.outcome_predictor(x)  # Final outcome prediction\n",
    "\n",
    "        \n",
    "        return y_pred, e_x\n",
    "\n",
    "def make_regression_loss(y_0_pred, y_1_pred, y_true, t_true):\n",
    "    \n",
    "    loss0 = (1 - t_true) * torch.square(y_0_pred - y_true)\n",
    "    loss1 = t_true * torch.square(y_1_pred - y_true)\n",
    "    loss = loss0 + loss1\n",
    "    return torch.mean(loss)\n",
    "\n",
    "def make_binary_classification_loss(t_pred, t_true):\n",
    "    return nn.BCELoss()(t_pred, t_true)\n",
    "\n",
    "def make_targeted_regularization_loss(e_x, y0_pred, y1_pred, Y, T, epsilon):\n",
    "    \"\"\" Computes the doubly robust loss \"\"\"\n",
    "    \n",
    "    # Compute predicted outcome based on treatment\n",
    "    y_pred = T * y1_pred + (1 - T) * y0_pred\n",
    "\n",
    "    \n",
    "    # Compute inverse probability weights\n",
    "    e_x = torch.clamp(e_x, 1e-6, 1 - 1e-6)  # Avoid division by zero\n",
    "    weight = (T - e_x) / (e_x * (1 - e_x))\n",
    "\n",
    "\n",
    "    # Compute y_pred_tilde (corrected y_pred with propsensity scores)\n",
    "    y_pred_tilde = y_pred + epsilon * weight\n",
    "    \n",
    "    # Targeted regularization loss\n",
    "    t_loss = torch.mean((Y-y_pred_tilde) ** 2)\n",
    "    \n",
    "    return t_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "596e082b-9f8e-40f8-a020-7091ebfe7e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  blood_pressure  cholesterol  treatment  outcome\n",
      "0   55             140          200          1      120\n",
      "1   40             130          180          2      125\n",
      "2   60             150          220          3      145\n",
      "Covariates shape: torch.Size([3, 3])\n",
      "Treatment shape: torch.Size([3])\n",
      "Outcome shape: torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'age': [55, 40, 60],\n",
    "    'blood_pressure': [140, 130, 150],\n",
    "    'cholesterol': [200, 180, 220],\n",
    "    'treatment': [1, 2, 3],  # Treatment as categorical variable\n",
    "    'outcome': [120, 125, 145]  # Observed outcome (only needed for training)\n",
    "})\n",
    "\n",
    "print(data)\n",
    "\n",
    "import torch\n",
    "\n",
    "def dataframe_to_tensors(df):\n",
    "    \"\"\"\n",
    "    Convert a Pandas DataFrame into PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Input DataFrame with covariates, treatments, and optionally outcomes.\n",
    "\n",
    "    Returns:\n",
    "    covariates_tensor (torch.Tensor): Shape (batch_size, num_covariates)\n",
    "    treatment_tensor (torch.Tensor): Shape (batch_size,)\n",
    "    outcome_tensor (torch.Tensor or None): Shape (batch_size, 1) if available, else None\n",
    "    \"\"\"\n",
    "    # Convert continuous covariates to float tensor\n",
    "    covariates = torch.tensor(df.iloc[:, :-2].values, dtype=torch.float32)  # All except last 2 cols\n",
    "    # Convert treatment to integer tensor\n",
    "    treatment = torch.tensor(df['treatment'].values, dtype=torch.long)  # Long tensor for embedding lookup\n",
    "    # Convert outcome if available\n",
    "    outcome = torch.tensor(df['outcome'].values, dtype=torch.float32).unsqueeze(1) if 'outcome' in df else None\n",
    "    \n",
    "    return covariates, treatment, outcome\n",
    "\n",
    "# Convert DataFrame\n",
    "covariates_tensor, treatment_tensor, outcome_tensor = dataframe_to_tensors(data)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Covariates shape:\", covariates_tensor.shape)  # Expected: (batch_size, num_covariates)\n",
    "print(\"Treatment shape:\", treatment_tensor.shape)  # Expected: (batch_size,)\n",
    "print(\"Outcome shape:\", outcome_tensor.shape)  # Expected: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d0a0c5f0-2b0f-43b7-9f93-9f9a8a560bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted outcome: tensor([[ 0.2193],\n",
      "        [ 0.2970],\n",
      "        [-0.4642]], grad_fn=<AddmmBackward0>)\n",
      "Predicted e_x: tensor([[0.0504],\n",
      "        [0.0319],\n",
      "        [0.0689]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "num_covariates = 3  # Age, BP, Cholesterol\n",
    "embedding_dim = 8   # Embedding dimension for both covariates & treatments\n",
    "num_treatments = 5  # Assume 5 possible treatments\n",
    "\n",
    "# Initialize the model\n",
    "model = TransTEE(num_covariates, embedding_dim, num_treatments)\n",
    "\n",
    "# Perform forward pass (inference)\n",
    "predicted_outcome, e_x = model(covariates_tensor, treatment_tensor)\n",
    "\n",
    "print(\"Predicted outcome:\", predicted_outcome)\n",
    "print(\"Predicted e_x:\", e_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1bc6fc0d-2e9c-4638-b2da-ba3a7df4e66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 17047.5039\n",
      "Epoch 10, Loss: 16614.7246\n",
      "Epoch 20, Loss: 16596.6367\n",
      "Epoch 30, Loss: 16568.8438\n",
      "Epoch 40, Loss: 16508.1270\n",
      "Epoch 50, Loss: 16507.9863\n",
      "Epoch 60, Loss: 16463.8730\n",
      "Epoch 70, Loss: 16421.8418\n",
      "Epoch 80, Loss: 16422.6426\n",
      "Epoch 90, Loss: 16358.5273\n"
     ]
    }
   ],
   "source": [
    "# Define loss function (Mean Squared Error for regression)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (one epoch for example)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred, e_x = model(covariates_tensor, treatment_tensor)  # Forward pass\n",
    "    regressionloss = loss_function(y_pred, outcome_tensor)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "572f28fa-b0f8-4d3d-be91-b5aa7aeff4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted outcome: (tensor([[2.5095],\n",
      "        [2.8087],\n",
      "        [2.6630]], grad_fn=<AddmmBackward0>), tensor([[0.1285],\n",
      "        [0.0761],\n",
      "        [0.1836]], grad_fn=<SigmoidBackward0>))\n"
     ]
    }
   ],
   "source": [
    "# Perform forward pass (inference)\n",
    "predicted_outcome = model(covariates_tensor, treatment_tensor)\n",
    "\n",
    "print(\"Predicted outcome:\", predicted_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "98830dff-3f1d-4cc8-8052-865ca126cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([800, 5]), y_train shape: torch.Size([800, 1]), w_train shape: torch.Size([800, 1])\n",
      "tau_train shape: torch.Size([800])\n"
     ]
    }
   ],
   "source": [
    "# Use Dragon Net input data frame\n",
    "from causalml.dataset import synthetic_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load synthetic dataset using updated API\n",
    "y, X, w, tau, b, e = synthetic_data(mode=1, n=1000, p=5, sigma=1.0, adj=0.0)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test, w_train, w_test, tau_train, tau_test = train_test_split(\n",
    "    X, y, w, tau, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train, y_test = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1), torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "w_train, w_test = torch.tensor(w_train, dtype=torch.float32).unsqueeze(1), torch.tensor(w_test, dtype=torch.float32).unsqueeze(1)\n",
    "tau_train, tau_test = torch.tensor(tau_train, dtype=torch.float32), torch.tensor(tau_test, dtype=torch.float32)\n",
    "\n",
    "# Print dataset shapes to verify\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, w_train shape: {w_train.shape}\")\n",
    "print(f\"tau_train shape: {tau_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "88891558-45e0-4877-abc2-5f404ba59581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted outcome: tensor([[ 4.1619e-02],\n",
      "        [-4.7174e-02],\n",
      "        [ 3.7158e-02],\n",
      "        [ 2.6077e-01],\n",
      "        [-2.1781e-01],\n",
      "        [-3.0013e-02],\n",
      "        [ 1.4985e-01],\n",
      "        [ 5.4474e-01],\n",
      "        [ 1.3593e-01],\n",
      "        [ 2.0516e-01],\n",
      "        [ 2.9578e-02],\n",
      "        [ 1.1050e-01],\n",
      "        [-8.9443e-02],\n",
      "        [-1.0838e-01],\n",
      "        [-2.7468e-02],\n",
      "        [ 3.1280e-01],\n",
      "        [ 3.7207e-01],\n",
      "        [ 3.6446e-01],\n",
      "        [-2.7050e-02],\n",
      "        [ 8.9254e-02],\n",
      "        [ 4.6575e-01],\n",
      "        [-1.3428e-01],\n",
      "        [-1.8785e-01],\n",
      "        [ 1.5875e-01],\n",
      "        [ 2.0064e-02],\n",
      "        [ 6.3650e-01],\n",
      "        [-3.4367e-01],\n",
      "        [-7.2917e-02],\n",
      "        [ 1.6352e-01],\n",
      "        [ 4.2504e-01],\n",
      "        [ 2.7069e-01],\n",
      "        [ 1.5387e-02],\n",
      "        [ 4.9464e-01],\n",
      "        [ 7.5986e-02],\n",
      "        [ 1.3178e-02],\n",
      "        [ 2.5009e-01],\n",
      "        [-7.4835e-02],\n",
      "        [-1.4526e-01],\n",
      "        [ 8.5950e-02],\n",
      "        [ 2.1924e-02],\n",
      "        [-2.4607e-02],\n",
      "        [ 9.6194e-02],\n",
      "        [ 3.8456e-01],\n",
      "        [ 2.2390e-01],\n",
      "        [ 2.3268e-01],\n",
      "        [ 2.5790e-01],\n",
      "        [-1.7460e-01],\n",
      "        [-1.4328e-01],\n",
      "        [ 3.8488e-01],\n",
      "        [ 1.0055e-01],\n",
      "        [-1.1348e-01],\n",
      "        [ 4.5950e-02],\n",
      "        [ 1.0181e-01],\n",
      "        [-1.9636e-01],\n",
      "        [ 3.7745e-01],\n",
      "        [ 4.5269e-01],\n",
      "        [ 3.8202e-01],\n",
      "        [ 5.1492e-01],\n",
      "        [ 9.6689e-02],\n",
      "        [-1.9290e-01],\n",
      "        [-5.1517e-03],\n",
      "        [ 1.1821e-01],\n",
      "        [ 1.6062e-01],\n",
      "        [ 4.3635e-01],\n",
      "        [-7.7456e-02],\n",
      "        [ 5.3772e-01],\n",
      "        [-7.3417e-02],\n",
      "        [ 3.0158e-01],\n",
      "        [-2.8072e-01],\n",
      "        [ 7.4854e-02],\n",
      "        [-2.2815e-01],\n",
      "        [ 1.5037e-01],\n",
      "        [-3.1582e-01],\n",
      "        [ 1.9851e-01],\n",
      "        [-3.6161e-02],\n",
      "        [ 8.8601e-02],\n",
      "        [ 3.2963e-01],\n",
      "        [-2.7846e-02],\n",
      "        [ 2.7259e-01],\n",
      "        [ 3.6709e-01],\n",
      "        [-3.6851e-02],\n",
      "        [ 1.2064e-01],\n",
      "        [ 4.1161e-02],\n",
      "        [-9.6429e-02],\n",
      "        [ 3.6952e-01],\n",
      "        [-2.7473e-01],\n",
      "        [ 2.9905e-02],\n",
      "        [ 3.1227e-01],\n",
      "        [ 7.0941e-01],\n",
      "        [-1.6150e-01],\n",
      "        [ 1.3391e-02],\n",
      "        [ 3.2191e-01],\n",
      "        [ 4.4582e-02],\n",
      "        [ 4.7016e-03],\n",
      "        [ 7.1521e-01],\n",
      "        [ 2.4827e-01],\n",
      "        [-3.1220e-01],\n",
      "        [-9.3871e-02],\n",
      "        [-8.1381e-03],\n",
      "        [-1.9595e-01],\n",
      "        [-3.1307e-01],\n",
      "        [-1.0481e-01],\n",
      "        [ 1.4253e-01],\n",
      "        [ 2.3382e-01],\n",
      "        [-1.0409e-01],\n",
      "        [ 1.4488e-01],\n",
      "        [ 2.6667e-01],\n",
      "        [-2.3018e-01],\n",
      "        [ 5.9037e-03],\n",
      "        [ 2.0992e-01],\n",
      "        [ 9.5213e-02],\n",
      "        [ 2.7151e-01],\n",
      "        [-5.5533e-02],\n",
      "        [ 4.8003e-01],\n",
      "        [ 6.1741e-02],\n",
      "        [ 2.6405e-01],\n",
      "        [ 2.5200e-02],\n",
      "        [ 3.2263e-01],\n",
      "        [ 3.9911e-01],\n",
      "        [ 4.7346e-01],\n",
      "        [-2.0146e-01],\n",
      "        [-1.6649e-01],\n",
      "        [ 6.5905e-02],\n",
      "        [ 1.7395e-01],\n",
      "        [ 1.3134e-01],\n",
      "        [ 6.7503e-02],\n",
      "        [ 3.7448e-01],\n",
      "        [ 1.5623e-01],\n",
      "        [ 2.3681e-01],\n",
      "        [ 3.3309e-01],\n",
      "        [ 2.2849e-01],\n",
      "        [ 4.3125e-01],\n",
      "        [ 4.7111e-01],\n",
      "        [ 4.2673e-02],\n",
      "        [ 1.9263e-01],\n",
      "        [-2.6064e-01],\n",
      "        [ 1.1544e-01],\n",
      "        [ 5.9079e-01],\n",
      "        [-1.1005e-02],\n",
      "        [ 2.0980e-01],\n",
      "        [-9.7998e-02],\n",
      "        [-2.9638e-01],\n",
      "        [ 1.5826e-01],\n",
      "        [ 3.5257e-01],\n",
      "        [-7.1841e-02],\n",
      "        [ 4.0852e-01],\n",
      "        [ 2.0797e-01],\n",
      "        [-2.9597e-02],\n",
      "        [ 3.4620e-02],\n",
      "        [ 3.2324e-01],\n",
      "        [ 1.5957e-02],\n",
      "        [-7.2101e-02],\n",
      "        [-8.3863e-02],\n",
      "        [-3.1202e-01],\n",
      "        [ 4.4620e-01],\n",
      "        [ 1.6571e-01],\n",
      "        [-2.3824e-01],\n",
      "        [ 3.6247e-01],\n",
      "        [ 2.5117e-01],\n",
      "        [-3.0388e-01],\n",
      "        [ 2.3087e-01],\n",
      "        [ 4.0468e-01],\n",
      "        [ 2.2562e-01],\n",
      "        [-8.6506e-02],\n",
      "        [-2.2535e-01],\n",
      "        [ 4.0960e-01],\n",
      "        [-2.0138e-01],\n",
      "        [ 2.5695e-01],\n",
      "        [ 2.4452e-01],\n",
      "        [ 2.4797e-02],\n",
      "        [-7.5151e-03],\n",
      "        [ 3.8652e-01],\n",
      "        [ 3.5928e-02],\n",
      "        [-3.2404e-01],\n",
      "        [ 4.3238e-01],\n",
      "        [ 7.0504e-02],\n",
      "        [ 1.5015e-01],\n",
      "        [-4.2079e-02],\n",
      "        [ 2.6944e-01],\n",
      "        [-1.0622e-02],\n",
      "        [-1.2160e-02],\n",
      "        [ 5.8016e-02],\n",
      "        [ 7.7206e-02],\n",
      "        [ 3.5369e-01],\n",
      "        [ 5.3278e-02],\n",
      "        [ 4.7450e-01],\n",
      "        [ 2.9128e-01],\n",
      "        [-3.8110e-01],\n",
      "        [-2.1710e-01],\n",
      "        [-1.4611e-01],\n",
      "        [-1.5621e-01],\n",
      "        [ 1.3096e-01],\n",
      "        [ 1.9282e-01],\n",
      "        [-1.3392e-01],\n",
      "        [ 1.2454e-01],\n",
      "        [ 2.0806e-01],\n",
      "        [-1.5929e-01],\n",
      "        [ 3.2906e-01],\n",
      "        [ 1.8844e-02],\n",
      "        [ 1.1630e-01],\n",
      "        [ 1.0835e-01],\n",
      "        [ 3.7513e-01],\n",
      "        [-2.0385e-01],\n",
      "        [-1.0525e-01],\n",
      "        [ 1.2598e-01],\n",
      "        [ 3.5153e-01],\n",
      "        [-1.5763e-01],\n",
      "        [-1.1124e-01],\n",
      "        [-1.8465e-01],\n",
      "        [-1.2879e-01],\n",
      "        [-3.1830e-02],\n",
      "        [-2.9072e-01],\n",
      "        [ 3.5219e-01],\n",
      "        [ 1.2658e-01],\n",
      "        [ 3.3837e-01],\n",
      "        [ 9.2928e-02],\n",
      "        [-3.9502e-02],\n",
      "        [ 1.3440e-02],\n",
      "        [ 5.4014e-01],\n",
      "        [ 3.1348e-01],\n",
      "        [ 4.1060e-01],\n",
      "        [ 5.0078e-02],\n",
      "        [ 1.0062e-01],\n",
      "        [ 4.1945e-02],\n",
      "        [ 2.3471e-01],\n",
      "        [ 9.2659e-02],\n",
      "        [-3.4781e-01],\n",
      "        [-1.0991e-01],\n",
      "        [ 1.8256e-02],\n",
      "        [-6.9741e-03],\n",
      "        [ 4.2443e-01],\n",
      "        [-7.5930e-02],\n",
      "        [-2.7280e-01],\n",
      "        [-1.9598e-01],\n",
      "        [ 2.7624e-01],\n",
      "        [ 1.7210e-01],\n",
      "        [ 1.7186e-01],\n",
      "        [ 3.4190e-01],\n",
      "        [ 1.9445e-01],\n",
      "        [ 1.5311e-01],\n",
      "        [ 8.3637e-02],\n",
      "        [-1.7052e-01],\n",
      "        [-1.5958e-02],\n",
      "        [-1.1000e-01],\n",
      "        [ 4.4260e-02],\n",
      "        [ 9.5015e-02],\n",
      "        [ 4.8367e-01],\n",
      "        [ 4.9640e-01],\n",
      "        [ 2.0298e-01],\n",
      "        [-1.2366e-02],\n",
      "        [-3.1472e-03],\n",
      "        [-2.6938e-02],\n",
      "        [ 7.0658e-02],\n",
      "        [ 9.6820e-02],\n",
      "        [ 3.2975e-01],\n",
      "        [ 1.5826e-01],\n",
      "        [-1.7275e-01],\n",
      "        [ 1.5072e-02],\n",
      "        [ 1.3520e-01],\n",
      "        [ 8.8403e-02],\n",
      "        [ 2.4193e-01],\n",
      "        [ 3.0695e-01],\n",
      "        [-2.3109e-01],\n",
      "        [ 1.2426e-01],\n",
      "        [ 1.7280e-01],\n",
      "        [ 5.2074e-01],\n",
      "        [ 2.8055e-01],\n",
      "        [-8.2422e-02],\n",
      "        [-1.7476e-01],\n",
      "        [ 7.2903e-02],\n",
      "        [ 2.6068e-01],\n",
      "        [ 1.4137e-01],\n",
      "        [ 4.3052e-01],\n",
      "        [ 2.3838e-01],\n",
      "        [ 4.9702e-01],\n",
      "        [-4.5888e-02],\n",
      "        [-2.2422e-01],\n",
      "        [ 1.2244e-01],\n",
      "        [-9.2738e-02],\n",
      "        [-3.9801e-02],\n",
      "        [ 2.1877e-01],\n",
      "        [ 4.0161e-01],\n",
      "        [ 2.3848e-01],\n",
      "        [ 3.2186e-01],\n",
      "        [ 6.5739e-02],\n",
      "        [ 9.3003e-02],\n",
      "        [ 9.7552e-02],\n",
      "        [ 2.9134e-01],\n",
      "        [-3.9527e-02],\n",
      "        [ 5.1989e-01],\n",
      "        [-1.8780e-01],\n",
      "        [ 2.7316e-01],\n",
      "        [ 7.1712e-02],\n",
      "        [ 2.1058e-01],\n",
      "        [ 4.2747e-01],\n",
      "        [-2.7491e-01],\n",
      "        [ 1.0638e-01],\n",
      "        [ 2.2922e-02],\n",
      "        [-7.3286e-02],\n",
      "        [ 3.9870e-02],\n",
      "        [-1.1693e-01],\n",
      "        [ 8.3842e-03],\n",
      "        [ 1.3275e-01],\n",
      "        [ 5.9919e-01],\n",
      "        [ 1.4389e-02],\n",
      "        [ 3.2378e-01],\n",
      "        [-1.2987e-01],\n",
      "        [ 4.0687e-02],\n",
      "        [ 2.5374e-01],\n",
      "        [-3.9025e-02],\n",
      "        [ 3.7610e-01],\n",
      "        [-2.7607e-01],\n",
      "        [-9.4754e-02],\n",
      "        [ 2.1260e-01],\n",
      "        [ 2.1397e-01],\n",
      "        [ 7.0743e-03],\n",
      "        [ 1.3914e-01],\n",
      "        [ 6.7979e-02],\n",
      "        [ 2.3216e-01],\n",
      "        [ 2.1829e-01],\n",
      "        [ 1.3949e-03],\n",
      "        [ 9.5471e-02],\n",
      "        [ 3.8220e-01],\n",
      "        [ 2.6634e-01],\n",
      "        [ 3.4822e-01],\n",
      "        [ 2.8257e-01],\n",
      "        [ 8.3705e-02],\n",
      "        [ 8.9720e-02],\n",
      "        [ 2.6600e-01],\n",
      "        [ 1.5329e-01],\n",
      "        [ 2.6027e-01],\n",
      "        [ 4.7451e-01],\n",
      "        [ 2.0687e-01],\n",
      "        [-1.6004e-01],\n",
      "        [ 2.0161e-01],\n",
      "        [ 1.8167e-01],\n",
      "        [ 5.4628e-01],\n",
      "        [ 6.3072e-01],\n",
      "        [ 3.2777e-01],\n",
      "        [-9.0247e-02],\n",
      "        [ 5.4999e-01],\n",
      "        [ 5.8702e-02],\n",
      "        [-1.2266e-01],\n",
      "        [ 4.3701e-01],\n",
      "        [ 2.7816e-01],\n",
      "        [ 1.4056e-01],\n",
      "        [ 6.2749e-01],\n",
      "        [ 1.7358e-01],\n",
      "        [-1.1300e-01],\n",
      "        [-1.2372e-01],\n",
      "        [ 6.4949e-02],\n",
      "        [-1.0942e-01],\n",
      "        [ 9.7331e-02],\n",
      "        [-1.8476e-01],\n",
      "        [ 1.0835e-01],\n",
      "        [ 1.7196e-01],\n",
      "        [ 3.6717e-01],\n",
      "        [ 4.0677e-01],\n",
      "        [-2.6011e-01],\n",
      "        [ 2.0631e-01],\n",
      "        [-6.8113e-02],\n",
      "        [ 3.6601e-01],\n",
      "        [-1.8639e-01],\n",
      "        [ 4.7342e-02],\n",
      "        [-5.2492e-02],\n",
      "        [ 1.5207e-01],\n",
      "        [-1.0795e-01],\n",
      "        [-2.0954e-01],\n",
      "        [-2.1991e-02],\n",
      "        [-5.1233e-02],\n",
      "        [-2.9733e-01],\n",
      "        [ 8.3888e-02],\n",
      "        [ 3.6185e-01],\n",
      "        [ 4.4857e-01],\n",
      "        [ 4.0211e-02],\n",
      "        [-3.0753e-01],\n",
      "        [ 5.2803e-01],\n",
      "        [ 2.3927e-01],\n",
      "        [ 1.6711e-01],\n",
      "        [ 6.3384e-02],\n",
      "        [-8.2554e-02],\n",
      "        [-5.8634e-03],\n",
      "        [-2.2982e-01],\n",
      "        [-8.7049e-02],\n",
      "        [-1.5132e-01],\n",
      "        [-6.9940e-02],\n",
      "        [ 6.2234e-01],\n",
      "        [ 5.5037e-01],\n",
      "        [ 8.4327e-02],\n",
      "        [-2.1801e-03],\n",
      "        [ 6.0866e-01],\n",
      "        [ 2.4399e-01],\n",
      "        [-4.3945e-02],\n",
      "        [ 3.7606e-03],\n",
      "        [-5.8041e-02],\n",
      "        [-1.9942e-01],\n",
      "        [ 1.1793e-01],\n",
      "        [ 1.7355e-01],\n",
      "        [ 1.8789e-01],\n",
      "        [ 2.8533e-01],\n",
      "        [-1.6697e-01],\n",
      "        [-1.4237e-01],\n",
      "        [-1.9956e-01],\n",
      "        [ 1.9730e-01],\n",
      "        [ 2.2876e-01],\n",
      "        [ 1.3226e-01],\n",
      "        [-2.7678e-02],\n",
      "        [ 8.1514e-02],\n",
      "        [-2.4537e-01],\n",
      "        [-8.8984e-02],\n",
      "        [ 3.5538e-02],\n",
      "        [-3.5275e-01],\n",
      "        [ 1.7812e-01],\n",
      "        [ 1.6532e-01],\n",
      "        [ 3.0404e-01],\n",
      "        [ 5.5842e-01],\n",
      "        [-1.2817e-01],\n",
      "        [ 5.1086e-01],\n",
      "        [ 2.5093e-01],\n",
      "        [ 4.8849e-02],\n",
      "        [-7.5505e-02],\n",
      "        [ 1.8992e-01],\n",
      "        [ 2.6867e-01],\n",
      "        [ 2.3588e-01],\n",
      "        [ 4.2426e-01],\n",
      "        [ 1.6382e-01],\n",
      "        [ 2.0168e-01],\n",
      "        [ 2.2150e-01],\n",
      "        [ 1.4191e-01],\n",
      "        [-6.6655e-02],\n",
      "        [ 2.5170e-02],\n",
      "        [-3.1825e-02],\n",
      "        [-1.0599e-01],\n",
      "        [-3.5956e-01],\n",
      "        [-1.8194e-01],\n",
      "        [-1.4885e-03],\n",
      "        [-8.6427e-02],\n",
      "        [ 1.4557e-01],\n",
      "        [ 1.0869e-01],\n",
      "        [-3.3174e-02],\n",
      "        [ 1.2269e-01],\n",
      "        [-9.5002e-03],\n",
      "        [ 2.1484e-01],\n",
      "        [ 4.1368e-01],\n",
      "        [ 4.1165e-01],\n",
      "        [-1.1074e-01],\n",
      "        [ 6.1492e-02],\n",
      "        [-1.9590e-02],\n",
      "        [ 2.9853e-02],\n",
      "        [-2.5874e-01],\n",
      "        [ 3.3721e-01],\n",
      "        [-1.8528e-01],\n",
      "        [ 2.5120e-01],\n",
      "        [ 4.6648e-02],\n",
      "        [ 2.1394e-01],\n",
      "        [-5.7052e-02],\n",
      "        [ 6.2954e-02],\n",
      "        [ 1.1083e-01],\n",
      "        [ 1.6048e-02],\n",
      "        [ 5.0061e-02],\n",
      "        [-1.6380e-01],\n",
      "        [ 4.6782e-01],\n",
      "        [-8.9221e-02],\n",
      "        [-2.8915e-02],\n",
      "        [ 2.4318e-01],\n",
      "        [ 4.0317e-01],\n",
      "        [ 5.0681e-01],\n",
      "        [ 1.0311e-01],\n",
      "        [-8.9696e-02],\n",
      "        [ 3.2210e-01],\n",
      "        [ 2.7524e-01],\n",
      "        [-1.5200e-01],\n",
      "        [-4.5569e-02],\n",
      "        [ 5.0256e-03],\n",
      "        [-3.6284e-01],\n",
      "        [-1.9482e-02],\n",
      "        [-3.3915e-01],\n",
      "        [-1.1097e-01],\n",
      "        [ 4.3087e-01],\n",
      "        [-1.8511e-01],\n",
      "        [-1.3029e-01],\n",
      "        [-9.7700e-02],\n",
      "        [ 1.7821e-01],\n",
      "        [ 2.7160e-02],\n",
      "        [-1.9398e-04],\n",
      "        [-5.7346e-02],\n",
      "        [-6.8244e-02],\n",
      "        [ 4.3709e-01],\n",
      "        [ 1.4262e-01],\n",
      "        [ 1.1440e-02],\n",
      "        [ 4.5195e-01],\n",
      "        [ 1.8189e-01],\n",
      "        [ 4.5769e-03],\n",
      "        [-2.5719e-01],\n",
      "        [ 3.3315e-01],\n",
      "        [ 2.3834e-01],\n",
      "        [ 2.1366e-01],\n",
      "        [ 1.6024e-01],\n",
      "        [ 1.4078e-01],\n",
      "        [ 9.3999e-02],\n",
      "        [ 1.1843e-01],\n",
      "        [ 2.5495e-01],\n",
      "        [-2.9126e-02],\n",
      "        [ 3.4140e-02],\n",
      "        [-1.0688e-01],\n",
      "        [ 1.8626e-02],\n",
      "        [ 1.4134e-01],\n",
      "        [ 3.0541e-01],\n",
      "        [ 5.0541e-01],\n",
      "        [ 2.4090e-01],\n",
      "        [ 3.0505e-01],\n",
      "        [-9.4545e-02],\n",
      "        [ 3.9404e-01],\n",
      "        [ 3.8452e-01],\n",
      "        [-2.1250e-02],\n",
      "        [-8.2577e-02],\n",
      "        [ 3.4634e-01],\n",
      "        [ 2.6899e-01],\n",
      "        [ 2.3265e-02],\n",
      "        [ 1.1286e-02],\n",
      "        [ 2.3001e-01],\n",
      "        [ 3.2123e-01],\n",
      "        [ 2.3369e-01],\n",
      "        [ 5.2048e-02],\n",
      "        [-4.4606e-02],\n",
      "        [-1.0141e-01],\n",
      "        [ 6.5566e-01],\n",
      "        [-5.5383e-02],\n",
      "        [ 1.5928e-01],\n",
      "        [ 1.0704e-02],\n",
      "        [ 2.7456e-01],\n",
      "        [ 1.4495e-01],\n",
      "        [-1.5298e-02],\n",
      "        [ 1.8260e-01],\n",
      "        [ 5.9462e-02],\n",
      "        [-1.1335e-01],\n",
      "        [ 2.8959e-01],\n",
      "        [ 1.1245e-02],\n",
      "        [-1.7670e-01],\n",
      "        [ 2.6285e-01],\n",
      "        [ 4.2119e-01],\n",
      "        [-4.9924e-02],\n",
      "        [ 8.7432e-02],\n",
      "        [ 1.0563e-01],\n",
      "        [ 4.2648e-01],\n",
      "        [ 2.0309e-01],\n",
      "        [ 4.7858e-01],\n",
      "        [ 2.6911e-01],\n",
      "        [ 2.1075e-01],\n",
      "        [ 3.9953e-01],\n",
      "        [-6.5394e-02],\n",
      "        [-2.6076e-01],\n",
      "        [-2.0145e-01],\n",
      "        [ 4.0159e-01],\n",
      "        [ 1.1723e-01],\n",
      "        [ 2.4790e-01],\n",
      "        [-1.6079e-01],\n",
      "        [-2.2818e-01],\n",
      "        [ 3.3726e-01],\n",
      "        [-1.0829e-01],\n",
      "        [ 3.2467e-02],\n",
      "        [-2.0012e-01],\n",
      "        [ 1.3626e-01],\n",
      "        [-1.6726e-01],\n",
      "        [ 8.9791e-02],\n",
      "        [ 3.4825e-01],\n",
      "        [-4.7173e-02],\n",
      "        [ 1.3667e-01],\n",
      "        [-1.8809e-01],\n",
      "        [ 4.2905e-01],\n",
      "        [-1.5529e-01],\n",
      "        [-1.6028e-02],\n",
      "        [ 7.6802e-02],\n",
      "        [ 1.8692e-01],\n",
      "        [ 4.4834e-02],\n",
      "        [ 2.3530e-01],\n",
      "        [ 4.7510e-01],\n",
      "        [ 3.3140e-01],\n",
      "        [-1.8008e-01],\n",
      "        [ 2.9652e-01],\n",
      "        [ 1.8687e-01],\n",
      "        [-2.5524e-01],\n",
      "        [ 2.3503e-01],\n",
      "        [ 3.0250e-01],\n",
      "        [ 3.6172e-01],\n",
      "        [ 2.8669e-01],\n",
      "        [-2.7292e-01],\n",
      "        [-2.2661e-02],\n",
      "        [ 1.0136e-02],\n",
      "        [ 1.8003e-01],\n",
      "        [ 4.3209e-01],\n",
      "        [ 4.1664e-01],\n",
      "        [ 2.2457e-01],\n",
      "        [ 1.1729e-01],\n",
      "        [ 1.3050e-02],\n",
      "        [ 3.6215e-02],\n",
      "        [-1.8944e-01],\n",
      "        [ 1.0316e-01],\n",
      "        [ 2.1233e-01],\n",
      "        [ 1.2824e-01],\n",
      "        [ 4.6840e-01],\n",
      "        [-6.3149e-03],\n",
      "        [ 2.0203e-01],\n",
      "        [ 2.0109e-01],\n",
      "        [ 1.8607e-01],\n",
      "        [ 1.2406e-01],\n",
      "        [ 3.3377e-01],\n",
      "        [ 1.0155e-01],\n",
      "        [ 3.0980e-01],\n",
      "        [-3.0127e-01],\n",
      "        [-1.8152e-01],\n",
      "        [ 2.1525e-01],\n",
      "        [ 5.0929e-02],\n",
      "        [ 5.5335e-01],\n",
      "        [ 1.9654e-02],\n",
      "        [ 2.9473e-01],\n",
      "        [-9.6614e-02],\n",
      "        [-4.8432e-02],\n",
      "        [ 1.8956e-01],\n",
      "        [ 1.3007e-01],\n",
      "        [ 3.3697e-01],\n",
      "        [ 3.6605e-01],\n",
      "        [-9.8594e-02],\n",
      "        [-1.1320e-01],\n",
      "        [ 2.1674e-01],\n",
      "        [ 1.0942e-01],\n",
      "        [ 2.1179e-01],\n",
      "        [-6.7655e-02],\n",
      "        [ 7.2313e-02],\n",
      "        [ 8.3227e-02],\n",
      "        [ 3.0281e-01],\n",
      "        [-2.3709e-01],\n",
      "        [ 7.3212e-02],\n",
      "        [ 2.2591e-01],\n",
      "        [-1.0084e-01],\n",
      "        [-4.7159e-02],\n",
      "        [-5.9420e-02],\n",
      "        [ 6.0801e-01],\n",
      "        [ 1.0766e-01],\n",
      "        [-3.2169e-02],\n",
      "        [ 9.9913e-02],\n",
      "        [ 3.2755e-01],\n",
      "        [ 1.0928e-01],\n",
      "        [-1.3621e-01],\n",
      "        [-1.9961e-02],\n",
      "        [ 3.0642e-01],\n",
      "        [ 3.1814e-01],\n",
      "        [-5.9720e-03],\n",
      "        [-3.8071e-02],\n",
      "        [ 2.8282e-01],\n",
      "        [-3.2982e-01],\n",
      "        [-2.0195e-01],\n",
      "        [ 1.6974e-01],\n",
      "        [ 3.1078e-01],\n",
      "        [ 2.0851e-01],\n",
      "        [-1.3798e-01],\n",
      "        [-2.1442e-01],\n",
      "        [-1.2372e-01],\n",
      "        [ 3.2572e-01],\n",
      "        [ 6.2752e-01],\n",
      "        [-1.0791e-01],\n",
      "        [ 4.1924e-01],\n",
      "        [ 2.6401e-01],\n",
      "        [-1.4066e-01],\n",
      "        [ 2.7623e-02],\n",
      "        [ 2.5428e-01],\n",
      "        [-9.1520e-03],\n",
      "        [ 3.1012e-01],\n",
      "        [ 2.8394e-01],\n",
      "        [ 6.4652e-02],\n",
      "        [ 5.8525e-02],\n",
      "        [-1.4637e-01],\n",
      "        [ 2.7785e-01],\n",
      "        [ 4.8891e-02],\n",
      "        [ 2.8412e-01],\n",
      "        [ 6.5809e-02],\n",
      "        [ 6.4049e-02],\n",
      "        [ 9.3394e-03],\n",
      "        [ 7.2318e-02],\n",
      "        [-6.0835e-03],\n",
      "        [ 5.4584e-02],\n",
      "        [ 3.7263e-01],\n",
      "        [-9.1604e-02],\n",
      "        [ 3.9399e-01],\n",
      "        [-2.0185e-01],\n",
      "        [-1.7249e-03],\n",
      "        [ 2.1052e-01],\n",
      "        [ 3.7940e-01],\n",
      "        [ 2.7108e-01],\n",
      "        [ 4.7876e-02],\n",
      "        [ 4.8720e-01],\n",
      "        [-3.6596e-02],\n",
      "        [ 1.2966e-01],\n",
      "        [ 3.6417e-01],\n",
      "        [-1.2505e-01],\n",
      "        [ 4.2038e-01],\n",
      "        [-2.3596e-01],\n",
      "        [ 3.4437e-01],\n",
      "        [ 2.5124e-01],\n",
      "        [ 8.5093e-02],\n",
      "        [ 9.6591e-02],\n",
      "        [ 3.9039e-01],\n",
      "        [ 2.0135e-01],\n",
      "        [ 2.1073e-01],\n",
      "        [ 4.7768e-01],\n",
      "        [-1.7676e-01],\n",
      "        [ 3.4938e-01],\n",
      "        [-1.7111e-01],\n",
      "        [ 4.2598e-01],\n",
      "        [ 1.0352e-01],\n",
      "        [ 1.3862e-01],\n",
      "        [ 3.2144e-01],\n",
      "        [-1.0895e-01],\n",
      "        [-5.6744e-02],\n",
      "        [-1.1334e-01],\n",
      "        [-2.0386e-01],\n",
      "        [ 2.8202e-01],\n",
      "        [ 2.2352e-01],\n",
      "        [ 2.7321e-01],\n",
      "        [ 4.2272e-01],\n",
      "        [ 1.0330e-01],\n",
      "        [ 1.0737e-01],\n",
      "        [ 1.5555e-01],\n",
      "        [ 4.8638e-01],\n",
      "        [-6.1130e-02],\n",
      "        [ 1.4915e-01],\n",
      "        [ 2.7267e-01],\n",
      "        [ 1.5836e-01],\n",
      "        [ 2.2205e-01],\n",
      "        [-2.2486e-01],\n",
      "        [ 1.5280e-01],\n",
      "        [ 6.2257e-02],\n",
      "        [ 2.3741e-02],\n",
      "        [-4.6383e-02],\n",
      "        [-2.6206e-03],\n",
      "        [ 5.1582e-02],\n",
      "        [ 9.9732e-02],\n",
      "        [-2.3053e-01],\n",
      "        [ 1.8861e-02],\n",
      "        [-3.3222e-02],\n",
      "        [ 4.5122e-01],\n",
      "        [-3.3356e-01],\n",
      "        [ 2.7103e-01],\n",
      "        [ 2.0958e-01],\n",
      "        [ 6.2508e-02],\n",
      "        [-3.2266e-02],\n",
      "        [ 3.7772e-01],\n",
      "        [ 1.9949e-01],\n",
      "        [-1.9226e-01],\n",
      "        [ 4.6442e-01],\n",
      "        [ 1.7223e-01],\n",
      "        [ 4.2367e-01],\n",
      "        [ 2.5925e-01],\n",
      "        [ 4.2551e-01],\n",
      "        [-5.3890e-02],\n",
      "        [-1.5285e-01],\n",
      "        [ 4.0978e-02],\n",
      "        [-2.4050e-01],\n",
      "        [-4.4428e-02],\n",
      "        [ 8.0310e-02],\n",
      "        [-1.4073e-01],\n",
      "        [ 6.3385e-02],\n",
      "        [ 3.7035e-02],\n",
      "        [-2.3306e-02],\n",
      "        [ 2.4856e-02],\n",
      "        [-1.6158e-01],\n",
      "        [ 1.4466e-01],\n",
      "        [ 3.2699e-01],\n",
      "        [ 2.7066e-02],\n",
      "        [ 3.4434e-01],\n",
      "        [ 6.8787e-02],\n",
      "        [-8.0493e-02],\n",
      "        [ 3.9263e-01],\n",
      "        [-5.7487e-02],\n",
      "        [ 3.3718e-01],\n",
      "        [ 1.2438e-02],\n",
      "        [ 1.0982e-01],\n",
      "        [ 2.6664e-01],\n",
      "        [ 2.0017e-01],\n",
      "        [-2.9998e-02],\n",
      "        [ 2.9740e-01],\n",
      "        [-2.2302e-02],\n",
      "        [-1.5225e-01],\n",
      "        [ 8.6497e-02],\n",
      "        [-4.3970e-02],\n",
      "        [-1.5443e-01],\n",
      "        [ 2.6512e-01],\n",
      "        [-1.2043e-01],\n",
      "        [-1.5584e-01],\n",
      "        [-2.9264e-02],\n",
      "        [ 3.3242e-01],\n",
      "        [ 2.6485e-01],\n",
      "        [ 3.7114e-01],\n",
      "        [ 9.5040e-02],\n",
      "        [ 1.5284e-01],\n",
      "        [ 8.1958e-02],\n",
      "        [ 1.3367e-01],\n",
      "        [-3.4282e-02],\n",
      "        [ 2.0838e-01],\n",
      "        [-7.3513e-02]], grad_fn=<AddmmBackward0>)\n",
      "Predicted e_x: tensor([[0.4381],\n",
      "        [0.4795],\n",
      "        [0.4464],\n",
      "        [0.4299],\n",
      "        [0.4698],\n",
      "        [0.4290],\n",
      "        [0.3987],\n",
      "        [0.3944],\n",
      "        [0.4568],\n",
      "        [0.4720],\n",
      "        [0.4506],\n",
      "        [0.3373],\n",
      "        [0.5029],\n",
      "        [0.4775],\n",
      "        [0.5338],\n",
      "        [0.4557],\n",
      "        [0.4472],\n",
      "        [0.3728],\n",
      "        [0.4481],\n",
      "        [0.4602],\n",
      "        [0.3916],\n",
      "        [0.4371],\n",
      "        [0.4883],\n",
      "        [0.3814],\n",
      "        [0.4705],\n",
      "        [0.3386],\n",
      "        [0.5226],\n",
      "        [0.5336],\n",
      "        [0.4835],\n",
      "        [0.4065],\n",
      "        [0.4008],\n",
      "        [0.5086],\n",
      "        [0.4649],\n",
      "        [0.4117],\n",
      "        [0.4898],\n",
      "        [0.4316],\n",
      "        [0.4687],\n",
      "        [0.5491],\n",
      "        [0.4665],\n",
      "        [0.4294],\n",
      "        [0.4611],\n",
      "        [0.4018],\n",
      "        [0.4330],\n",
      "        [0.3519],\n",
      "        [0.5129],\n",
      "        [0.4624],\n",
      "        [0.5137],\n",
      "        [0.4181],\n",
      "        [0.4012],\n",
      "        [0.4434],\n",
      "        [0.4333],\n",
      "        [0.4176],\n",
      "        [0.3828],\n",
      "        [0.4491],\n",
      "        [0.3992],\n",
      "        [0.4127],\n",
      "        [0.4106],\n",
      "        [0.3356],\n",
      "        [0.3996],\n",
      "        [0.4832],\n",
      "        [0.4607],\n",
      "        [0.5208],\n",
      "        [0.4689],\n",
      "        [0.3989],\n",
      "        [0.4758],\n",
      "        [0.3912],\n",
      "        [0.5144],\n",
      "        [0.3867],\n",
      "        [0.5223],\n",
      "        [0.4230],\n",
      "        [0.4527],\n",
      "        [0.4709],\n",
      "        [0.5094],\n",
      "        [0.4261],\n",
      "        [0.4252],\n",
      "        [0.3989],\n",
      "        [0.4475],\n",
      "        [0.4498],\n",
      "        [0.4253],\n",
      "        [0.4516],\n",
      "        [0.4283],\n",
      "        [0.4440],\n",
      "        [0.4508],\n",
      "        [0.5257],\n",
      "        [0.3557],\n",
      "        [0.5165],\n",
      "        [0.4509],\n",
      "        [0.4540],\n",
      "        [0.4160],\n",
      "        [0.4111],\n",
      "        [0.3983],\n",
      "        [0.3551],\n",
      "        [0.3762],\n",
      "        [0.4817],\n",
      "        [0.3521],\n",
      "        [0.5648],\n",
      "        [0.5114],\n",
      "        [0.5212],\n",
      "        [0.3949],\n",
      "        [0.5049],\n",
      "        [0.4749],\n",
      "        [0.4050],\n",
      "        [0.4453],\n",
      "        [0.4968],\n",
      "        [0.3685],\n",
      "        [0.4085],\n",
      "        [0.4128],\n",
      "        [0.4976],\n",
      "        [0.4362],\n",
      "        [0.4052],\n",
      "        [0.4485],\n",
      "        [0.4424],\n",
      "        [0.5232],\n",
      "        [0.4125],\n",
      "        [0.4670],\n",
      "        [0.5200],\n",
      "        [0.5200],\n",
      "        [0.4166],\n",
      "        [0.3998],\n",
      "        [0.3455],\n",
      "        [0.4512],\n",
      "        [0.4926],\n",
      "        [0.4586],\n",
      "        [0.4729],\n",
      "        [0.4277],\n",
      "        [0.5620],\n",
      "        [0.4252],\n",
      "        [0.3962],\n",
      "        [0.3833],\n",
      "        [0.4152],\n",
      "        [0.4422],\n",
      "        [0.4506],\n",
      "        [0.4211],\n",
      "        [0.4134],\n",
      "        [0.4682],\n",
      "        [0.5273],\n",
      "        [0.4320],\n",
      "        [0.3735],\n",
      "        [0.4231],\n",
      "        [0.4368],\n",
      "        [0.4402],\n",
      "        [0.4398],\n",
      "        [0.5128],\n",
      "        [0.4281],\n",
      "        [0.5028],\n",
      "        [0.4225],\n",
      "        [0.4266],\n",
      "        [0.4534],\n",
      "        [0.4784],\n",
      "        [0.4275],\n",
      "        [0.4680],\n",
      "        [0.4667],\n",
      "        [0.4876],\n",
      "        [0.4260],\n",
      "        [0.3517],\n",
      "        [0.3760],\n",
      "        [0.5251],\n",
      "        [0.4361],\n",
      "        [0.4006],\n",
      "        [0.4735],\n",
      "        [0.3879],\n",
      "        [0.3856],\n",
      "        [0.4543],\n",
      "        [0.4189],\n",
      "        [0.4584],\n",
      "        [0.3741],\n",
      "        [0.5212],\n",
      "        [0.4548],\n",
      "        [0.4154],\n",
      "        [0.4933],\n",
      "        [0.4383],\n",
      "        [0.4283],\n",
      "        [0.4795],\n",
      "        [0.5185],\n",
      "        [0.3459],\n",
      "        [0.4484],\n",
      "        [0.4532],\n",
      "        [0.5253],\n",
      "        [0.4407],\n",
      "        [0.4201],\n",
      "        [0.4510],\n",
      "        [0.4177],\n",
      "        [0.4629],\n",
      "        [0.5053],\n",
      "        [0.4247],\n",
      "        [0.4257],\n",
      "        [0.4596],\n",
      "        [0.5346],\n",
      "        [0.4751],\n",
      "        [0.4321],\n",
      "        [0.4841],\n",
      "        [0.4510],\n",
      "        [0.4733],\n",
      "        [0.4178],\n",
      "        [0.4100],\n",
      "        [0.3971],\n",
      "        [0.4969],\n",
      "        [0.3917],\n",
      "        [0.4276],\n",
      "        [0.4494],\n",
      "        [0.4038],\n",
      "        [0.3996],\n",
      "        [0.4252],\n",
      "        [0.5889],\n",
      "        [0.4421],\n",
      "        [0.3931],\n",
      "        [0.5071],\n",
      "        [0.4998],\n",
      "        [0.4453],\n",
      "        [0.4834],\n",
      "        [0.4423],\n",
      "        [0.4783],\n",
      "        [0.3273],\n",
      "        [0.5120],\n",
      "        [0.4222],\n",
      "        [0.5038],\n",
      "        [0.4028],\n",
      "        [0.4246],\n",
      "        [0.3893],\n",
      "        [0.3608],\n",
      "        [0.4379],\n",
      "        [0.4198],\n",
      "        [0.3724],\n",
      "        [0.4834],\n",
      "        [0.4236],\n",
      "        [0.3867],\n",
      "        [0.4459],\n",
      "        [0.4784],\n",
      "        [0.4179],\n",
      "        [0.4785],\n",
      "        [0.3921],\n",
      "        [0.5205],\n",
      "        [0.5533],\n",
      "        [0.4047],\n",
      "        [0.4454],\n",
      "        [0.4053],\n",
      "        [0.4139],\n",
      "        [0.3675],\n",
      "        [0.4381],\n",
      "        [0.4507],\n",
      "        [0.4794],\n",
      "        [0.4854],\n",
      "        [0.4705],\n",
      "        [0.4903],\n",
      "        [0.4649],\n",
      "        [0.5037],\n",
      "        [0.4220],\n",
      "        [0.4089],\n",
      "        [0.4213],\n",
      "        [0.4253],\n",
      "        [0.4648],\n",
      "        [0.4259],\n",
      "        [0.4943],\n",
      "        [0.3916],\n",
      "        [0.4064],\n",
      "        [0.4588],\n",
      "        [0.4279],\n",
      "        [0.4349],\n",
      "        [0.3778],\n",
      "        [0.4685],\n",
      "        [0.4044],\n",
      "        [0.3964],\n",
      "        [0.4556],\n",
      "        [0.4102],\n",
      "        [0.4857],\n",
      "        [0.4710],\n",
      "        [0.4011],\n",
      "        [0.5148],\n",
      "        [0.5337],\n",
      "        [0.3896],\n",
      "        [0.3885],\n",
      "        [0.4346],\n",
      "        [0.4586],\n",
      "        [0.4004],\n",
      "        [0.3140],\n",
      "        [0.4185],\n",
      "        [0.5013],\n",
      "        [0.4484],\n",
      "        [0.4939],\n",
      "        [0.4184],\n",
      "        [0.4076],\n",
      "        [0.3856],\n",
      "        [0.4798],\n",
      "        [0.4477],\n",
      "        [0.3931],\n",
      "        [0.3959],\n",
      "        [0.4331],\n",
      "        [0.4431],\n",
      "        [0.5726],\n",
      "        [0.4224],\n",
      "        [0.4768],\n",
      "        [0.3592],\n",
      "        [0.4090],\n",
      "        [0.3960],\n",
      "        [0.4330],\n",
      "        [0.4175],\n",
      "        [0.3737],\n",
      "        [0.5406],\n",
      "        [0.4075],\n",
      "        [0.3711],\n",
      "        [0.4996],\n",
      "        [0.5245],\n",
      "        [0.4373],\n",
      "        [0.3398],\n",
      "        [0.4953],\n",
      "        [0.4530],\n",
      "        [0.5344],\n",
      "        [0.4383],\n",
      "        [0.4809],\n",
      "        [0.4802],\n",
      "        [0.4362],\n",
      "        [0.5730],\n",
      "        [0.5013],\n",
      "        [0.4347],\n",
      "        [0.3606],\n",
      "        [0.4072],\n",
      "        [0.4684],\n",
      "        [0.4187],\n",
      "        [0.5047],\n",
      "        [0.4428],\n",
      "        [0.3793],\n",
      "        [0.4452],\n",
      "        [0.4104],\n",
      "        [0.4319],\n",
      "        [0.4103],\n",
      "        [0.4615],\n",
      "        [0.4459],\n",
      "        [0.4665],\n",
      "        [0.4491],\n",
      "        [0.4152],\n",
      "        [0.4555],\n",
      "        [0.3766],\n",
      "        [0.4236],\n",
      "        [0.4688],\n",
      "        [0.4365],\n",
      "        [0.5138],\n",
      "        [0.3705],\n",
      "        [0.3572],\n",
      "        [0.4363],\n",
      "        [0.5256],\n",
      "        [0.4145],\n",
      "        [0.4353],\n",
      "        [0.4732],\n",
      "        [0.3603],\n",
      "        [0.4555],\n",
      "        [0.5107],\n",
      "        [0.3315],\n",
      "        [0.3625],\n",
      "        [0.4207],\n",
      "        [0.5029],\n",
      "        [0.4158],\n",
      "        [0.4533],\n",
      "        [0.4298],\n",
      "        [0.4950],\n",
      "        [0.3892],\n",
      "        [0.4755],\n",
      "        [0.3955],\n",
      "        [0.4184],\n",
      "        [0.5381],\n",
      "        [0.3860],\n",
      "        [0.4454],\n",
      "        [0.4200],\n",
      "        [0.5156],\n",
      "        [0.4628],\n",
      "        [0.4254],\n",
      "        [0.4797],\n",
      "        [0.4886],\n",
      "        [0.4616],\n",
      "        [0.4529],\n",
      "        [0.4405],\n",
      "        [0.5380],\n",
      "        [0.4442],\n",
      "        [0.4392],\n",
      "        [0.3799],\n",
      "        [0.4079],\n",
      "        [0.5156],\n",
      "        [0.4397],\n",
      "        [0.4242],\n",
      "        [0.3648],\n",
      "        [0.4552],\n",
      "        [0.5101],\n",
      "        [0.5293],\n",
      "        [0.4413],\n",
      "        [0.4148],\n",
      "        [0.4434],\n",
      "        [0.5155],\n",
      "        [0.3261],\n",
      "        [0.4000],\n",
      "        [0.3827],\n",
      "        [0.4615],\n",
      "        [0.3455],\n",
      "        [0.5015],\n",
      "        [0.4256],\n",
      "        [0.4387],\n",
      "        [0.4794],\n",
      "        [0.4948],\n",
      "        [0.4530],\n",
      "        [0.4135],\n",
      "        [0.4799],\n",
      "        [0.4285],\n",
      "        [0.4651],\n",
      "        [0.5612],\n",
      "        [0.4571],\n",
      "        [0.4180],\n",
      "        [0.4322],\n",
      "        [0.4517],\n",
      "        [0.4640],\n",
      "        [0.4377],\n",
      "        [0.4626],\n",
      "        [0.5243],\n",
      "        [0.3659],\n",
      "        [0.5217],\n",
      "        [0.4114],\n",
      "        [0.4075],\n",
      "        [0.4926],\n",
      "        [0.3851],\n",
      "        [0.4490],\n",
      "        [0.3589],\n",
      "        [0.4115],\n",
      "        [0.4782],\n",
      "        [0.5038],\n",
      "        [0.4520],\n",
      "        [0.4153],\n",
      "        [0.4543],\n",
      "        [0.4565],\n",
      "        [0.3733],\n",
      "        [0.4284],\n",
      "        [0.4691],\n",
      "        [0.3879],\n",
      "        [0.5294],\n",
      "        [0.4700],\n",
      "        [0.4671],\n",
      "        [0.4606],\n",
      "        [0.4911],\n",
      "        [0.5550],\n",
      "        [0.4053],\n",
      "        [0.5544],\n",
      "        [0.4306],\n",
      "        [0.4137],\n",
      "        [0.5336],\n",
      "        [0.4823],\n",
      "        [0.4929],\n",
      "        [0.5102],\n",
      "        [0.3586],\n",
      "        [0.3554],\n",
      "        [0.4758],\n",
      "        [0.4418],\n",
      "        [0.5171],\n",
      "        [0.4899],\n",
      "        [0.5217],\n",
      "        [0.3615],\n",
      "        [0.4834],\n",
      "        [0.4829],\n",
      "        [0.3828],\n",
      "        [0.4606],\n",
      "        [0.4901],\n",
      "        [0.3741],\n",
      "        [0.4019],\n",
      "        [0.4306],\n",
      "        [0.4659],\n",
      "        [0.5181],\n",
      "        [0.4099],\n",
      "        [0.4898],\n",
      "        [0.4062],\n",
      "        [0.5028],\n",
      "        [0.4172],\n",
      "        [0.3923],\n",
      "        [0.4321],\n",
      "        [0.4467],\n",
      "        [0.3883],\n",
      "        [0.4159],\n",
      "        [0.4797],\n",
      "        [0.4510],\n",
      "        [0.4486],\n",
      "        [0.3942],\n",
      "        [0.4898],\n",
      "        [0.4696],\n",
      "        [0.4215],\n",
      "        [0.4262],\n",
      "        [0.4422],\n",
      "        [0.4352],\n",
      "        [0.5116],\n",
      "        [0.3391],\n",
      "        [0.3991],\n",
      "        [0.3469],\n",
      "        [0.5071],\n",
      "        [0.4418],\n",
      "        [0.3618],\n",
      "        [0.4778],\n",
      "        [0.4834],\n",
      "        [0.4134],\n",
      "        [0.5142],\n",
      "        [0.4078],\n",
      "        [0.5485],\n",
      "        [0.4697],\n",
      "        [0.4286],\n",
      "        [0.4063],\n",
      "        [0.4008],\n",
      "        [0.4126],\n",
      "        [0.4619],\n",
      "        [0.4434],\n",
      "        [0.3623],\n",
      "        [0.4368],\n",
      "        [0.4553],\n",
      "        [0.4649],\n",
      "        [0.3367],\n",
      "        [0.4067],\n",
      "        [0.4736],\n",
      "        [0.3974],\n",
      "        [0.3991],\n",
      "        [0.3670],\n",
      "        [0.4846],\n",
      "        [0.5187],\n",
      "        [0.4397],\n",
      "        [0.5155],\n",
      "        [0.5657],\n",
      "        [0.3592],\n",
      "        [0.3817],\n",
      "        [0.4738],\n",
      "        [0.4633],\n",
      "        [0.4466],\n",
      "        [0.4300],\n",
      "        [0.4069],\n",
      "        [0.4145],\n",
      "        [0.4667],\n",
      "        [0.5183],\n",
      "        [0.4158],\n",
      "        [0.4915],\n",
      "        [0.4818],\n",
      "        [0.4286],\n",
      "        [0.4154],\n",
      "        [0.5428],\n",
      "        [0.5249],\n",
      "        [0.4447],\n",
      "        [0.3565],\n",
      "        [0.5580],\n",
      "        [0.3857],\n",
      "        [0.5254],\n",
      "        [0.5156],\n",
      "        [0.3961],\n",
      "        [0.4285],\n",
      "        [0.4109],\n",
      "        [0.4766],\n",
      "        [0.4014],\n",
      "        [0.3893],\n",
      "        [0.3709],\n",
      "        [0.4304],\n",
      "        [0.4244],\n",
      "        [0.4374],\n",
      "        [0.5196],\n",
      "        [0.3863],\n",
      "        [0.5378],\n",
      "        [0.4872],\n",
      "        [0.4459],\n",
      "        [0.4613],\n",
      "        [0.4140],\n",
      "        [0.4338],\n",
      "        [0.4518],\n",
      "        [0.3558],\n",
      "        [0.4396],\n",
      "        [0.4921],\n",
      "        [0.5075],\n",
      "        [0.4591],\n",
      "        [0.5384],\n",
      "        [0.4296],\n",
      "        [0.4959],\n",
      "        [0.4730],\n",
      "        [0.4079],\n",
      "        [0.4290],\n",
      "        [0.3747],\n",
      "        [0.4809],\n",
      "        [0.4163],\n",
      "        [0.4314],\n",
      "        [0.4203],\n",
      "        [0.4194],\n",
      "        [0.4634],\n",
      "        [0.4049],\n",
      "        [0.3745],\n",
      "        [0.4599],\n",
      "        [0.4128],\n",
      "        [0.3786],\n",
      "        [0.4404],\n",
      "        [0.3947],\n",
      "        [0.5205],\n",
      "        [0.3801],\n",
      "        [0.4675],\n",
      "        [0.4283],\n",
      "        [0.3935],\n",
      "        [0.5047],\n",
      "        [0.4473],\n",
      "        [0.4148],\n",
      "        [0.4183],\n",
      "        [0.3682],\n",
      "        [0.4473],\n",
      "        [0.5272],\n",
      "        [0.4070],\n",
      "        [0.4282],\n",
      "        [0.4530],\n",
      "        [0.4174],\n",
      "        [0.5081],\n",
      "        [0.3384],\n",
      "        [0.4517],\n",
      "        [0.4609],\n",
      "        [0.4252],\n",
      "        [0.4130],\n",
      "        [0.4161],\n",
      "        [0.4117],\n",
      "        [0.5173],\n",
      "        [0.3987],\n",
      "        [0.4984],\n",
      "        [0.4672],\n",
      "        [0.4018],\n",
      "        [0.4785],\n",
      "        [0.3975],\n",
      "        [0.4110],\n",
      "        [0.4472],\n",
      "        [0.5450],\n",
      "        [0.4580],\n",
      "        [0.4416],\n",
      "        [0.5443],\n",
      "        [0.3736],\n",
      "        [0.3838],\n",
      "        [0.5140],\n",
      "        [0.4542],\n",
      "        [0.3950],\n",
      "        [0.4822],\n",
      "        [0.4818],\n",
      "        [0.5267],\n",
      "        [0.4605],\n",
      "        [0.4908],\n",
      "        [0.4144],\n",
      "        [0.4112],\n",
      "        [0.4196],\n",
      "        [0.3631],\n",
      "        [0.4606],\n",
      "        [0.5238],\n",
      "        [0.4901],\n",
      "        [0.3854],\n",
      "        [0.4622],\n",
      "        [0.5226],\n",
      "        [0.4149],\n",
      "        [0.4897],\n",
      "        [0.4684],\n",
      "        [0.4520],\n",
      "        [0.5504],\n",
      "        [0.3968],\n",
      "        [0.4641],\n",
      "        [0.5164],\n",
      "        [0.4214],\n",
      "        [0.4374],\n",
      "        [0.4648],\n",
      "        [0.5507],\n",
      "        [0.4070],\n",
      "        [0.3583],\n",
      "        [0.3835],\n",
      "        [0.3916],\n",
      "        [0.4743],\n",
      "        [0.4462],\n",
      "        [0.4119],\n",
      "        [0.3680],\n",
      "        [0.4899],\n",
      "        [0.4667],\n",
      "        [0.3881],\n",
      "        [0.4480],\n",
      "        [0.4853],\n",
      "        [0.4677],\n",
      "        [0.4741],\n",
      "        [0.3884],\n",
      "        [0.4087],\n",
      "        [0.4150],\n",
      "        [0.4764],\n",
      "        [0.5196],\n",
      "        [0.4228],\n",
      "        [0.5034],\n",
      "        [0.3862],\n",
      "        [0.3350],\n",
      "        [0.4742],\n",
      "        [0.4475],\n",
      "        [0.4679],\n",
      "        [0.4875],\n",
      "        [0.4475],\n",
      "        [0.4757],\n",
      "        [0.4598],\n",
      "        [0.4227],\n",
      "        [0.4986],\n",
      "        [0.4396],\n",
      "        [0.4033],\n",
      "        [0.4162],\n",
      "        [0.4427],\n",
      "        [0.3968],\n",
      "        [0.3713],\n",
      "        [0.4508],\n",
      "        [0.4213],\n",
      "        [0.3732],\n",
      "        [0.5046],\n",
      "        [0.3923],\n",
      "        [0.5473],\n",
      "        [0.4688],\n",
      "        [0.4120],\n",
      "        [0.4065],\n",
      "        [0.3712],\n",
      "        [0.3731],\n",
      "        [0.4135],\n",
      "        [0.3964],\n",
      "        [0.3889],\n",
      "        [0.4653],\n",
      "        [0.4286],\n",
      "        [0.5085],\n",
      "        [0.3977],\n",
      "        [0.4005],\n",
      "        [0.4208],\n",
      "        [0.4546],\n",
      "        [0.5047],\n",
      "        [0.4540],\n",
      "        [0.4860],\n",
      "        [0.5468],\n",
      "        [0.4722],\n",
      "        [0.4819],\n",
      "        [0.4002],\n",
      "        [0.3578],\n",
      "        [0.4894],\n",
      "        [0.4849],\n",
      "        [0.4474],\n",
      "        [0.4093],\n",
      "        [0.5179],\n",
      "        [0.3733],\n",
      "        [0.4383],\n",
      "        [0.4483],\n",
      "        [0.4067],\n",
      "        [0.5155],\n",
      "        [0.4612],\n",
      "        [0.4486],\n",
      "        [0.4291],\n",
      "        [0.4948],\n",
      "        [0.4798],\n",
      "        [0.4158],\n",
      "        [0.4745],\n",
      "        [0.5040],\n",
      "        [0.4225],\n",
      "        [0.4377],\n",
      "        [0.4201],\n",
      "        [0.5177],\n",
      "        [0.4743],\n",
      "        [0.4107],\n",
      "        [0.4093],\n",
      "        [0.3866],\n",
      "        [0.4638],\n",
      "        [0.4263],\n",
      "        [0.4666],\n",
      "        [0.4253],\n",
      "        [0.4647],\n",
      "        [0.4017],\n",
      "        [0.4833],\n",
      "        [0.3578],\n",
      "        [0.4375],\n",
      "        [0.5073],\n",
      "        [0.4415],\n",
      "        [0.5139],\n",
      "        [0.5037],\n",
      "        [0.3800],\n",
      "        [0.5239],\n",
      "        [0.4737],\n",
      "        [0.4298],\n",
      "        [0.4714],\n",
      "        [0.4417],\n",
      "        [0.4318],\n",
      "        [0.4961],\n",
      "        [0.4011],\n",
      "        [0.5138],\n",
      "        [0.4065],\n",
      "        [0.4902],\n",
      "        [0.4438],\n",
      "        [0.4349],\n",
      "        [0.4633],\n",
      "        [0.4202],\n",
      "        [0.4422],\n",
      "        [0.4435],\n",
      "        [0.4272],\n",
      "        [0.4617],\n",
      "        [0.4550],\n",
      "        [0.4701],\n",
      "        [0.4205],\n",
      "        [0.4003],\n",
      "        [0.3806],\n",
      "        [0.4501],\n",
      "        [0.4230],\n",
      "        [0.4187],\n",
      "        [0.5005],\n",
      "        [0.4470],\n",
      "        [0.4280],\n",
      "        [0.4343],\n",
      "        [0.4887],\n",
      "        [0.4293],\n",
      "        [0.3741],\n",
      "        [0.4657],\n",
      "        [0.4079],\n",
      "        [0.4261],\n",
      "        [0.4771],\n",
      "        [0.4346],\n",
      "        [0.5324]], grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "num_covariates = 5  # Age, BP, Cholesterol\n",
    "embedding_dim = 40   # Embedding dimension for both covariates & treatments\n",
    "num_treatments = 2  # Assume 2 possible treatments\n",
    "\n",
    "# Initialize the model\n",
    "model = TransTEE(num_covariates, embedding_dim, num_treatments)\n",
    "\n",
    "# Perform forward pass (inference)\n",
    "predicted_outcome, e_x = model(X_train, w_train.int().squeeze())\n",
    "\n",
    "print(\"Predicted outcome:\", predicted_outcome)\n",
    "print(\"Predicted e_x:\", e_x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c35db85f-6ee7-4f5a-bd71-18174244ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss = 3.9088,          regression loss: 3.4922,         bce loss: 0.6812,         t_loss: 3.4845,         epsilon: 0.0010,         \n",
      "Epoch 10: Loss = 1.4173,          regression loss: 1.2264,         bce loss: 0.6696,         t_loss: 1.2395,         epsilon: 0.0043,         \n",
      "Epoch 20: Loss = 1.3869,          regression loss: 1.2010,         bce loss: 0.6582,         t_loss: 1.2009,         epsilon: 0.0034,         \n",
      "Epoch 30: Loss = 1.3797,          regression loss: 1.1949,         bce loss: 0.6475,         t_loss: 1.2004,         epsilon: 0.0025,         \n",
      "Epoch 40: Loss = 1.3806,          regression loss: 1.1981,         bce loss: 0.6375,         t_loss: 1.1880,         epsilon: 0.0003,         \n",
      "Epoch 50: Loss = 1.3708,          regression loss: 1.1897,         bce loss: 0.6282,         t_loss: 1.1828,         epsilon: -0.0025,         \n",
      "Epoch 60: Loss = 1.3357,          regression loss: 1.1568,         bce loss: 0.6197,         t_loss: 1.1691,         epsilon: -0.0053,         \n",
      "Epoch 70: Loss = 1.2581,          regression loss: 1.0876,         bce loss: 0.6118,         t_loss: 1.0936,         epsilon: -0.0084,         \n",
      "Epoch 80: Loss = 1.2349,          regression loss: 1.0692,         bce loss: 0.6045,         t_loss: 1.0520,         epsilon: -0.0097,         \n",
      "Epoch 90: Loss = 1.2049,          regression loss: 1.0406,         bce loss: 0.5981,         t_loss: 1.0456,         epsilon: -0.0081,         \n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "# hyper-praameter\n",
    "alpha = 0.1\n",
    "beta = 0.1\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    \n",
    "    y_pred, e_x = model(X_train, w_train.int().squeeze())  # Forward pass\n",
    "\n",
    "    y_0_pred, e_x = model(X_train, torch.zeros(X_train.shape[0]).int())\n",
    "    y_1_pred, e_x = model(X_train, torch.ones(X_train.shape[0]).int())\n",
    "    \n",
    "    regression_loss = loss_function(y_pred, y_train)  # Compute loss\n",
    "    bce_loss = make_binary_classification_loss(e_x, w_train)\n",
    "    vanila_loss = regression_loss + alpha * bce_loss\n",
    "    \n",
    "    t_loss = make_targeted_regularization_loss(e_x, y_0_pred, y_1_pred, y_train, w_train, model.epsilon)\n",
    "    \n",
    "    loss = vanila_loss + beta * t_loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f},  \\\n",
    "        regression loss: {regression_loss.item():.4f}, \\\n",
    "        bce loss: {bce_loss.item():.4f}, \\\n",
    "        t_loss: {t_loss.item():.4f}, \\\n",
    "        epsilon: {model.epsilon.item():.4f}, \\\n",
    "        \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d267d4a3-0fec-434f-a294-30c12cd36011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error in Treatment Effect Estimation: 0.1844\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y0_pred_test, e_x = model(X_test, torch.zeros(X_test.shape[0]).int())\n",
    "    y1_pred_test, e_x = model(X_test, torch.ones(X_test.shape[0]).int())\n",
    "\n",
    "    # Estimate Individual Treatment Effects (ITE)\n",
    "    tau_hat = (y1_pred_test - y0_pred_test).squeeze().numpy()\n",
    "\n",
    "    # Compute Mean Absolute Error\n",
    "    mae = np.mean(np.abs(tau_hat - tau_test.numpy()))\n",
    "    print(f\"Mean Absolute Error in Treatment Effect Estimation: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e89f7-3ed2-4799-b5d2-9120af70aa68",
   "metadata": {},
   "source": [
    "# Training transTEE on larger size of dataset leverage torch.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a8f6c43b-31d0-4549-8469-a7333e80a866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 0: Loss = 1.3980,              regression loss: 1.2113,             bce loss: 0.6565,             t_loss: 1.2113,             epsilon: 0.0107,             \n",
      "Epoch 1: Loss = 1.1742,              regression loss: 1.0114,             bce loss: 0.6237,             t_loss: 1.0050,             epsilon: 0.0058,             \n",
      "Epoch 1: Loss = 1.1413,              regression loss: 0.9819,             bce loss: 0.6151,             t_loss: 0.9787,             epsilon: 0.0046,             \n",
      "Epoch 2: Loss = 1.0814,              regression loss: 0.9330,             bce loss: 0.5637,             t_loss: 0.9204,             epsilon: 0.0035,             \n",
      "Epoch 3: Loss = 1.0034,              regression loss: 0.8621,             bce loss: 0.5449,             t_loss: 0.8678,             epsilon: 0.0062,             \n",
      "Epoch 3: Loss = 1.3048,              regression loss: 1.1374,             bce loss: 0.5457,             t_loss: 1.1275,             epsilon: 0.0050,             \n",
      "Epoch 4: Loss = 1.1281,              regression loss: 0.9726,             bce loss: 0.5643,             t_loss: 0.9908,             epsilon: 0.0069,             \n",
      "Epoch 5: Loss = 1.1712,              regression loss: 1.0153,             bce loss: 0.5628,             t_loss: 0.9958,             epsilon: 0.0051,             \n",
      "Epoch 5: Loss = 1.1845,              regression loss: 1.0280,             bce loss: 0.5442,             t_loss: 1.0205,             epsilon: 0.0031,             \n",
      "Epoch 6: Loss = 1.1535,              regression loss: 0.9993,             bce loss: 0.5371,             t_loss: 1.0046,             epsilon: 0.0021,             \n",
      "Epoch 6: Loss = 1.1847,              regression loss: 1.0280,             bce loss: 0.5408,             t_loss: 1.0265,             epsilon: 0.0015,             \n",
      "Epoch 7: Loss = 1.2870,              regression loss: 1.1193,             bce loss: 0.5560,             t_loss: 1.1207,             epsilon: 0.0011,             \n",
      "Epoch 8: Loss = 1.1537,              regression loss: 0.9979,             bce loss: 0.5669,             t_loss: 0.9915,             epsilon: -0.0018,             \n",
      "Epoch 8: Loss = 1.1308,              regression loss: 0.9799,             bce loss: 0.5292,             t_loss: 0.9798,             epsilon: 0.0020,             \n",
      "Epoch 9: Loss = 1.2059,              regression loss: 1.0455,             bce loss: 0.5658,             t_loss: 1.0374,             epsilon: 0.0029,             \n",
      "\n",
      "Mean Absolute Error for transTEE: 0.0742\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from causalml.dataset import synthetic_data\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load synthetic dataset with larger sample size\n",
    "y, X, w, tau, b, e = synthetic_data(mode=1, n=50000, p=10, sigma=1.0, adj=0.0)  # Increased dataset size\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test, w_train, w_test, tau_train, tau_test = train_test_split(\n",
    "    X, y, w, tau, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train, y_test = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1), torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "w_train, w_test = torch.tensor(w_train, dtype=torch.float32).unsqueeze(1), torch.tensor(w_test, dtype=torch.float32).unsqueeze(1)\n",
    "tau_train, tau_test = torch.tensor(tau_train, dtype=torch.float32), torch.tensor(tau_test, dtype=torch.float32)\n",
    "\n",
    "# --- PyTorch Dataset & DataLoader ---\n",
    "class CausalDataset(Dataset):\n",
    "    def __init__(self, X, y, w):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.w[idx]\n",
    "\n",
    "# Create data loaders for mini-batch training\n",
    "batch_size = 512\n",
    "train_dataset = CausalDataset(X_train, y_train, w_train)\n",
    "test_dataset = CausalDataset(X_test, y_test, w_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train.shape[1]\n",
    "model = TransTEE(num_covariates, embedding_dim, num_treatments).to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# --- Train DragonNet with Mini-batch Training ---\n",
    "num_epochs = 10\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_X, batch_y, batch_w in train_loader:\n",
    "        batch_X, batch_y, batch_w = batch_X.to(device), batch_y.to(device), batch_w.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        y_pred, e_x = model(batch_X, batch_w.int().squeeze())  # Forward pass\n",
    "        y_0_pred, e_x = model(batch_X, torch.zeros(batch_X.shape[0]).int())\n",
    "        y_1_pred, e_x = model(batch_X, torch.ones(batch_X.shape[0]).int())\n",
    "        \n",
    "        regression_loss = loss_function(y_pred, batch_y)  # Compute loss\n",
    "        bce_loss = make_binary_classification_loss(e_x, batch_w)\n",
    "        vanila_loss = regression_loss + alpha * bce_loss\n",
    "\n",
    "        t_loss = make_targeted_regularization_loss(e_x, y_0_pred, y_1_pred, batch_y, batch_w, model.epsilon)\n",
    "    \n",
    "        loss = vanila_loss + beta * t_loss\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.4f},  \\\n",
    "            regression loss: {regression_loss.item():.4f}, \\\n",
    "            bce loss: {bce_loss.item():.4f}, \\\n",
    "            t_loss: {t_loss.item():.4f}, \\\n",
    "            epsilon: {model.epsilon.item():.4f}, \\\n",
    "            \")\n",
    "\n",
    "\n",
    "# --- Evaluate transTEE ---\n",
    "model.eval()\n",
    "tau_hat = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, _, _ in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        y0_pred_test, e_x = model(batch_X, torch.zeros(batch_X.shape[0]).int())\n",
    "        y1_pred_test, e_x = model(batch_X, torch.ones(batch_X.shape[0]).int())\n",
    "        tau_hat.extend((y1_pred_test - y0_pred_test).cpu().numpy())\n",
    "\n",
    "tau_hat = np.array(tau_hat).flatten()\n",
    "\n",
    "# --- Compare with True Treatment Effects ---\n",
    "mae = mean_absolute_error(tau_test, tau_hat)\n",
    "print(f\"\\nMean Absolute Error for transTEE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "95b0e0b2-5e62-4cb0-a6ea-78b001c54c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525147\n"
     ]
    }
   ],
   "source": [
    "print(sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8a8bfe-a42b-4896-896e-88b8aea6b789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
