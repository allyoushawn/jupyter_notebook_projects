{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "587b627a-c885-47a9-991a-7dc506d42199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CovariateEmbedding(nn.Module):\n",
    "    def __init__(self, num_covariates, embedding_dim):\n",
    "        super(CovariateEmbedding, self).__init__()\n",
    "        self.embedding_layers = nn.ModuleList([\n",
    "            nn.Linear(1, embedding_dim) for _ in range(num_covariates)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, num_covariates)\n",
    "        Returns: Tensor of shape (batch_size, num_covariates, embedding_dim)\n",
    "        \"\"\"\n",
    "        embedded = [layer(x[:, i:i+1]) for i, layer in enumerate(self.embedding_layers)]\n",
    "        return torch.stack(embedded, dim=1)  # Shape: (batch, num_covariates, embedding_dim)\n",
    "\n",
    "\n",
    "class TreatmentEmbedding(nn.Module):\n",
    "    def __init__(self, num_treatments, embedding_dim):\n",
    "        super(TreatmentEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(num_treatments, embedding_dim)\n",
    "\n",
    "    def forward(self, t):\n",
    "        \"\"\"\n",
    "        t: Tensor of shape (batch_size,)\n",
    "        Returns: Tensor of shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        return self.embedding(t)\n",
    "\n",
    "\n",
    "class TransformerCovariateEncoder(nn.Module):\n",
    "    def __init__(self, num_covariates, embedding_dim, num_heads=4, num_layers=2):\n",
    "        super(TransformerCovariateEncoder, self).__init__()\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, num_covariates, embedding_dim)\n",
    "        Returns: Tensor of shape (batch_size, num_covariates, embedding_dim)\n",
    "        \"\"\"\n",
    "        x = x.permute(1, 0, 2)  # Transformers expect (seq_len, batch, dim)\n",
    "        x = self.transformer(x)\n",
    "        return x.permute(1, 0, 2)  # Convert back to (batch_size, num_covariates, embedding_dim)\n",
    "\n",
    "\n",
    "class TreatmentCovariateCrossAttention(nn.Module):\n",
    "    def __init__(self, embedding_dim, num_heads=4, num_layers=1):\n",
    "        super(TreatmentCovariateCrossAttention, self).__init__()\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embedding_dim, nhead=num_heads)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "    def forward(self, covariate_embeddings, treatment_embeddings):\n",
    "        \"\"\"\n",
    "        covariate_embeddings: (batch_size, num_covariates, embedding_dim) -> Acts as \"memory\" (key & value)\n",
    "        treatment_embeddings: (batch_size, embedding_dim) -> Acts as \"query\"\n",
    "        \n",
    "        Returns: (batch_size, num_covariates, embedding_dim) - Updated covariate representation\n",
    "        \"\"\"\n",
    "        # Expand treatment embeddings to match covariates\n",
    "        treatment_embeddings = treatment_embeddings.unsqueeze(1)  # Shape (batch_size, 1, embedding_dim)\n",
    "\n",
    "        # TransformerDecoder requires (seq_len, batch, dim) format\n",
    "        memory = covariate_embeddings.permute(1, 0, 2)  # (num_covariates, batch, embedding_dim)\n",
    "        query = treatment_embeddings.permute(1, 0, 2)  # (1, batch, embedding_dim)\n",
    "\n",
    "        # Apply Transformer Decoder (cross-attention)\n",
    "        updated_covariates = self.transformer_decoder(query, memory)  # Shape: (1, batch, embedding_dim)\n",
    "\n",
    "        return updated_covariates.permute(1, 0, 2)  # Convert back to (batch_size, 1, embedding_dim)\n",
    "\n",
    "\n",
    "class OutcomePrediction(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(OutcomePrediction, self).__init__()\n",
    "        self.fc = nn.Linear(embedding_dim, 1)  # Final regression layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: Tensor of shape (batch_size, 1, embedding_dim)\n",
    "        Returns: (batch_size, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten before prediction\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "class TransTEE(nn.Module):\n",
    "    def __init__(self, num_covariates, embedding_dim, num_treatments):\n",
    "        super(TransTEE, self).__init__()\n",
    "        self.covariate_embedding = CovariateEmbedding(num_covariates, embedding_dim)\n",
    "        self.treatment_embedding = TreatmentEmbedding(num_treatments, embedding_dim)\n",
    "        self.covariate_encoder = TransformerCovariateEncoder(num_covariates, embedding_dim)\n",
    "        self.cross_attention = TreatmentCovariateCrossAttention(embedding_dim)\n",
    "        self.outcome_predictor = OutcomePrediction(embedding_dim)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        \"\"\"\n",
    "        x: Covariates (batch_size, num_covariates)\n",
    "        t: Treatments (batch_size,)\n",
    "\n",
    "        Returns: Estimated outcome (batch_size, 1)\n",
    "        \"\"\"\n",
    "        x = self.covariate_embedding(x)  # Encode covariates\n",
    "        t = self.treatment_embedding(t)  # Encode treatment\n",
    "        x = self.covariate_encoder(x)  # Self-attention on covariates\n",
    "        x = self.cross_attention(x, t)  # Treatment-covariate interactions\n",
    "        y_pred = self.outcome_predictor(x)  # Final outcome prediction\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "596e082b-9f8e-40f8-a020-7091ebfe7e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  blood_pressure  cholesterol  treatment  outcome\n",
      "0   55             140          200          1      120\n",
      "1   40             130          180          2      125\n",
      "2   60             150          220          3      145\n",
      "Covariates shape: torch.Size([3, 3])\n",
      "Treatment shape: torch.Size([3])\n",
      "Outcome shape: torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example dataset\n",
    "data = pd.DataFrame({\n",
    "    'age': [55, 40, 60],\n",
    "    'blood_pressure': [140, 130, 150],\n",
    "    'cholesterol': [200, 180, 220],\n",
    "    'treatment': [1, 2, 3],  # Treatment as categorical variable\n",
    "    'outcome': [120, 125, 145]  # Observed outcome (only needed for training)\n",
    "})\n",
    "\n",
    "print(data)\n",
    "\n",
    "import torch\n",
    "\n",
    "def dataframe_to_tensors(df):\n",
    "    \"\"\"\n",
    "    Convert a Pandas DataFrame into PyTorch tensors.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Input DataFrame with covariates, treatments, and optionally outcomes.\n",
    "\n",
    "    Returns:\n",
    "    covariates_tensor (torch.Tensor): Shape (batch_size, num_covariates)\n",
    "    treatment_tensor (torch.Tensor): Shape (batch_size,)\n",
    "    outcome_tensor (torch.Tensor or None): Shape (batch_size, 1) if available, else None\n",
    "    \"\"\"\n",
    "    # Convert continuous covariates to float tensor\n",
    "    covariates = torch.tensor(df.iloc[:, :-2].values, dtype=torch.float32)  # All except last 2 cols\n",
    "    # Convert treatment to integer tensor\n",
    "    treatment = torch.tensor(df['treatment'].values, dtype=torch.long)  # Long tensor for embedding lookup\n",
    "    # Convert outcome if available\n",
    "    outcome = torch.tensor(df['outcome'].values, dtype=torch.float32).unsqueeze(1) if 'outcome' in df else None\n",
    "    \n",
    "    return covariates, treatment, outcome\n",
    "\n",
    "# Convert DataFrame\n",
    "covariates_tensor, treatment_tensor, outcome_tensor = dataframe_to_tensors(data)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Covariates shape:\", covariates_tensor.shape)  # Expected: (batch_size, num_covariates)\n",
    "print(\"Treatment shape:\", treatment_tensor.shape)  # Expected: (batch_size,)\n",
    "print(\"Outcome shape:\", outcome_tensor.shape)  # Expected: (batch_size, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0a0c5f0-2b0f-43b7-9f93-9f9a8a560bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted outcome: tensor([[-1.6538],\n",
      "        [-0.3132],\n",
      "        [ 1.1677]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "num_covariates = 3  # Age, BP, Cholesterol\n",
    "embedding_dim = 8   # Embedding dimension for both covariates & treatments\n",
    "num_treatments = 5  # Assume 5 possible treatments\n",
    "\n",
    "# Initialize the model\n",
    "model = TransTEE(num_covariates, embedding_dim, num_treatments)\n",
    "\n",
    "# Perform forward pass (inference)\n",
    "predicted_outcome = model(covariates_tensor, treatment_tensor)\n",
    "\n",
    "print(\"Predicted outcome:\", predicted_outcome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1bc6fc0d-2e9c-4638-b2da-ba3a7df4e66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 17089.8926\n",
      "Epoch 10, Loss: 16571.9609\n",
      "Epoch 20, Loss: 16542.5371\n",
      "Epoch 30, Loss: 16476.1855\n",
      "Epoch 40, Loss: 16453.0371\n",
      "Epoch 50, Loss: 16405.7246\n",
      "Epoch 60, Loss: 16397.1191\n",
      "Epoch 70, Loss: 16333.0781\n",
      "Epoch 80, Loss: 16326.8389\n",
      "Epoch 90, Loss: 16271.6143\n"
     ]
    }
   ],
   "source": [
    "# Define loss function (Mean Squared Error for regression)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop (one epoch for example)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(covariates_tensor, treatment_tensor)  # Forward pass\n",
    "    loss = loss_function(y_pred, outcome_tensor)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "572f28fa-b0f8-4d3d-be91-b5aa7aeff4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted outcome: tensor([[5.7516],\n",
      "        [5.7539],\n",
      "        [5.6316]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Perform forward pass (inference)\n",
    "predicted_outcome = model(covariates_tensor, treatment_tensor)\n",
    "\n",
    "print(\"Predicted outcome:\", predicted_outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "98830dff-3f1d-4cc8-8052-865ca126cfe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: torch.Size([800, 5]), y_train shape: torch.Size([800, 1]), w_train shape: torch.Size([800, 1])\n",
      "tau_train shape: torch.Size([800])\n"
     ]
    }
   ],
   "source": [
    "# Use Dragon Net input data frame\n",
    "from causalml.dataset import synthetic_data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load synthetic dataset using updated API\n",
    "y, X, w, tau, b, e = synthetic_data(mode=1, n=1000, p=5, sigma=1.0, adj=0.0)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test, w_train, w_test, tau_train, tau_test = train_test_split(\n",
    "    X, y, w, tau, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train, y_test = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1), torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "w_train, w_test = torch.tensor(w_train, dtype=torch.float32).unsqueeze(1), torch.tensor(w_test, dtype=torch.float32).unsqueeze(1)\n",
    "tau_train, tau_test = torch.tensor(tau_train, dtype=torch.float32), torch.tensor(tau_test, dtype=torch.float32)\n",
    "\n",
    "# Print dataset shapes to verify\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}, w_train shape: {w_train.shape}\")\n",
    "print(f\"tau_train shape: {tau_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88891558-45e0-4877-abc2-5f404ba59581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted outcome: tensor([[ 1.0359e+00],\n",
      "        [ 2.9412e-01],\n",
      "        [ 2.3322e-01],\n",
      "        [ 4.3096e-01],\n",
      "        [ 7.5649e-01],\n",
      "        [ 1.0054e+00],\n",
      "        [ 9.5990e-01],\n",
      "        [ 8.9026e-01],\n",
      "        [ 5.0006e-01],\n",
      "        [ 2.8021e-01],\n",
      "        [ 1.3730e-01],\n",
      "        [ 9.9433e-02],\n",
      "        [ 1.0567e+00],\n",
      "        [ 4.2498e-01],\n",
      "        [ 3.2436e-01],\n",
      "        [ 2.7496e-01],\n",
      "        [ 1.5074e-01],\n",
      "        [ 8.6410e-01],\n",
      "        [ 3.0293e-01],\n",
      "        [ 5.2863e-01],\n",
      "        [ 1.1586e+00],\n",
      "        [ 6.8384e-01],\n",
      "        [ 8.1149e-01],\n",
      "        [ 7.5807e-01],\n",
      "        [ 8.1780e-01],\n",
      "        [ 7.3987e-01],\n",
      "        [ 2.3601e-01],\n",
      "        [ 6.0317e-01],\n",
      "        [ 2.1394e-01],\n",
      "        [ 6.2874e-01],\n",
      "        [ 2.7720e-01],\n",
      "        [ 3.5176e-01],\n",
      "        [ 1.5916e-01],\n",
      "        [ 8.6406e-01],\n",
      "        [ 2.4870e-01],\n",
      "        [ 4.0038e-02],\n",
      "        [ 4.1144e-01],\n",
      "        [ 1.0755e+00],\n",
      "        [ 4.7650e-02],\n",
      "        [ 1.9686e-01],\n",
      "        [ 1.4573e-01],\n",
      "        [ 5.9275e-02],\n",
      "        [ 6.1628e-02],\n",
      "        [ 1.1367e+00],\n",
      "        [ 1.0002e+00],\n",
      "        [ 5.8916e-01],\n",
      "        [ 1.0555e+00],\n",
      "        [ 8.8207e-01],\n",
      "        [ 2.7536e-01],\n",
      "        [ 7.1163e-01],\n",
      "        [ 9.1804e-01],\n",
      "        [ 6.5293e-01],\n",
      "        [ 7.4530e-01],\n",
      "        [ 2.0966e-01],\n",
      "        [ 1.0468e+00],\n",
      "        [ 2.0840e-01],\n",
      "        [ 8.5448e-01],\n",
      "        [ 8.8575e-01],\n",
      "        [ 8.1802e-01],\n",
      "        [ 1.7778e-01],\n",
      "        [ 1.5920e-01],\n",
      "        [ 2.4561e-01],\n",
      "        [ 9.0549e-01],\n",
      "        [ 2.0785e-01],\n",
      "        [ 9.3004e-01],\n",
      "        [ 2.4127e-01],\n",
      "        [ 3.4518e-01],\n",
      "        [ 3.5884e-01],\n",
      "        [ 1.0866e-01],\n",
      "        [ 3.5073e-01],\n",
      "        [ 2.7534e-01],\n",
      "        [ 1.1879e+00],\n",
      "        [ 9.6880e-01],\n",
      "        [-3.5279e-02],\n",
      "        [ 9.4815e-01],\n",
      "        [ 2.4400e-01],\n",
      "        [ 9.2273e-01],\n",
      "        [-2.6045e-02],\n",
      "        [-7.8754e-03],\n",
      "        [ 3.4344e-01],\n",
      "        [ 8.6255e-01],\n",
      "        [ 8.7130e-01],\n",
      "        [ 2.7746e-01],\n",
      "        [ 2.9736e-01],\n",
      "        [ 1.5834e-01],\n",
      "        [ 2.2308e-01],\n",
      "        [ 8.4460e-01],\n",
      "        [ 1.6905e-01],\n",
      "        [ 2.2477e-01],\n",
      "        [ 9.3811e-01],\n",
      "        [ 9.1840e-01],\n",
      "        [ 1.5223e-01],\n",
      "        [ 8.2961e-01],\n",
      "        [-1.2310e-01],\n",
      "        [ 4.7023e-01],\n",
      "        [ 6.9289e-01],\n",
      "        [ 2.4958e-01],\n",
      "        [ 7.6798e-01],\n",
      "        [ 1.1514e-01],\n",
      "        [ 9.6923e-01],\n",
      "        [ 3.3621e-01],\n",
      "        [ 9.4736e-01],\n",
      "        [ 9.7376e-01],\n",
      "        [ 2.0861e-01],\n",
      "        [ 4.1401e-01],\n",
      "        [ 7.9418e-01],\n",
      "        [ 2.5588e-01],\n",
      "        [ 8.4717e-01],\n",
      "        [ 3.4331e-01],\n",
      "        [ 5.0211e-01],\n",
      "        [ 9.8852e-01],\n",
      "        [ 7.9073e-01],\n",
      "        [ 6.8554e-01],\n",
      "        [-3.3564e-02],\n",
      "        [ 2.0798e-01],\n",
      "        [ 2.4267e-01],\n",
      "        [ 9.5369e-01],\n",
      "        [ 1.2957e-02],\n",
      "        [ 5.7778e-01],\n",
      "        [ 6.9693e-01],\n",
      "        [ 6.4722e-01],\n",
      "        [ 9.8720e-03],\n",
      "        [ 1.9672e-01],\n",
      "        [ 8.3474e-01],\n",
      "        [ 2.0558e-01],\n",
      "        [ 8.2099e-01],\n",
      "        [ 6.2975e-02],\n",
      "        [ 9.1141e-01],\n",
      "        [-6.1195e-03],\n",
      "        [ 8.2888e-01],\n",
      "        [ 5.4937e-01],\n",
      "        [ 1.0319e-01],\n",
      "        [ 8.4315e-01],\n",
      "        [ 2.2584e-01],\n",
      "        [ 3.1259e-01],\n",
      "        [ 6.7579e-01],\n",
      "        [ 9.9736e-01],\n",
      "        [ 6.1090e-01],\n",
      "        [ 8.4020e-02],\n",
      "        [ 4.1516e-01],\n",
      "        [ 5.5397e-01],\n",
      "        [ 7.9521e-01],\n",
      "        [ 6.7176e-02],\n",
      "        [ 1.4775e-01],\n",
      "        [ 9.9381e-01],\n",
      "        [ 5.0616e-01],\n",
      "        [ 1.4233e-02],\n",
      "        [ 7.6477e-01],\n",
      "        [ 8.3711e-01],\n",
      "        [ 7.9078e-01],\n",
      "        [ 6.5887e-01],\n",
      "        [ 1.4495e-01],\n",
      "        [ 9.9517e-01],\n",
      "        [ 8.6144e-01],\n",
      "        [ 2.3412e-01],\n",
      "        [ 6.8366e-01],\n",
      "        [ 6.4967e-01],\n",
      "        [ 1.7002e-01],\n",
      "        [ 1.3074e-01],\n",
      "        [ 7.5172e-01],\n",
      "        [ 8.8191e-01],\n",
      "        [ 9.1167e-01],\n",
      "        [ 9.3999e-01],\n",
      "        [ 3.3429e-01],\n",
      "        [ 7.0817e-02],\n",
      "        [ 9.6587e-01],\n",
      "        [ 2.1931e-01],\n",
      "        [ 2.7839e-01],\n",
      "        [ 9.4594e-01],\n",
      "        [ 6.5083e-01],\n",
      "        [ 3.1991e-01],\n",
      "        [ 7.6162e-01],\n",
      "        [ 9.7866e-01],\n",
      "        [ 6.4058e-01],\n",
      "        [ 2.2863e-01],\n",
      "        [ 4.2336e-01],\n",
      "        [ 2.1980e-01],\n",
      "        [ 5.8164e-02],\n",
      "        [ 1.0271e+00],\n",
      "        [ 1.0681e+00],\n",
      "        [ 6.9969e-01],\n",
      "        [ 4.6161e-02],\n",
      "        [ 6.9384e-01],\n",
      "        [ 9.0892e-01],\n",
      "        [ 3.0200e-02],\n",
      "        [ 2.3341e-02],\n",
      "        [ 1.9430e-01],\n",
      "        [ 9.3298e-01],\n",
      "        [ 5.6090e-01],\n",
      "        [ 5.6445e-01],\n",
      "        [ 9.0390e-01],\n",
      "        [ 9.5476e-01],\n",
      "        [ 9.7676e-01],\n",
      "        [ 1.3889e-02],\n",
      "        [ 2.4196e-01],\n",
      "        [ 6.9949e-01],\n",
      "        [ 8.3598e-01],\n",
      "        [ 2.9772e-01],\n",
      "        [ 9.7781e-01],\n",
      "        [ 3.1833e-01],\n",
      "        [ 6.7369e-01],\n",
      "        [ 6.0630e-01],\n",
      "        [ 2.8529e-01],\n",
      "        [ 1.5778e-01],\n",
      "        [ 7.0218e-01],\n",
      "        [ 8.0148e-01],\n",
      "        [ 2.7424e-01],\n",
      "        [ 2.5006e-01],\n",
      "        [ 2.2221e-01],\n",
      "        [ 1.2501e-01],\n",
      "        [ 7.2105e-01],\n",
      "        [ 6.8777e-01],\n",
      "        [ 1.4181e-01],\n",
      "        [ 8.0937e-01],\n",
      "        [ 1.1692e+00],\n",
      "        [ 7.1416e-01],\n",
      "        [ 7.0705e-01],\n",
      "        [ 7.1308e-01],\n",
      "        [ 4.7029e-01],\n",
      "        [ 7.4397e-01],\n",
      "        [ 4.9379e-01],\n",
      "        [ 1.7924e-01],\n",
      "        [ 3.5188e-01],\n",
      "        [ 1.0905e+00],\n",
      "        [ 2.3870e-01],\n",
      "        [ 1.1943e-01],\n",
      "        [ 9.5155e-01],\n",
      "        [ 9.3071e-01],\n",
      "        [ 2.9171e-01],\n",
      "        [ 7.2494e-01],\n",
      "        [ 8.8786e-01],\n",
      "        [ 8.0241e-01],\n",
      "        [ 4.6973e-01],\n",
      "        [ 4.2986e-01],\n",
      "        [ 1.1838e-01],\n",
      "        [ 1.7371e-01],\n",
      "        [ 6.9671e-01],\n",
      "        [ 2.6534e-01],\n",
      "        [ 9.0743e-01],\n",
      "        [ 6.7113e-01],\n",
      "        [ 5.9458e-01],\n",
      "        [ 7.3765e-01],\n",
      "        [ 5.7982e-02],\n",
      "        [ 9.1880e-01],\n",
      "        [ 2.6296e-01],\n",
      "        [ 3.7372e-01],\n",
      "        [ 2.3092e-01],\n",
      "        [ 1.1979e-01],\n",
      "        [ 3.0150e-01],\n",
      "        [ 6.2586e-01],\n",
      "        [ 8.6608e-01],\n",
      "        [ 7.8266e-01],\n",
      "        [ 2.0662e-01],\n",
      "        [ 2.6375e-01],\n",
      "        [ 1.4505e-01],\n",
      "        [ 8.1490e-01],\n",
      "        [ 1.5213e-01],\n",
      "        [ 8.9510e-01],\n",
      "        [ 4.9400e-02],\n",
      "        [ 2.9861e-01],\n",
      "        [ 7.5432e-01],\n",
      "        [ 3.7122e-01],\n",
      "        [ 8.3019e-01],\n",
      "        [ 9.0037e-01],\n",
      "        [ 2.1706e-01],\n",
      "        [-1.4657e-01],\n",
      "        [ 8.3942e-01],\n",
      "        [ 7.2452e-01],\n",
      "        [ 8.5663e-01],\n",
      "        [-6.5451e-02],\n",
      "        [-5.2808e-02],\n",
      "        [ 1.0843e-01],\n",
      "        [ 2.4114e-01],\n",
      "        [ 2.5095e-01],\n",
      "        [ 5.5214e-03],\n",
      "        [ 7.6098e-01],\n",
      "        [ 7.6336e-01],\n",
      "        [ 6.5090e-01],\n",
      "        [ 9.8425e-01],\n",
      "        [ 8.4566e-01],\n",
      "        [ 5.9470e-01],\n",
      "        [ 6.8305e-01],\n",
      "        [ 7.9358e-01],\n",
      "        [ 8.8542e-01],\n",
      "        [ 2.6929e-01],\n",
      "        [ 1.0699e+00],\n",
      "        [ 1.1424e+00],\n",
      "        [ 3.1723e-01],\n",
      "        [ 2.0580e-01],\n",
      "        [ 8.6136e-01],\n",
      "        [ 8.1589e-01],\n",
      "        [ 7.6380e-01],\n",
      "        [ 9.1509e-01],\n",
      "        [-1.0935e-01],\n",
      "        [ 3.9938e-01],\n",
      "        [ 6.1484e-02],\n",
      "        [ 1.4012e-01],\n",
      "        [ 2.9147e-01],\n",
      "        [ 6.3122e-01],\n",
      "        [ 3.4119e-01],\n",
      "        [ 1.0859e-01],\n",
      "        [ 8.7140e-02],\n",
      "        [ 3.7996e-01],\n",
      "        [ 1.8423e-01],\n",
      "        [ 6.0706e-01],\n",
      "        [ 1.8501e-01],\n",
      "        [ 1.9147e-02],\n",
      "        [ 6.4891e-01],\n",
      "        [ 6.5082e-01],\n",
      "        [ 7.8594e-01],\n",
      "        [ 5.2008e-02],\n",
      "        [ 6.8713e-02],\n",
      "        [ 8.6622e-01],\n",
      "        [ 7.9975e-01],\n",
      "        [ 8.9187e-01],\n",
      "        [ 3.6857e-01],\n",
      "        [ 8.1702e-01],\n",
      "        [ 1.6826e-01],\n",
      "        [ 8.5058e-01],\n",
      "        [ 2.3401e-01],\n",
      "        [ 2.1308e-01],\n",
      "        [ 1.1521e+00],\n",
      "        [ 2.1646e-01],\n",
      "        [ 8.4922e-01],\n",
      "        [ 9.2584e-01],\n",
      "        [ 1.1272e-01],\n",
      "        [ 9.2110e-01],\n",
      "        [ 8.7405e-01],\n",
      "        [ 9.9454e-01],\n",
      "        [ 5.1627e-01],\n",
      "        [ 8.6261e-01],\n",
      "        [ 7.8918e-01],\n",
      "        [ 7.6361e-01],\n",
      "        [ 2.7113e-01],\n",
      "        [ 3.7973e-01],\n",
      "        [ 6.6133e-01],\n",
      "        [ 6.3336e-01],\n",
      "        [ 2.7490e-01],\n",
      "        [ 1.0709e+00],\n",
      "        [ 3.3169e-02],\n",
      "        [ 9.9352e-02],\n",
      "        [ 4.6379e-02],\n",
      "        [ 1.0162e+00],\n",
      "        [ 9.3997e-01],\n",
      "        [ 1.8238e-01],\n",
      "        [ 6.3254e-01],\n",
      "        [ 9.1442e-01],\n",
      "        [ 1.5056e-01],\n",
      "        [ 8.3024e-01],\n",
      "        [ 8.7821e-01],\n",
      "        [ 1.8514e-01],\n",
      "        [ 8.4694e-01],\n",
      "        [ 3.4339e-01],\n",
      "        [ 6.0412e-01],\n",
      "        [ 3.4901e-01],\n",
      "        [ 8.6516e-01],\n",
      "        [ 1.0847e-01],\n",
      "        [ 7.6365e-01],\n",
      "        [ 8.7228e-01],\n",
      "        [ 4.5512e-01],\n",
      "        [ 6.5658e-01],\n",
      "        [ 3.3262e-01],\n",
      "        [ 7.7982e-01],\n",
      "        [ 2.7024e-01],\n",
      "        [ 9.1850e-01],\n",
      "        [ 3.5542e-01],\n",
      "        [ 7.8208e-01],\n",
      "        [ 9.1674e-01],\n",
      "        [ 7.6812e-01],\n",
      "        [ 8.3394e-01],\n",
      "        [ 1.4352e-01],\n",
      "        [ 9.2952e-02],\n",
      "        [ 1.2489e+00],\n",
      "        [ 4.0123e-01],\n",
      "        [ 7.0654e-01],\n",
      "        [ 8.3042e-01],\n",
      "        [ 1.1293e-01],\n",
      "        [ 2.8100e-01],\n",
      "        [ 3.4711e-01],\n",
      "        [ 1.0638e-01],\n",
      "        [ 8.5170e-01],\n",
      "        [ 2.2535e-01],\n",
      "        [ 3.9855e-01],\n",
      "        [ 2.3208e-01],\n",
      "        [ 4.2317e-01],\n",
      "        [ 8.9889e-01],\n",
      "        [ 3.4277e-01],\n",
      "        [ 1.0233e+00],\n",
      "        [ 1.6284e-01],\n",
      "        [ 5.3962e-01],\n",
      "        [ 1.5074e-01],\n",
      "        [ 9.3832e-01],\n",
      "        [ 4.7911e-01],\n",
      "        [ 3.0634e-01],\n",
      "        [ 9.4725e-01],\n",
      "        [ 4.5199e-01],\n",
      "        [ 8.7694e-01],\n",
      "        [ 6.8797e-02],\n",
      "        [ 8.9455e-01],\n",
      "        [ 7.5044e-01],\n",
      "        [-9.3362e-03],\n",
      "        [ 1.0186e+00],\n",
      "        [ 8.2599e-01],\n",
      "        [ 9.4330e-01],\n",
      "        [ 1.0897e+00],\n",
      "        [ 3.0566e-01],\n",
      "        [ 1.1479e+00],\n",
      "        [ 4.0652e-01],\n",
      "        [ 8.2597e-01],\n",
      "        [ 1.6302e-01],\n",
      "        [ 1.4836e-01],\n",
      "        [-1.8769e-01],\n",
      "        [ 1.1426e+00],\n",
      "        [ 1.1717e-01],\n",
      "        [ 1.0420e+00],\n",
      "        [ 7.3010e-01],\n",
      "        [ 8.5656e-01],\n",
      "        [ 6.7326e-01],\n",
      "        [ 7.9804e-01],\n",
      "        [ 5.1582e-01],\n",
      "        [-9.7773e-02],\n",
      "        [ 1.6256e-01],\n",
      "        [ 1.0110e+00],\n",
      "        [ 3.4481e-01],\n",
      "        [ 2.5547e-01],\n",
      "        [ 2.6431e-01],\n",
      "        [-3.2373e-02],\n",
      "        [ 1.5864e-01],\n",
      "        [ 2.8162e-01],\n",
      "        [ 7.1222e-01],\n",
      "        [ 8.5412e-01],\n",
      "        [ 1.1765e-01],\n",
      "        [ 1.8940e-01],\n",
      "        [ 1.1157e-01],\n",
      "        [ 2.8254e-01],\n",
      "        [ 5.6089e-01],\n",
      "        [ 2.5453e-01],\n",
      "        [ 9.2811e-01],\n",
      "        [ 2.1871e-01],\n",
      "        [ 2.6362e-01],\n",
      "        [ 1.6966e-01],\n",
      "        [ 8.6505e-01],\n",
      "        [ 8.0458e-01],\n",
      "        [-2.0167e-02],\n",
      "        [ 1.1059e-01],\n",
      "        [ 8.3400e-01],\n",
      "        [ 6.7191e-01],\n",
      "        [-1.5413e-01],\n",
      "        [ 2.4184e-01],\n",
      "        [ 1.7789e-01],\n",
      "        [ 7.3311e-01],\n",
      "        [ 7.6101e-01],\n",
      "        [-6.7535e-02],\n",
      "        [ 2.2732e-01],\n",
      "        [ 2.3506e-01],\n",
      "        [ 8.7574e-01],\n",
      "        [ 1.0282e-01],\n",
      "        [ 4.1590e-01],\n",
      "        [ 8.9669e-01],\n",
      "        [ 3.1562e-01],\n",
      "        [ 7.6051e-01],\n",
      "        [-2.7519e-02],\n",
      "        [ 6.2495e-01],\n",
      "        [ 2.8441e-01],\n",
      "        [ 8.1950e-01],\n",
      "        [ 9.8850e-01],\n",
      "        [ 9.0506e-01],\n",
      "        [ 7.4209e-01],\n",
      "        [ 9.9311e-01],\n",
      "        [ 7.2933e-01],\n",
      "        [ 9.9607e-01],\n",
      "        [ 5.4238e-01],\n",
      "        [ 1.1289e-01],\n",
      "        [ 8.7332e-01],\n",
      "        [ 7.5204e-01],\n",
      "        [ 6.5644e-01],\n",
      "        [ 1.5661e-01],\n",
      "        [ 8.8522e-01],\n",
      "        [-1.0932e-01],\n",
      "        [ 1.3069e+00],\n",
      "        [ 1.7111e-01],\n",
      "        [ 7.3625e-01],\n",
      "        [ 1.5016e-01],\n",
      "        [ 9.7133e-01],\n",
      "        [ 6.4084e-02],\n",
      "        [ 8.5605e-01],\n",
      "        [ 3.0433e-01],\n",
      "        [ 8.8120e-01],\n",
      "        [ 3.2162e-01],\n",
      "        [ 4.9545e-01],\n",
      "        [ 3.2946e-01],\n",
      "        [ 8.6245e-01],\n",
      "        [ 1.4651e-01],\n",
      "        [ 1.2940e-01],\n",
      "        [ 7.2779e-01],\n",
      "        [ 8.0779e-02],\n",
      "        [ 8.9668e-01],\n",
      "        [ 1.2103e+00],\n",
      "        [-3.5592e-02],\n",
      "        [ 6.6911e-02],\n",
      "        [ 7.3839e-01],\n",
      "        [ 2.5142e-01],\n",
      "        [ 8.7467e-01],\n",
      "        [ 3.0951e-01],\n",
      "        [ 2.2763e-01],\n",
      "        [ 1.6574e-01],\n",
      "        [ 1.3365e-01],\n",
      "        [ 2.2386e-01],\n",
      "        [ 7.7961e-01],\n",
      "        [-6.5090e-03],\n",
      "        [ 4.6392e-01],\n",
      "        [ 8.1967e-02],\n",
      "        [ 2.9329e-01],\n",
      "        [ 4.0314e-01],\n",
      "        [ 1.3782e-01],\n",
      "        [ 1.2450e-01],\n",
      "        [ 8.4036e-02],\n",
      "        [ 1.0449e+00],\n",
      "        [ 9.5604e-01],\n",
      "        [ 8.8013e-01],\n",
      "        [ 2.5327e-01],\n",
      "        [ 7.0605e-01],\n",
      "        [ 8.3420e-01],\n",
      "        [ 7.1778e-01],\n",
      "        [ 7.1294e-01],\n",
      "        [ 8.2138e-01],\n",
      "        [ 1.6542e-01],\n",
      "        [ 8.4943e-01],\n",
      "        [ 8.1793e-01],\n",
      "        [ 2.6569e-01],\n",
      "        [ 1.0191e-01],\n",
      "        [ 1.0129e+00],\n",
      "        [ 6.9953e-01],\n",
      "        [ 1.7749e-01],\n",
      "        [ 2.1946e-01],\n",
      "        [ 1.8958e-01],\n",
      "        [ 8.2064e-01],\n",
      "        [ 4.8026e-01],\n",
      "        [ 1.9649e-01],\n",
      "        [ 1.0575e+00],\n",
      "        [ 7.1672e-01],\n",
      "        [ 2.4919e-01],\n",
      "        [ 7.4376e-01],\n",
      "        [ 7.3292e-01],\n",
      "        [ 5.9721e-01],\n",
      "        [ 9.1458e-01],\n",
      "        [ 1.9736e-01],\n",
      "        [ 2.6571e-01],\n",
      "        [ 2.0341e-01],\n",
      "        [ 6.1551e-01],\n",
      "        [ 5.4103e-01],\n",
      "        [ 6.7818e-01],\n",
      "        [ 8.3157e-01],\n",
      "        [ 7.6951e-01],\n",
      "        [ 7.2681e-01],\n",
      "        [ 1.6112e-01],\n",
      "        [ 9.2449e-01],\n",
      "        [ 7.2835e-01],\n",
      "        [ 7.4779e-01],\n",
      "        [ 2.9295e-01],\n",
      "        [ 3.4495e-01],\n",
      "        [ 2.7698e-02],\n",
      "        [ 9.2667e-01],\n",
      "        [ 7.5219e-01],\n",
      "        [ 6.4822e-01],\n",
      "        [ 1.0823e+00],\n",
      "        [ 1.0362e-01],\n",
      "        [ 1.0184e+00],\n",
      "        [ 9.0877e-01],\n",
      "        [ 3.1456e-01],\n",
      "        [ 7.7306e-01],\n",
      "        [ 2.8945e-01],\n",
      "        [ 6.7436e-01],\n",
      "        [ 5.5675e-02],\n",
      "        [ 7.5100e-01],\n",
      "        [ 7.7959e-01],\n",
      "        [ 1.9765e-01],\n",
      "        [ 5.3511e-01],\n",
      "        [ 2.7375e-01],\n",
      "        [ 2.9166e-01],\n",
      "        [ 1.9973e-01],\n",
      "        [ 8.3459e-01],\n",
      "        [ 1.8022e-01],\n",
      "        [ 8.7015e-01],\n",
      "        [ 1.0840e-01],\n",
      "        [ 6.8866e-01],\n",
      "        [ 1.0922e-01],\n",
      "        [ 2.9283e-01],\n",
      "        [ 6.9096e-01],\n",
      "        [ 2.3137e-01],\n",
      "        [ 9.0599e-02],\n",
      "        [ 1.8240e-01],\n",
      "        [ 9.5658e-01],\n",
      "        [ 8.2659e-01],\n",
      "        [ 5.4742e-01],\n",
      "        [ 6.8415e-01],\n",
      "        [ 7.5270e-01],\n",
      "        [ 2.6151e-01],\n",
      "        [ 2.8119e-01],\n",
      "        [ 2.1744e-01],\n",
      "        [ 1.2104e+00],\n",
      "        [ 2.1831e-01],\n",
      "        [ 9.1726e-01],\n",
      "        [ 8.0046e-02],\n",
      "        [ 1.4481e-01],\n",
      "        [ 5.5145e-01],\n",
      "        [ 1.8634e-01],\n",
      "        [ 7.5911e-01],\n",
      "        [ 9.1857e-01],\n",
      "        [ 6.5691e-01],\n",
      "        [ 6.1216e-01],\n",
      "        [ 7.6042e-01],\n",
      "        [ 1.8104e-01],\n",
      "        [ 1.6300e-01],\n",
      "        [ 9.0209e-01],\n",
      "        [ 1.8657e-01],\n",
      "        [ 7.3745e-01],\n",
      "        [ 8.2442e-01],\n",
      "        [ 1.8783e-02],\n",
      "        [ 8.5076e-01],\n",
      "        [ 9.2363e-01],\n",
      "        [ 6.2014e-01],\n",
      "        [ 9.6302e-01],\n",
      "        [ 9.1564e-01],\n",
      "        [ 2.3388e-01],\n",
      "        [ 7.2938e-02],\n",
      "        [ 1.1534e+00],\n",
      "        [ 6.3871e-03],\n",
      "        [ 7.2018e-01],\n",
      "        [ 8.0948e-01],\n",
      "        [ 1.2678e-01],\n",
      "        [ 2.2421e-01],\n",
      "        [ 8.6814e-01],\n",
      "        [ 1.9266e-01],\n",
      "        [ 9.9386e-01],\n",
      "        [ 8.8164e-01],\n",
      "        [ 1.1659e+00],\n",
      "        [ 8.4407e-01],\n",
      "        [ 7.7547e-01],\n",
      "        [ 9.1634e-01],\n",
      "        [ 1.0829e+00],\n",
      "        [ 1.2402e-01],\n",
      "        [ 8.3460e-02],\n",
      "        [ 3.3776e-01],\n",
      "        [ 3.2214e-01],\n",
      "        [ 8.3910e-01],\n",
      "        [ 1.0409e+00],\n",
      "        [ 9.8416e-02],\n",
      "        [ 7.8361e-01],\n",
      "        [ 1.6374e-01],\n",
      "        [-6.7657e-02],\n",
      "        [ 9.3173e-01],\n",
      "        [ 8.5819e-01],\n",
      "        [ 9.3004e-01],\n",
      "        [ 9.4186e-01],\n",
      "        [ 2.5206e-01],\n",
      "        [ 1.1308e+00],\n",
      "        [ 6.3291e-01],\n",
      "        [ 6.5887e-01],\n",
      "        [ 1.3095e-01],\n",
      "        [ 6.4692e-01],\n",
      "        [ 8.1233e-01],\n",
      "        [ 6.9959e-01],\n",
      "        [ 1.5939e-01],\n",
      "        [ 5.7222e-01],\n",
      "        [ 1.2728e+00],\n",
      "        [ 6.1587e-02],\n",
      "        [ 8.9811e-01],\n",
      "        [ 7.2122e-01],\n",
      "        [ 1.8663e-01],\n",
      "        [ 2.3081e-01],\n",
      "        [ 8.9187e-01],\n",
      "        [-3.5295e-02],\n",
      "        [ 1.0033e+00],\n",
      "        [ 1.3520e-02],\n",
      "        [ 9.2161e-01],\n",
      "        [ 2.1140e-01],\n",
      "        [ 1.7942e-01],\n",
      "        [ 3.5496e-01],\n",
      "        [ 1.4943e-01],\n",
      "        [ 1.1399e+00],\n",
      "        [ 6.9094e-01],\n",
      "        [ 2.9877e-01],\n",
      "        [ 2.1954e-01],\n",
      "        [ 2.0822e-01],\n",
      "        [ 1.9616e-01],\n",
      "        [ 9.8533e-01],\n",
      "        [ 7.2820e-01],\n",
      "        [ 9.5859e-02],\n",
      "        [ 1.9309e-01],\n",
      "        [ 2.4153e-01],\n",
      "        [ 7.2515e-01],\n",
      "        [-7.9875e-03],\n",
      "        [ 4.8825e-02],\n",
      "        [ 8.9448e-01],\n",
      "        [ 8.0742e-01],\n",
      "        [ 4.8310e-01],\n",
      "        [ 9.2542e-01],\n",
      "        [ 8.0923e-01],\n",
      "        [ 8.6322e-01],\n",
      "        [ 1.8637e-01],\n",
      "        [ 1.9697e-01],\n",
      "        [ 8.2333e-01],\n",
      "        [ 1.4137e-01],\n",
      "        [ 1.4922e-01],\n",
      "        [ 2.8779e-01],\n",
      "        [ 2.8575e-01],\n",
      "        [ 7.4905e-01],\n",
      "        [ 2.5568e-01],\n",
      "        [ 7.6024e-01],\n",
      "        [ 1.1509e+00],\n",
      "        [ 8.8032e-01],\n",
      "        [ 7.8387e-01],\n",
      "        [ 3.7315e-01],\n",
      "        [ 6.7514e-01],\n",
      "        [ 6.3222e-01],\n",
      "        [ 1.7909e-01],\n",
      "        [ 2.8813e-01],\n",
      "        [ 7.2866e-01],\n",
      "        [ 1.0969e-01],\n",
      "        [ 9.5130e-02],\n",
      "        [ 1.6436e-01],\n",
      "        [ 3.5603e-01],\n",
      "        [ 1.9798e-01],\n",
      "        [ 2.6087e-01],\n",
      "        [ 2.0225e-01],\n",
      "        [ 1.1139e-01],\n",
      "        [ 1.9946e-02],\n",
      "        [ 9.4064e-01],\n",
      "        [ 1.4575e-01],\n",
      "        [ 1.2658e-01],\n",
      "        [ 1.8866e-01],\n",
      "        [ 2.0549e-01],\n",
      "        [ 9.6643e-01],\n",
      "        [ 7.0781e-01],\n",
      "        [ 2.7782e-01],\n",
      "        [-1.0206e-02],\n",
      "        [ 7.5228e-01],\n",
      "        [ 9.6230e-01],\n",
      "        [ 6.8714e-01],\n",
      "        [ 7.6827e-01],\n",
      "        [ 9.2453e-02],\n",
      "        [ 9.5392e-01],\n",
      "        [ 5.5206e-02],\n",
      "        [ 7.7809e-01],\n",
      "        [ 9.4532e-01],\n",
      "        [ 7.2615e-01],\n",
      "        [ 4.7554e-02],\n",
      "        [ 2.0353e-01],\n",
      "        [ 2.2752e-01],\n",
      "        [ 2.3017e-01],\n",
      "        [ 7.1767e-01],\n",
      "        [ 8.1923e-01],\n",
      "        [ 5.7021e-02],\n",
      "        [-3.4504e-04],\n",
      "        [ 9.8812e-01],\n",
      "        [ 8.8867e-01],\n",
      "        [ 1.4888e-01],\n",
      "        [ 1.2015e-01],\n",
      "        [ 7.4898e-02],\n",
      "        [ 3.3280e-01],\n",
      "        [ 5.9047e-01],\n",
      "        [ 1.0076e+00],\n",
      "        [ 2.6311e-01],\n",
      "        [ 9.1539e-01],\n",
      "        [ 8.5880e-01],\n",
      "        [ 1.8094e-01],\n",
      "        [-9.4228e-02],\n",
      "        [ 8.2656e-01],\n",
      "        [ 7.1111e-01],\n",
      "        [ 1.1145e+00],\n",
      "        [ 1.0970e+00],\n",
      "        [ 9.1308e-01],\n",
      "        [ 1.0161e+00],\n",
      "        [ 5.2797e-01],\n",
      "        [ 8.5325e-01],\n",
      "        [ 7.7204e-01],\n",
      "        [ 2.9690e-01],\n",
      "        [ 5.2257e-01],\n",
      "        [ 9.1476e-01],\n",
      "        [ 1.2915e-01],\n",
      "        [ 2.1977e-02],\n",
      "        [ 9.1152e-01],\n",
      "        [ 7.4366e-01],\n",
      "        [ 1.1035e+00],\n",
      "        [ 2.4252e-01],\n",
      "        [ 9.7650e-01],\n",
      "        [ 3.6816e-02],\n",
      "        [ 8.6334e-01],\n",
      "        [ 3.3496e-01],\n",
      "        [ 2.5269e-02],\n",
      "        [ 3.4215e-01],\n",
      "        [ 3.2095e-01],\n",
      "        [ 7.5865e-01],\n",
      "        [ 1.9026e-01],\n",
      "        [ 3.6270e-01],\n",
      "        [ 8.0161e-01],\n",
      "        [ 4.7036e-02],\n",
      "        [ 2.1673e-01],\n",
      "        [ 5.0162e-01]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define model parameters\n",
    "num_covariates = 5  # Age, BP, Cholesterol\n",
    "embedding_dim = 20   # Embedding dimension for both covariates & treatments\n",
    "num_treatments = 2  # Assume 2 possible treatments\n",
    "\n",
    "# Initialize the model\n",
    "model = TransTEE(num_covariates, embedding_dim, num_treatments)\n",
    "\n",
    "# Perform forward pass (inference)\n",
    "predicted_outcome = model(X_train, w_train.int().squeeze())\n",
    "\n",
    "print(\"Predicted outcome:\", predicted_outcome)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c35db85f-6ee7-4f5a-bd71-18174244ed8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 2.5690\n",
      "Epoch 10, Loss: 1.1526\n",
      "Epoch 20, Loss: 1.1516\n",
      "Epoch 30, Loss: 1.1349\n",
      "Epoch 40, Loss: 1.1396\n",
      "Epoch 50, Loss: 1.1229\n",
      "Epoch 60, Loss: 1.0771\n",
      "Epoch 70, Loss: 1.0431\n",
      "Epoch 80, Loss: 1.0479\n",
      "Epoch 90, Loss: 1.0320\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = model(X_train, w_train.int().squeeze())  # Forward pass\n",
    "    loss = loss_function(y_pred, y_train)  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update weights\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d267d4a3-0fec-434f-a294-30c12cd36011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error in Treatment Effect Estimation: 0.2883\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y0_pred_test = model(X_test, torch.zeros(X_test.shape[0]).int())\n",
    "    y1_pred_test = model(X_test, torch.ones(X_test.shape[0]).int())\n",
    "\n",
    "    # Estimate Individual Treatment Effects (ITE)\n",
    "    tau_hat = (y1_pred_test - y0_pred_test).squeeze().numpy()\n",
    "\n",
    "    # Compute Mean Absolute Error\n",
    "    mae = np.mean(np.abs(tau_hat - tau_test.numpy()))\n",
    "    print(f\"Mean Absolute Error in Treatment Effect Estimation: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e89f7-3ed2-4799-b5d2-9120af70aa68",
   "metadata": {},
   "source": [
    "# Training transTEE on larger size of dataset leverage torch.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a8f6c43b-31d0-4549-8469-a7333e80a866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Step 50: Loss = 1.0689\n",
      "Step 100: Loss = 1.0806\n",
      "Step 150: Loss = 1.1676\n",
      "Step 200: Loss = 1.0960\n",
      "Step 250: Loss = 1.0676\n",
      "Step 300: Loss = 1.0490\n",
      "Step 350: Loss = 1.1014\n",
      "Step 400: Loss = 1.0645\n",
      "Step 450: Loss = 1.0006\n",
      "Step 500: Loss = 1.0752\n",
      "Step 550: Loss = 1.0046\n",
      "Step 600: Loss = 1.0622\n",
      "Step 650: Loss = 0.9824\n",
      "Step 700: Loss = 0.9412\n",
      "Step 750: Loss = 0.9430\n",
      "\n",
      "Mean Absolute Error for transTEE: 0.0773\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from causalml.dataset import synthetic_data\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load synthetic dataset with larger sample size\n",
    "y, X, w, tau, b, e = synthetic_data(mode=1, n=50000, p=10, sigma=1.0, adj=0.0)  # Increased dataset size\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test, w_train, w_test, tau_train, tau_test = train_test_split(\n",
    "    X, y, w, tau, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train, X_test = torch.tensor(X_train, dtype=torch.float32), torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train, y_test = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1), torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "w_train, w_test = torch.tensor(w_train, dtype=torch.float32).unsqueeze(1), torch.tensor(w_test, dtype=torch.float32).unsqueeze(1)\n",
    "tau_train, tau_test = torch.tensor(tau_train, dtype=torch.float32), torch.tensor(tau_test, dtype=torch.float32)\n",
    "\n",
    "# --- PyTorch Dataset & DataLoader ---\n",
    "class CausalDataset(Dataset):\n",
    "    def __init__(self, X, y, w):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.w = w\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx], self.w[idx]\n",
    "\n",
    "# Create data loaders for mini-batch training\n",
    "batch_size = 512\n",
    "train_dataset = CausalDataset(X_train, y_train, w_train)\n",
    "test_dataset = CausalDataset(X_test, y_test, w_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize model\n",
    "input_dim = X_train.shape[1]\n",
    "model = TransTEE(num_covariates, embedding_dim, num_treatments).to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# --- Train DragonNet with Mini-batch Training ---\n",
    "num_epochs = 10\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    for batch_X, batch_y, batch_w in train_loader:\n",
    "        batch_X, batch_y, batch_w = batch_X.to(device), batch_y.to(device), batch_w.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        y_pred = model(batch_X, batch_w.int().squeeze())  # Forward pass\n",
    "        loss = loss_function(y_pred, batch_y)  # Compute loss\n",
    "    \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print(f\"Step {step}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# --- Evaluate transTEE ---\n",
    "model.eval()\n",
    "tau_hat = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, _, _ in test_loader:\n",
    "        batch_X = batch_X.to(device)\n",
    "        y0_pred_test = model(batch_X, torch.zeros(batch_X.shape[0]).int())\n",
    "        y1_pred_test = model(batch_X, torch.ones(batch_X.shape[0]).int())\n",
    "        tau_hat.extend((y1_pred_test - y0_pred_test).cpu().numpy())\n",
    "\n",
    "tau_hat = np.array(tau_hat).flatten()\n",
    "\n",
    "# --- Compare with True Treatment Effects ---\n",
    "mae = mean_absolute_error(tau_test, tau_hat)\n",
    "print(f\"\\nMean Absolute Error for transTEE: {mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b0e0b2-5e62-4cb0-a6ea-78b001c54c30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
