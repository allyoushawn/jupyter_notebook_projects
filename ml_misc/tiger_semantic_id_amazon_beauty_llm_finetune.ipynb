{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvEf-n_mdqnI"
      },
      "source": [
        "# TIGER SemanticID: Qwen3-8B Fine-tuning for SID Recommendation\n",
        "\n",
        "This notebook fine-tunes Qwen3-8B to generate Semantic IDs for next-item recommendation.\n",
        "\n",
        "## Pipeline Overview\n",
        "\n",
        "1. **Build Dialogs**: Convert user histories to conversational format + build trie\n",
        "2. **Stage A (Vocab)**: Fine-tune only embeddings to learn 1,027 new SID tokens\n",
        "3. **Stage B (Full)**: Fine-tune entire model on recommendation task\n",
        "4. **Inference**: Generate SIDs with level + trie constraints\n",
        "5. **Evaluation**: Measure SID@K, Invalid-ID@K, qualitative examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2St8dGkgdpgZ",
        "outputId": "08bd7933-2f31-4ed1-b401-3df4fbf703f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install dependencies (Colab)\n",
        "!pip install -q transformers accelerate peft datasets bitsandbytes tiktoken sentencepiece jsonlines orjson"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZtNiKY2dtKl",
        "outputId": "c98b9745-5e42-4a99-93d0-0e17db6b1319"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "assert os.path.exists('/content/drive')\n",
        "WORK_DIR = '/content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty'\n",
        "%mkdir -p $WORK_DIR\n",
        "%cd $WORK_DIR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53kXihAvdyxX",
        "outputId": "25c69676-bb45-49cf-ab47-18eec592ebf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'recsys_playground'...\n",
            "remote: Enumerating objects: 450, done.\u001b[K\n",
            "remote: Counting objects: 100% (197/197), done.\u001b[K\n",
            "remote: Compressing objects: 100% (126/126), done.\u001b[K\n",
            "remote: Total 450 (delta 139), reused 125 (delta 71), pack-reused 253 (from 1)\u001b[K\n",
            "Receiving objects: 100% (450/450), 202.92 KiB | 6.34 MiB/s, done.\n",
            "Resolving deltas: 100% (260/260), done.\n",
            "/content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty/recsys_playground\n",
            "Fetching origin\n",
            "Branch '20250908_tiger_dev' set up to track remote branch '20250908_tiger_dev' from 'origin'.\n",
            "Switched to a new branch '20250908_tiger_dev'\n"
          ]
        }
      ],
      "source": [
        "# Clone repo, install dependencies, and make src importable (Colab-friendly)\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "repo_url = 'https://github.com/allyoushawn/recsys_playground.git'\n",
        "repo_dir = 'recsys_playground'\n",
        "branch_name = '20250908_tiger_dev'\n",
        "\n",
        "import os\n",
        "if IN_COLAB:\n",
        "    if os.path.exists(repo_dir):\n",
        "      !rm -rf {repo_dir}\n",
        "    !git clone $repo_url\n",
        "    %cd $repo_dir\n",
        "    !git fetch --all\n",
        "    !git checkout $branch_name || echo 'Branch not found; staying on default.'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27oC-y2Ld12X",
        "outputId": "93a636fe-615d-4ebc-9f49-a37441f37a10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artifacts: /content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty/artifacts\n",
            "LLM outputs: /content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty/artifacts/llm\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, f'{WORK_DIR}/recysys_playground/tiger_semantic_id_amazon_beauty/src')\n",
        "\n",
        "# Config\n",
        "ARTIFACTS_DIR = f'{WORK_DIR}/artifacts'\n",
        "LLM_DIR = f'{ARTIFACTS_DIR}/llm'\n",
        "#LLM_DIR = '/content/llm'\n",
        "!mkdir -p $LLM_DIR\n",
        "\n",
        "print(f\"Artifacts: {ARTIFACTS_DIR}\")\n",
        "print(f\"LLM outputs: {LLM_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNjMWATPeHhC"
      },
      "source": [
        "## 1. Build Dialogs\n",
        "\n",
        "Convert user histories to chat-style JSONL format and build trie of valid SID continuations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o11C6G3pd4EK",
        "outputId": "6710f71b-dbc3-46ed-b721-fcb383eb879f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading artifacts from /content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty/artifacts...\n",
            "Loaded 12101 semantic IDs\n",
            "Building SID trie...\n",
            "Saved trie to /content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty/artifacts/llm/sid_trie.pkl\n",
            "  - valid_c2: 1 L1 codes\n",
            "  - valid_c3: 1 (L1,L2) pairs\n",
            "  - valid_c4: 1 (L1,L2,L3) triples\n",
            "Loaded sequences for 22363 users\n",
            "Creating dialogs...\n",
            "Building dialogs: 100% 22363/22363 [00:00<00:00, 22366.82it/s]\n",
            "Created 37052 dialogs\n",
            "Average history length: 8.0 items\n",
            "Split: 35199 train, 1853 valid\n",
            "Saved training dialogs to /content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty/artifacts/llm/dialogs_train.jsonl\n",
            "Saved validation dialogs to /content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty/artifacts/llm/dialogs_valid.jsonl\n",
            "\n",
            "=== Example Dialog ===\n",
            "SYSTEM:\n",
            "You are a recommender that must reply ONLY with the next product's Semantic ID as 4 tokens in order: L1, L2, L3, L4.\n",
            "Valid token ranges by level:\n",
            "- L1: <sid_0>.. <sid_255>\n",
            "- L2: <sid_256>.. <sid_511>\n",
            "- L3: <sid_512>.. <sid_767>\n",
            "- L4: <sid_768>.. <sid_1023>\n",
            "Do not output anything else.\n",
            "\n",
            "USER:\n",
            "History:\n",
            "<sid_163> <sid_316> <sid_593> <sid_2384>\n",
            "<sid_163> <sid_316> <sid_593> <sid_5325>\n",
            "<sid_163> <sid_316> <sid_593> <sid_6447>\n",
            "<sid_163> <sid_316> <sid_593> <sid_7323>\n",
            "<sid_163> <sid_316> <sid_593> <sid_8243>\n",
            "<sid_163> <sid_316> <sid_593> <sid_3951>\n",
            "<sid_163> <sid_316> <sid_593> <sid_3144>\n",
            "<sid_163> <sid_316> <sid_593> <sid_3921>\n",
            "Recommend next:\n",
            "\n",
            "ASSISTANT:\n",
            "<sid_163> <sid_316> <sid_593> <sid_5593>\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Build dialogs and trie\n",
        "!python -m tiger_semantic_id_amazon_beauty.src.llm.build_sid_dialogs \\\n",
        "    --artifacts_dir $ARTIFACTS_DIR \\\n",
        "    --out $LLM_DIR \\\n",
        "    --history_len 8 \\\n",
        "    --train_ratio 0.95"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IF-lHsSTeJCB",
        "outputId": "7c46a6b8-2438-44b8-bf9b-750c7f67f107"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dialogs: 35199\n",
            "Valid dialogs: 1853\n",
            "Trie exists: True\n",
            "\n",
            "=== Example Dialog ===\n",
            "SYSTEM:\n",
            "You are a recommender that must reply ONLY with the next product's Semantic ID as 4 tokens in order: L1, L2, L3, L4.\n",
            "Valid token ranges by level:\n",
            "- L1: <sid_0>.. <sid_255>\n",
            "- L2: <sid_256>.. <sid_511>\n",
            "...\n",
            "\n",
            "USER:\n",
            "History:\n",
            "<sid_163> <sid_316> <sid_593> <sid_2384>\n",
            "<sid_163> <sid_316> <sid_593> <sid_5325>\n",
            "<sid_163> <sid_316> <sid_593> <sid_6447>\n",
            "<sid_163> <sid_316> <sid_593> <sid_7323>\n",
            "<sid_163> <sid_316> <sid_59...\n",
            "\n",
            "ASSISTANT:\n",
            "<sid_163> <sid_316> <sid_593> <sid_5593>\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Verify outputs\n",
        "import jsonlines\n",
        "\n",
        "train_path = f'{LLM_DIR}/dialogs_train.jsonl'\n",
        "valid_path = f'{LLM_DIR}/dialogs_valid.jsonl'\n",
        "trie_path = f'{LLM_DIR}/sid_trie.pkl'\n",
        "\n",
        "with jsonlines.open(train_path) as reader:\n",
        "    train_dialogs = list(reader)\n",
        "\n",
        "with jsonlines.open(valid_path) as reader:\n",
        "    valid_dialogs = list(reader)\n",
        "\n",
        "print(f\"Train dialogs: {len(train_dialogs)}\")\n",
        "print(f\"Valid dialogs: {len(valid_dialogs)}\")\n",
        "print(f\"Trie exists: {os.path.exists(trie_path)}\")\n",
        "\n",
        "# Show example\n",
        "print(\"\\n=== Example Dialog ===\")\n",
        "example = train_dialogs[0]\n",
        "for msg in example['messages']:\n",
        "    print(f\"{msg['role'].upper()}:\")\n",
        "    print(msg['content'][:200] + \"...\" if len(msg['content']) > 200 else msg['content'])\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYJKKHjfDsfu"
      },
      "source": [
        "## 2. Tokenizer Resize\n",
        "\n",
        "Add 1,027 new tokens to Qwen tokenizer and initialize embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptaQoRWdeMVA"
      },
      "outputs": [],
      "source": [
        "# Resize tokenizer and model\n",
        "!python -m tiger_semantic_id_amazon_beauty.src.llm.tokenizer_resize_qwen \\\n",
        "    --base Qwen/Qwen3-8B \\\n",
        "    --out $LLM_DIR/qwen3_vocab_stage \\\n",
        "    --torch_dtype bfloat16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veoyVoV8I2qQ"
      },
      "source": [
        "## 3. Stage A: Vocabulary Extension\n",
        "\n",
        "Fine-tune **only embeddings** to teach the model the new SID tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFu4JKa6DuFh",
        "outputId": "855749b5-3dac-418f-e087-718c8904e90f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2025-10-15 05:20:18.864402: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-10-15 05:20:18.884307: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760505618.905792    1721 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760505618.912446    1721 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1760505618.928870    1721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1760505618.928896    1721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1760505618.928899    1721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1760505618.928902    1721 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-10-15 05:20:18.934076: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n=== Stage A: Vocabulary \n\n... [Output truncated - 45692 total characters] ...\n"
        }
      ],
      "source": [
        "# Stage A: Embeddings only\n",
        "!python -m tiger_semantic_id_amazon_beauty.src.llm.finetune_qwen_vocab \\\n",
        "    --data $LLM_DIR/dialogs_train.jsonl \\\n",
        "    --valid $LLM_DIR/dialogs_valid.jsonl \\\n",
        "    --in_model $LLM_DIR/qwen3_vocab_stage \\\n",
        "    --out_model $LLM_DIR/qwen3_vocab_stage \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --learning_rate 5e-4 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --warmup_ratio 0.03 \\\n",
        "    --logging_steps 50 \\\n",
        "    --save_steps 500 \\\n",
        "    --bf16 \\\n",
        "    --gradient_checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZCqZNJ0pmmlO",
        "outputId": "febee86b-9fe3-4791-9761-7b2e9e4c3f5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU memory cleared\n"
          ]
        }
      ],
      "source": [
        "# Clear GPU memory before Stage B to avoid OOM\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# Force garbage collection\n",
        "gc.collect()\n",
        "\n",
        "# Clear CUDA cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "print(\"GPU memory cleared\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQX-QzfYI9KH"
      },
      "source": [
        "## 4. Stage B: LoRA Fine-tuning\n",
        "\n",
        "Fine-tune with **LoRA adapters** for memory-efficient training (~20-25GB VRAM instead of ~60GB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "X6y2XTmLI4gK",
        "outputId": "3eadfaa7-91a3-4b19-8e22-9136c55f36bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "2025-10-15 07:15:01.890240: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n2025-10-15 07:15:01.908126: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760512501.929232   30121 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760512501.935659   30121 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1760512501.952127   30121 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1760512501.952153   30121 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1760512501.952156   30121 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1760512501.952159   30121 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n2025-10-15 07:15:01.957009: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\nTo enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n=== Stage B: LoRA Fine-t\n\n... [Output truncated - 46275 total characters] ...\n"
        }
      ],
      "source": [
        "# Stage B: LoRA fine-tuning with memory optimizations\n",
        "!python -m tiger_semantic_id_amazon_beauty.src.llm.finetune_qwen_lora \\\n",
        "    --data $LLM_DIR/dialogs_train.jsonl \\\n",
        "    --valid $LLM_DIR/dialogs_valid.jsonl \\\n",
        "    --in_model $LLM_DIR/qwen3_vocab_stage \\\n",
        "    --out_model $LLM_DIR/qwen3_lora_adapter \\\n",
        "    --sid_trie $LLM_DIR/sid_trie.pkl \\\n",
        "    --lora_r 16 \\\n",
        "    --lora_alpha 32 \\\n",
        "    --lora_dropout 0.05 \\\n",
        "    --per_device_train_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 8 \\\n",
        "    --learning_rate 1e-4 \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --warmup_ratio 0.03 \\\n",
        "    --logging_steps 50 \\\n",
        "    --save_steps 500 \\\n",
        "    --bf16 \\\n",
        "    --gradient_checkpointing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Off7Cn1yJBBz"
      },
      "source": [
        "## 5. Inference Demo\n",
        "\n",
        "Generate SIDs with level and trie constraints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222,
          "referenced_widgets": [
            "1c166286807d44b1935470fd80090bc7",
            "be2dd31990c944ef864458ad53ebedfd",
            "a67d4926ac844636b2e86ddaf0a89cb6",
            "6f3dd298aaf848a88f425eb0ea55db4f",
            "8d057e1bea684b52969b49269a347150",
            "dea75217a82d4a6abe8f1573efb28a5e",
            "b9d55c16439e43bdae66cc2c421873f2",
            "23870d1b441348b0bb862b4b1335c89b",
            "9d75bc230624480aa569bb5362e43be9",
            "d4e87d2f369f4bfab0d6a918575d6648",
            "04dec68809144fe8bcb0b85e12e05978"
          ]
        },
        "id": "cFc_F63RJCgK",
        "outputId": "dd4461cd-cd78-49fd-92f9-8f60c2c7761e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model from /content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty/artifacts/llm/qwen3_lora_adapter...\n",
            "Loading tokenizer from LoRA adapter: /content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty/artifacts/llm/qwen3_lora_adapter...\n",
            "Loading base model from /content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty/artifacts/llm/qwen3_vocab_stage...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1c166286807d44b1935470fd80090bc7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading LoRA adapter from /content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty/artifacts/llm/qwen3_lora_adapter...\n",
            "\u2713 Verified SID tokens in vocabulary (vocab_size=152696)\n",
            "Loading trie from /content/drive/MyDrive/colab/tiger_semantic_id_amazon_beauty/artifacts/llm/sid_trie.pkl...\n",
            "Model loaded!\n",
            "Recommender loaded!\n"
          ]
        }
      ],
      "source": [
        "# Interactive inference with LoRA adapter\n",
        "from tiger_semantic_id_amazon_beauty.src.llm.inference_qwen import SIDRecommender\n",
        "import json\n",
        "\n",
        "# Load recommender with LoRA adapter\n",
        "recommender = SIDRecommender(\n",
        "    model_path=f'{LLM_DIR}/qwen3_lora_adapter',\n",
        "    base_model_path=f'{LLM_DIR}/qwen3_vocab_stage',\n",
        "    trie_path=f'{LLM_DIR}/sid_trie.pkl',\n",
        "    is_lora_adapter=True,\n",
        ")\n",
        "\n",
        "# Load mappings\n",
        "with open(f'{ARTIFACTS_DIR}/sid_to_items.json') as f:\n",
        "    sid_to_items = json.load(f)\n",
        "\n",
        "print(\"Recommender loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2HFlC01JEAn",
        "outputId": "978977c0-8a7e-40a3-dea8-584ed3bffc46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Generated Recommendation ===\n",
            "Generated SID: (163, 60, 81, 166)\n",
            "Mapped Items:\n"
          ]
        }
      ],
      "source": [
        "# Example: Generate from history\n",
        "history_sids = [\n",
        "    (64, 54, 125, 0),\n",
        "    (64, 156, 194, 0),\n",
        "    (112, 191, 11, 4),\n",
        "]\n",
        "\n",
        "results = recommender.recommend(\n",
        "    history_sids=history_sids,\n",
        "    sid_to_items=sid_to_items,\n",
        "    top_k=5,\n",
        ")\n",
        "\n",
        "print(\"\\n=== Generated Recommendation ===\")\n",
        "if results:\n",
        "    result = results[0]\n",
        "    print(f\"Generated SID: {result['sid']}\")\n",
        "    print(f\"Mapped Items:\")\n",
        "    for item_id in result['items']:\n",
        "        print(f\"  - {item_id}\")\n",
        "else:\n",
        "    print(\"No valid SID generated\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zsRCCyJgIT_",
        "outputId": "7fef8de2-42e0-4cc0-e1cf-75f470dc1e26"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'sid': (163, 60, 81, 166), 'items': []}]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sixkZhMgR_m",
        "outputId": "9e905577-687f-4df1-bf1a-ff998084ee0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'163-60-81-0': [0],\n",
              " '163-60-81-1': [1],\n",
              " '163-60-81-2': [2],\n",
              " '163-60-81-3': [3],\n",
              " '163-60-81-4': [4],\n",
              " '163-60-81-5': [5],\n",
              " '163-60-81-6': [6],\n",
              " '163-60-81-7': [7],\n",
              " '163-60-81-8': [8],\n",
              " '163-60-81-9': [9],\n",
              " '163-60-81-10': [10],\n",
              " '163-60-81-11': [11],\n",
              " '163-60-81-12': [12],\n",
              " '163-60-81-13': [13],\n",
              " '163-60-81-14': [14],\n",
              " '163-60-81-15': [15],\n",
              " '163-60-81-16': [16],\n",
              " '163-60-81-17': [17],\n",
              " '163-60-81-18': [18],\n",
              " '163-60-81-19': [19],\n",
              " '163-60-81-20': [20],\n",
              " '163-60-81-21': [21],\n",
              " '163-60-81-22': [22],\n",
              " '163-60-81-23': [23],\n",
              " '163-60-81-24': [24],\n",
              " '163-60-81-25': [25],\n",
              " '163-60-81-26': [26],\n",
              " '163-60-81-27': [27],\n",
              " '163-60-81-28': [28],\n",
              " '163-60-81-29': [29],\n",
              " '163-60-81-30': [30],\n",
              " '163-60-81-31': [31],\n",
              " '163-60-81-32': [32],\n",
              " '163-60-81-33': [33],\n",
              " '163-60-81-34': [34],\n",
              " '163-60-81-35': [35],\n",
              " '163-60-81-36': [36],\n",
              " '163-60-81-37': [37],\n",
              " '163-60-81-38': [38],\n",
              " '163-60-81-39': [39],\n",
              " '163-60-81-40': [40],\n",
              " '163-60-81-41': [41],\n",
              " '163-60-81-42': [42],\n",
              " '163-60-81-43': [43],\n",
              " '163-60-81-44': [44],\n",
              " '163-60-81-45': [45],\n",
              " '163-60-81-46': [46],\n",
              " '163-60-81-47': [47],\n",
              " '163-60-81-48': [48],\n",
              " '163-60-81-49': [49],\n",
              " '163-60-81-50': [50],\n",
              " '163-60-81-51': [51],\n",
              " '163-60-81-52': [52],\n",
              " '163-60-81-53': [53],\n",
              " '163-60-81-54': [54],\n",
              " '163-60-81-55': [55],\n",
              " '163-60-81-56': [56],\n",
              " '163-60-81-57': [57],\n",
              " '163-60-81-58': [58],\n",
              " '163-60-81-59': [59],\n",
              " '163-60-81-60': [60],\n",
              " '163-60-81-61': [61],\n",
              " '163-60-81-62': [62],\n",
              " '163-60-81-63': [63],\n",
              " '163-60-81-64': [64],\n",
              " '163-60-81-65': [65],\n",
              " '163-60-81-66': [66],\n",
              " '163-60-81-67': [67],\n",
              " '163-60-81-68': [68],\n",
              " '163-60-81-69': [69],\n",
              " '163-60-81-70': [70],\n",
              " '163-60-81-71': [71],\n",
              " '163-60-81-72': [72],\n",
              " '163-60-81-73': [73],\n",
              " '163-60-81-74': [74],\n",
              " '163-60-81-75': [75],\n",
              " '163-60-81-76': [76],\n",
              " '163-60-81-77': [77],\n",
              " '163-60-81-78': [78],\n",
              " '163-60-81-79': [79],\n",
              " '163-60-81-80': [80],\n",
              " '163-60-81-81': [81],\n",
              " '163-60-81-82': [82],\n",
              " '163-60-81-83': [83],\n",
              " '163-60-81-84': [84],\n",
              " '163-60-81-85': [85],\n",
              " '163-60-81-86': [86],\n",
              " '163-60-81-87': [87],\n",
              " '163-60-81-88': [88],\n",
              " '163-60-81-89': [89],\n",
              " '163-60-81-90': [90],\n",
              " '163-60-81-91': [91],\n",
              " '163-60-81-92': [92],\n",
              " '163-60-81-93': [93],\n",
              " '163-60-81-94': [94],\n",
              " '163-60-81-95': [95],\n",
              " '163-60-81-96': [96],\n",
              " '163-60-81-97': [97],\n",
              " '163-60-81-98': [98],\n",
              " '163-60-81-99': [99],\n",
              " '163-60-81-100': [100],\n",
              " '163-60-81-101': [101],\n",
              " '163-60-81-102': [102],\n",
              " '163-60-81-103': [103],\n",
              " '163-60-81-104': [104],\n",
              " '163-60-81-105': [105],\n",
              " '163-60-81-106': [106],\n",
              " '163-60-81-107': [107],\n",
              " '163-60-81-108': [108],\n",
              " '163-60-81-109': [109],\n",
              " '163-60-81-110': [110],\n",
              " '163-60-81-111': [111],\n",
              " '163-60-81-112': [112],\n",
              " '163-60-81-113': [113],\n",
              " '163-60-81-114': [114],\n",
              " '163-60-81-115': [115],\n",
              " '163-60-81-116': [116],\n",
              " '163-60-81-117': [117],\n",
              " '163-60-81-118': [118],\n",
              " '163-60-81-119': [119],\n",
              " '163-60-81-120': [120],\n",
              " '163-60-81-121': [121],\n",
              " '163-60-81-122': [122],\n",
              " '163-60-81-123': [123],\n",
              " '163-60-81-124': [124],\n",
              " '163-60-81-125': [125],\n",
              " '163-60-81-126': [126],\n",
              " '163-60-81-127': [127],\n",
              " '163-60-81-128': [128],\n",
              " '163-60-81-129': [129],\n",
              " '163-60-81-130': [130],\n",
              " '163-60-81-131': [131],\n",
              " '163-60-81-132': [132],\n",
              " '163-60-81-133': [133],\n",
              " '163-60-81-134': [134],\n",
              " '163-60-81-135': [135],\n",
              " '163-60-81-136': [136],\n",
              " '163-60-81-137': [137],\n",
              " '163-60-81-138': [138],\n",
              " '163-60-81-139': [139],\n",
              " '163-60-81-140': [140],\n",
              " '163-60-81-141': [141],\n",
              " '163-60-81-142': [142],\n",
              " '163-60-81-143': [143],\n",
              " '163-60-81-144': [144],\n",
              " '163-60-81-145': [145],\n",
              " '163-60-81-146': [146],\n",
              " '163-60-81-147': [147],\n",
              " '163-60-81-148': [148],\n",
              " '163-60-81-149': [149],\n",
              " '163-60-81-150': [150],\n",
              " '163-60-81-151': [151],\n",
              " '163-60-81-152': [152],\n",
              " '163-60-81-153': [153],\n",
              " '163-60-81-154': [154],\n",
              " '163-60-81-155': [155],\n",
              " '163-60-81-156': [156],\n",
              " '163-60-81-157': [157],\n",
              " '163-60-81-158': [158],\n",
              " '163-60-81-159': [159],\n",
              " '163-60-81-160': [160],\n",
              " '163-60-81-161': [161],\n",
              " '163-60-81-162': [162],\n",
              " '163-60-81-163': [163],\n",
              " '163-60-81-164': [164],\n",
              " '163-60-81-165': [165],\n",
              " '163-60-81-166': [166],\n",
              " '163-60-81-167': [167],\n",
              " '163-60-81-168': [168],\n",
              " '163-60-81-169': [169],\n",
              " '163-60-81-170': [170],\n",
              " '163-60-81-171': [171],\n",
              " '163-60-81-172': [172],\n",
              " '163-60-81-173': [173],\n",
              " '163-60-81-174': [174],\n",
              " '163-60-81-175': [175],\n",
              " '163-60-81-176': [176],\n",
              " '163-60-81-177': [177],\n",
              " '163-60-81-178': [178],\n",
              " '163-60-81-179': [179],\n",
              " '163-60-81-180': [180],\n",
              " '163-60-81-181': [181],\n",
              " '163-60-81-182': [182],\n",
              " '163-60-81-183': [183],\n",
              " '163-60-81-184': [184],\n",
              " '163-60-81-185': [185],\n",
              " '163-60-81-186': [186],\n",
              " '163-60-81-187': [187],\n",
              " '163-60-81-188': [188],\n",
              " '163-60-81-189': [189],\n",
              " '163-60-81-190': [190],\n",
              " '163-60-81-191': [191],\n",
              " '163-60-81-192': [192],\n",
              " '163-60-81-193': [193],\n",
              " '163-60-81-194': [194],\n",
              " '163-60-81-195': [195],\n",
              " '163-60-81-196': [196],\n",
              " '163-60-81-197': [197],\n",
              " '163-60-81-198': [198],\n",
              " '163-60-81-199': [199],\n",
              " '163-60-81-200': [200],\n",
              " '163-60-81-201': [201],\n",
              " '163-60-81-202': [202],\n",
              " '163-60-81-203': [203],\n",
              " '163-60-81-204': [204],\n",
              " '163-60-81-205': [205],\n",
              " '163-60-81-206': [206],\n",
              " '163-60-81-207': [207],\n",
              " '163-60-81-208': [208],\n",
              " '163-60-81-209': [209],\n",
              " '163-60-81-210': [210],\n",
              " '163-60-81-211': [211],\n",
              " '163-60-81-212': [212],\n",
              " '163-60-81-213': [213],\n",
              " '163-60-81-214': [214],\n",
              " '163-60-81-215': [215],\n",
              " '163-60-81-216': [216],\n",
              " '163-60-81-217': [217],\n",
              " '163-60-81-218': [218],\n",
              " '163-60-81-219': [219],\n",
              " '163-60-81-220': [220],\n",
              " '163-60-81-221': [221],\n",
              " '163-60-81-222': [222],\n",
              " '163-60-81-223': [223],\n",
              " '163-60-81-224': [224],\n",
              " '163-60-81-225': [225],\n",
              " '163-60-81-226': [226],\n",
              " '163-60-81-227': [227],\n",
              " '163-60-81-228': [228],\n",
              " '163-60-81-229': [229],\n",
              " '163-60-81-230': [230],\n",
              " '163-60-81-231': [231],\n",
              " '163-60-81-232': [232],\n",
              " '163-60-81-233': [233],\n",
              " '163-60-81-234': [234],\n",
              " '163-60-81-235': [235],\n",
              " '163-60-81-236': [236],\n",
              " '163-60-81-237': [237],\n",
              " '163-60-81-238': [238],\n",
              " '163-60-81-239': [239],\n",
              " '163-60-81-240': [240],\n",
              " '163-60-81-241': [241],\n",
              " '163-60-81-242': [242],\n",
              " '163-60-81-243': [243],\n",
              " '163-60-81-244': [244],\n",
              " '163-60-81-245': [245],\n",
              " '163-60-81-246': [246],\n",
              " '163-60-81-247': [247],\n",
              " '163-60-81-248': [248],\n",
              " '163-60-81-249': [249],\n",
              " '163-60-81-250': [250],\n",
              " '163-60-81-251': [251],\n",
              " '163-60-81-252': [252],\n",
              " '163-60-81-253': [253],\n",
              " '163-60-81-254': [254],\n",
              " '163-60-81-255': [255],\n",
              " '163-60-81-256': [256],\n",
              " '163-60-81-257': [257],\n",
              " '163-60-81-258': [258],\n",
              " '163-60-81-259': [259],\n",
              " '163-60-81-260': [260],\n",
              " '163-60-81-261': [261],\n",
              " '163-60-81-262': [262],\n",
              " '163-60-81-263': [263],\n",
              " '163-60-81-264': [264],\n",
              " '163-60-81-265': [265],\n",
              " '163-60-81-266': [266],\n",
              " '163-60-81-267': [267],\n",
              " '163-60-81-268': [268],\n",
              " '163-60-81-269': [269],\n",
              " '163-60-81-270': [270],\n",
              " '163-60-81-271': [271],\n",
              " '163-60-81-272': [272],\n",
              " '163-60-81-273': [273],\n",
              " '163-60-81-274': [274],\n",
              " '163-60-81-275': [275],\n",
              " '163-60-81-276': [276],\n",
              " '163-60-81-277': [277],\n",
              " '163-60-81-278': [278],\n",
              " '163-60-81-279': [279],\n",
              " '163-60-81-280': [280],\n",
              " '163-60-81-281': [281],\n",
              " '163-60-81-282': [282],\n",
              " '163-60-81-283': [283],\n",
              " '163-60-81-284': [284],\n",
              " '163-60-81-285': [285],\n",
              " '163-60-81-286': [286],\n",
              " '163-60-81-287': [287],\n",
              " '163-60-81-288': [288],\n",
              " '163-60-81-289': [289],\n",
              " '163-60-81-290': [290],\n",
              " '163-60-81-291': [291],\n",
              " '163-60-81-292': [292],\n",
              " '163-60-81-293': [293],\n",
              " '163-60-81-294': [294],\n",
              " '163-60-81-295': [295],\n",
              " '163-60-81-296': [296],\n",
              " '163-60-81-297': [297],\n",
              " '163-60-81-298': [298],\n",
              " '163-60-81-299': [299],\n",
              " '163-60-81-300': [300],\n",
              " '163-60-81-301': [301],\n",
              " '163-60-81-302': [302],\n",
              " '163-60-81-303': [303],\n",
              " '163-60-81-304': [304],\n",
              " '163-60-81-305': [305],\n",
              " '163-60-81-306': [306],\n",
              " '163-60-81-307': [307],\n",
              " '163-60-81-308': [308],\n",
              " '163-60-81-309': [309],\n",
              " '163-60-81-310': [310],\n",
              " '163-60-81-311': [311],\n",
              " '163-60-81-312': [312],\n",
              " '163-60-81-313': [313],\n",
              " '163-60-81-314': [314],\n",
              " '163-60-81-315': [315],\n",
              " '163-60-81-316': [316],\n",
              " '163-60-81-317': [317],\n",
              " '163-60-81-318': [318],\n",
              " '163-60-81-319': [319],\n",
              " '163-60-81-320': [320],\n",
              " '163-60-81-321': [321],\n",
              " '163-60-81-322': [322],\n",
              " '163-60-81-323': [323],\n",
              " '163-60-81-324': [324],\n",
              " '163-60-81-325': [325],\n",
              " '163-60-81-326': [326],\n",
              " '163-60-81-327': [327],\n",
              " '163-60-81-328': [328],\n",
              " '163-60-81-329': [329],\n",
              " '163-60-81-330': [330],\n",
              " '163-60-81-331': [331],\n",
              " '163-60-81-332': [332],\n",
              " '163-60-81-333': [333],\n",
              " '163-60-81-334': [334],\n",
              " '163-60-81-335': [335],\n",
              " '163-60-81-336': [336],\n",
              " '163-60-81-337': [337],\n",
              " '163-60-81-338': [338],\n",
              " '163-60-81-339': [339],\n",
              " '163-60-81-340': [340],\n",
              " '163-60-81-341': [341],\n",
              " '163-60-81-342': [342],\n",
              " '163-60-81-343': [343],\n",
              " '163-60-81-344': [344],\n",
              " '163-60-81-345': [345],\n",
              " '163-60-81-346': [346],\n",
              " '163-60-81-347': [347],\n",
              " '163-60-81-348': [348],\n",
              " '163-60-81-349': [349],\n",
              " '163-60-81-350': [350],\n",
              " '163-60-81-351': [351],\n",
              " '163-60-81-352': [352],\n",
              " '163-60-81-353': [353],\n",
              " '163-60-81-354': [354],\n",
              " '163-60-81-355': [355],\n",
              " '163-60-81-356': [356],\n",
              " '163-60-81-357': [357],\n",
              " '163-60-81-358': [358],\n",
              " '163-60-81-359': [359],\n",
              " '163-60-81-360': [360],\n",
              " '163-60-81-361': [361],\n",
              " '163-60-81-362': [362],\n",
              " '163-60-81-363': [363],\n",
              " '163-60-81-364': [364],\n",
              " '163-60-81-365': [365],\n",
              " '163-60-81-366': [366],\n",
              " '163-60-81-367': [367],\n",
              " '163-60-81-368': [368],\n",
              " '163-60-81-369': [369],\n",
              " '163-60-81-370': [370],\n",
              " '163-60-81-371': [371],\n",
              " '163-60-81-372': [372],\n",
              " '163-60-81-373': [373],\n",
              " '163-60-81-374': [374],\n",
              " '163-60-81-375': [375],\n",
              " '163-60-81-376': [376],\n",
              " '163-60-81-377': [377],\n",
              " '163-60-81-378': [378],\n",
              " '163-60-81-379': [379],\n",
              " '163-60-81-380': [380],\n",
              " '163-60-81-381': [381],\n",
              " '163-60-81-382': [382],\n",
              " '163-60-81-383': [383],\n",
              " '163-60-81-384': [384],\n",
              " '163-60-81-385': [385],\n",
              " '163-60-81-386': [386],\n",
              " '163-60-81-387': [387],\n",
              " '163-60-81-388': [388],\n",
              " '163-60-81-389': [389],\n",
              " '163-60-81-390': [390],\n",
              " '163-60-81-391': [391],\n",
              " '163-60-81-392': [392],\n",
              " '163-60-81-393': [393],\n",
              " '163-60-81-394': [394],\n",
              " '163-60-81-395': [395],\n",
              " '163-60-81-396': [396],\n",
              " '163-60-81-397': [397],\n",
              " '163-60-81-398': [398],\n",
              " '163-60-81-399': [399],\n",
              " '163-60-81-400': [400],\n",
              " '163-60-81-401': [401],\n",
              " '163-60-81-402': [402],\n",
              " '163-60-81-403': [403],\n",
              " '163-60-81-404': [404],\n",
              " '163-60-81-405': [405],\n",
              " '163-60-81-406': [406],\n",
              " '163-60-81-407': [407],\n",
              " '163-60-81-408': [408],\n",
              " '163-60-81-409': [409],\n",
              " '163-60-81-410': [410],\n",
              " '163-60-81-411': [411],\n",
              " '163-60-81-412': [412],\n",
              " '163-60-81-413': [413],\n",
              " '163-60-81-414': [414],\n",
              " '163-60-81-415': [415],\n",
              " '163-60-81-416': [416],\n",
              " '163-60-81-417': [417],\n",
              " '163-60-81-418': [418],\n",
              " '163-60-81-419': [419],\n",
              " '163-60-81-420': [420],\n",
              " '163-60-81-421': [421],\n",
              " '163-60-81-422': [422],\n",
              " '163-60-81-423': [423],\n",
              " '163-60-81-424': [424],\n",
              " '163-60-81-425': [425],\n",
              " '163-60-81-426': [426],\n",
              " '163-60-81-427': [427],\n",
              " '163-60-81-428': [428],\n",
              " '163-60-81-429': [429],\n",
              " '163-60-81-430': [430],\n",
              " '163-60-81-431': [431],\n",
              " '163-60-81-432': [432],\n",
              " '163-60-81-433': [433],\n",
              " '163-60-81-434': [434],\n",
              " '163-60-81-435': [435],\n",
              " '163-60-81-436': [436],\n",
              " '163-60-81-437': [437],\n",
              " '163-60-81-438': [438],\n",
              " '163-60-81-439': [439],\n",
              " '163-60-81-440': [440],\n",
              " '163-60-81-441': [441],\n",
              " '163-60-81-442': [442],\n",
              " '163-60-81-443': [443],\n",
              " '163-60-81-444': [444],\n",
              " '163-60-81-445': [445],\n",
              " '163-60-81-446': [446],\n",
              " '163-60-81-447': [447],\n",
              " '163-60-81-448': [448],\n",
              " '163-60-81-449': [449],\n",
              " '163-60-81-450': [450],\n",
              " '163-60-81-451': [451],\n",
              " '163-60-81-452': [452],\n",
              " '163-60-81-453': [453],\n",
              " '163-60-81-454': [454],\n",
              " '163-60-81-455': [455],\n",
              " '163-60-81-456': [456],\n",
              " '163-60-81-457': [457],\n",
              " '163-60-81-458': [458],\n",
              " '163-60-81-459': [459],\n",
              " '163-60-81-460': [460],\n",
              " '163-60-81-461': [461],\n",
              " '163-60-81-462': [462],\n",
              " '163-60-81-463': [463],\n",
              " '163-60-81-464': [464],\n",
              " '163-60-81-465': [465],\n",
              " '163-60-81-466': [466],\n",
              " '163-60-81-467': [467],\n",
              " '163-60-81-468': [468],\n",
              " '163-60-81-469': [469],\n",
              " '163-60-81-470': [470],\n",
              " '163-60-81-471': [471],\n",
              " '163-60-81-472': [472],\n",
              " '163-60-81-473': [473],\n",
              " '163-60-81-474': [474],\n",
              " '163-60-81-475': [475],\n",
              " '163-60-81-476': [476],\n",
              " '163-60-81-477': [477],\n",
              " '163-60-81-478': [478],\n",
              " '163-60-81-479': [479],\n",
              " '163-60-81-480': [480],\n",
              " '163-60-81-481': [481],\n",
              " '163-60-81-482': [482],\n",
              " '163-60-81-483': [483],\n",
              " '163-60-81-484': [484],\n",
              " '163-60-81-485': [485],\n",
              " '163-60-81-486': [486],\n",
              " '163-60-81-487': [487],\n",
              " '163-60-81-488': [488],\n",
              " '163-60-81-489': [489],\n",
              " '163-60-81-490': [490],\n",
              " '163-60-81-491': [491],\n",
              " '163-60-81-492': [492],\n",
              " '163-60-81-493': [493],\n",
              " '163-60-81-494': [494],\n",
              " '163-60-81-495': [495],\n",
              " '163-60-81-496': [496],\n",
              " '163-60-81-497': [497],\n",
              " '163-60-81-498': [498],\n",
              " '163-60-81-499': [499],\n",
              " '163-60-81-500': [500],\n",
              " '163-60-81-501': [501],\n",
              " '163-60-81-502': [502],\n",
              " '163-60-81-503': [503],\n",
              " '163-60-81-504': [504],\n",
              " '163-60-81-505': [505],\n",
              " '163-60-81-506': [506],\n",
              " '163-60-81-507': [507],\n",
              " '163-60-81-508': [508],\n",
              " '163-60-81-509': [509],\n",
              " '163-60-81-510': [510],\n",
              " '163-60-81-511': [511],\n",
              " '163-60-81-512': [512],\n",
              " '163-60-81-513': [513],\n",
              " '163-60-81-514': [514],\n",
              " '163-60-81-515': [515],\n",
              " '163-60-81-516': [516],\n",
              " '163-60-81-517': [517],\n",
              " '163-60-81-518': [518],\n",
              " '163-60-81-519': [519],\n",
              " '163-60-81-520': [520],\n",
              " '163-60-81-521': [521],\n",
              " '163-60-81-522': [522],\n",
              " '163-60-81-523': [523],\n",
              " '163-60-81-524': [524],\n",
              " '163-60-81-525': [525],\n",
              " '163-60-81-526': [526],\n",
              " '163-60-81-527': [527],\n",
              " '163-60-81-528': [528],\n",
              " '163-60-81-529': [529],\n",
              " '163-60-81-530': [530],\n",
              " '163-60-81-531': [531],\n",
              " '163-60-81-532': [532],\n",
              " '163-60-81-533': [533],\n",
              " '163-60-81-534': [534],\n",
              " '163-60-81-535': [535],\n",
              " '163-60-81-536': [536],\n",
              " '163-60-81-537': [537],\n",
              " '163-60-81-538': [538],\n",
              " '163-60-81-539': [539],\n",
              " '163-60-81-540': [540],\n",
              " '163-60-81-541': [541],\n",
              " '163-60-81-542': [542],\n",
              " '163-60-81-543': [543],\n",
              " '163-60-81-544': [544],\n",
              " '163-60-81-545': [545],\n",
              " '163-60-81-546': [546],\n",
              " '163-60-81-547': [547],\n",
              " '163-60-81-548': [548],\n",
              " '163-60-81-549': [549],\n",
              " '163-60-81-550': [550],\n",
              " '163-60-81-551': [551],\n",
              " '163-60-81-552': [552],\n",
              " '163-60-81-553': [553],\n",
              " '163-60-81-554': [554],\n",
              " '163-60-81-555': [555],\n",
              " '163-60-81-556': [556],\n",
              " '163-60-81-557': [557],\n",
              " '163-60-81-558': [558],\n",
              " '163-60-81-559': [559],\n",
              " '163-60-81-560': [560],\n",
              " '163-60-81-561': [561],\n",
              " '163-60-81-562': [562],\n",
              " '163-60-81-563': [563],\n",
              " '163-60-81-564': [564],\n",
              " '163-60-81-565': [565],\n",
              " '163-60-81-566': [566],\n",
              " '163-60-81-567': [567],\n",
              " '163-60-81-568': [568],\n",
              " '163-60-81-569': [569],\n",
              " '163-60-81-570': [570],\n",
              " '163-60-81-571': [571],\n",
              " '163-60-81-572': [572],\n",
              " '163-60-81-573': [573],\n",
              " '163-60-81-574': [574],\n",
              " '163-60-81-575': [575],\n",
              " '163-60-81-576': [576],\n",
              " '163-60-81-577': [577],\n",
              " '163-60-81-578': [578],\n",
              " '163-60-81-579': [579],\n",
              " '163-60-81-580': [580],\n",
              " '163-60-81-581': [581],\n",
              " '163-60-81-582': [582],\n",
              " '163-60-81-583': [583],\n",
              " '163-60-81-584': [584],\n",
              " '163-60-81-585': [585],\n",
              " '163-60-81-586': [586],\n",
              " '163-60-81-587': [587],\n",
              " '163-60-81-588': [588],\n",
              " '163-60-81-589': [589],\n",
              " '163-60-81-590': [590],\n",
              " '163-60-81-591': [591],\n",
              " '163-60-81-592': [592],\n",
              " '163-60-81-593': [593],\n",
              " '163-60-81-594': [594],\n",
              " '163-60-81-595': [595],\n",
              " '163-60-81-596': [596],\n",
              " '163-60-81-597': [597],\n",
              " '163-60-81-598': [598],\n",
              " '163-60-81-599': [599],\n",
              " '163-60-81-600': [600],\n",
              " '163-60-81-601': [601],\n",
              " '163-60-81-602': [602],\n",
              " '163-60-81-603': [603],\n",
              " '163-60-81-604': [604],\n",
              " '163-60-81-605': [605],\n",
              " '163-60-81-606': [606],\n",
              " '163-60-81-607': [607],\n",
              " '163-60-81-608': [608],\n",
              " '163-60-81-609': [609],\n",
              " '163-60-81-610': [610],\n",
              " '163-60-81-611': [611],\n",
              " '163-60-81-612': [612],\n",
              " '163-60-81-613': [613],\n",
              " '163-60-81-614': [614],\n",
              " '163-60-81-615': [615],\n",
              " '163-60-81-616': [616],\n",
              " '163-60-81-617': [617],\n",
              " '163-60-81-618': [618],\n",
              " '163-60-81-619': [619],\n",
              " '163-60-81-620': [620],\n",
              " '163-60-81-621': [621],\n",
              " '163-60-81-622': [622],\n",
              " '163-60-81-623': [623],\n",
              " '163-60-81-624': [624],\n",
              " '163-60-81-625': [625],\n",
              " '163-60-81-626': [626],\n",
              " '163-60-81-627': [627],\n",
              " '163-60-81-628': [628],\n",
              " '163-60-81-629': [629],\n",
              " '163-60-81-630': [630],\n",
              " '163-60-81-631': [631],\n",
              " '163-60-81-632': [632],\n",
              " '163-60-81-633': [633],\n",
              " '163-60-81-634': [634],\n",
              " '163-60-81-635': [635],\n",
              " '163-60-81-636': [636],\n",
              " '163-60-81-637': [637],\n",
              " '163-60-81-638': [638],\n",
              " '163-60-81-639': [639],\n",
              " '163-60-81-640': [640],\n",
              " '163-60-81-641': [641],\n",
              " '163-60-81-642': [642],\n",
              " '163-60-81-643': [643],\n",
              " '163-60-81-644': [644],\n",
              " '163-60-81-645': [645],\n",
              " '163-60-81-646': [646],\n",
              " '163-60-81-647': [647],\n",
              " '163-60-81-648': [648],\n",
              " '163-60-81-649': [649],\n",
              " '163-60-81-650': [650],\n",
              " '163-60-81-651': [651],\n",
              " '163-60-81-652': [652],\n",
              " '163-60-81-653': [653],\n",
              " '163-60-81-654': [654],\n",
              " '163-60-81-655': [655],\n",
              " '163-60-81-656': [656],\n",
              " '163-60-81-657': [657],\n",
              " '163-60-81-658': [658],\n",
              " '163-60-81-659': [659],\n",
              " '163-60-81-660': [660],\n",
              " '163-60-81-661': [661],\n",
              " '163-60-81-662': [662],\n",
              " '163-60-81-663': [663],\n",
              " '163-60-81-664': [664],\n",
              " '163-60-81-665': [665],\n",
              " '163-60-81-666': [666],\n",
              " '163-60-81-667': [667],\n",
              " '163-60-81-668': [668],\n",
              " '163-60-81-669': [669],\n",
              " '163-60-81-670': [670],\n",
              " '163-60-81-671': [671],\n",
              " '163-60-81-672': [672],\n",
              " '163-60-81-673': [673],\n",
              " '163-60-81-674': [674],\n",
              " '163-60-81-675': [675],\n",
              " '163-60-81-676': [676],\n",
              " '163-60-81-677': [677],\n",
              " '163-60-81-678': [678],\n",
              " '163-60-81-679': [679],\n",
              " '163-60-81-680': [680],\n",
              " '163-60-81-681': [681],\n",
              " '163-60-81-682': [682],\n",
              " '163-60-81-683': [683],\n",
              " '163-60-81-684': [684],\n",
              " '163-60-81-685': [685],\n",
              " '163-60-81-686': [686],\n",
              " '163-60-81-687': [687],\n",
              " '163-60-81-688': [688],\n",
              " '163-60-81-689': [689],\n",
              " '163-60-81-690': [690],\n",
              " '163-60-81-691': [691],\n",
              " '163-60-81-692': [692],\n",
              " '163-60-81-693': [693],\n",
              " '163-60-81-694': [694],\n",
              " '163-60-81-695': [695],\n",
              " '163-60-81-696': [696],\n",
              " '163-60-81-697': [697],\n",
              " '163-60-81-698': [698],\n",
              " '163-60-81-699': [699],\n",
              " '163-60-81-700': [700],\n",
              " '163-60-81-701': [701],\n",
              " '163-60-81-702': [702],\n",
              " '163-60-81-703': [703],\n",
              " '163-60-81-704': [704],\n",
              " '163-60-81-705': [705],\n",
              " '163-60-81-706': [706],\n",
              " '163-60-81-707': [707],\n",
              " '163-60-81-708': [708],\n",
              " '163-60-81-709': [709],\n",
              " '163-60-81-710': [710],\n",
              " '163-60-81-711': [711],\n",
              " '163-60-81-712': [712],\n",
              " '163-60-81-713': [713],\n",
              " '163-60-81-714': [714],\n",
              " '163-60-81-715': [715],\n",
              " '163-60-81-716': [716],\n",
              " '163-60-81-717': [717],\n",
              " '163-60-81-718': [718],\n",
              " '163-60-81-719': [719],\n",
              " '163-60-81-720': [720],\n",
              " '163-60-81-721': [721],\n",
              " '163-60-81-722': [722],\n",
              " '163-60-81-723': [723],\n",
              " '163-60-81-724': [724],\n",
              " '163-60-81-725': [725],\n",
              " '163-60-81-726': [726],\n",
              " '163-60-81-727': [727],\n",
              " '163-60-81-728': [728],\n",
              " '163-60-81-729': [729],\n",
              " '163-60-81-730': [730],\n",
              " '163-60-81-731': [731],\n",
              " '163-60-81-732': [732],\n",
              " '163-60-81-733': [733],\n",
              " '163-60-81-734': [734],\n",
              " '163-60-81-735': [735],\n",
              " '163-60-81-736': [736],\n",
              " '163-60-81-737': [737],\n",
              " '163-60-81-738': [738],\n",
              " '163-60-81-739': [739],\n",
              " '163-60-81-740': [740],\n",
              " '163-60-81-741': [741],\n",
              " '163-60-81-742': [742],\n",
              " '163-60-81-743': [743],\n",
              " '163-60-81-744': [744],\n",
              " '163-60-81-745': [745],\n",
              " '163-60-81-746': [746],\n",
              " '163-60-81-747': [747],\n",
              " '163-60-81-748': [748],\n",
              " '163-60-81-749': [749],\n",
              " '163-60-81-750': [750],\n",
              " '163-60-81-751': [751],\n",
              " '163-60-81-752': [752],\n",
              " '163-60-81-753': [753],\n",
              " '163-60-81-754': [754],\n",
              " '163-60-81-755': [755],\n",
              " '163-60-81-756': [756],\n",
              " '163-60-81-757': [757],\n",
              " '163-60-81-758': [758],\n",
              " '163-60-81-759': [759],\n",
              " '163-60-81-760': [760],\n",
              " '163-60-81-761': [761],\n",
              " '163-60-81-762': [762],\n",
              " '163-60-81-763': [763],\n",
              " '163-60-81-764': [764],\n",
              " '163-60-81-765': [765],\n",
              " '163-60-81-766': [766],\n",
              " '163-60-81-767': [767],\n",
              " '163-60-81-768': [768],\n",
              " '163-60-81-769': [769],\n",
              " '163-60-81-770': [770],\n",
              " '163-60-81-771': [771],\n",
              " '163-60-81-772': [772],\n",
              " '163-60-81-773': [773],\n",
              " '163-60-81-774': [774],\n",
              " '163-60-81-775': [775],\n",
              " '163-60-81-776': [776],\n",
              " '163-60-81-777': [777],\n",
              " '163-60-81-778': [778],\n",
              " '163-60-81-779': [779],\n",
              " '163-60-81-780': [780],\n",
              " '163-60-81-781': [781],\n",
              " '163-60-81-782': [782],\n",
              " '163-60-81-783': [783],\n",
              " '163-60-81-784': [784],\n",
              " '163-60-81-785': [785],\n",
              " '163-60-81-786': [786],\n",
              " '163-60-81-787': [787],\n",
              " '163-60-81-788': [788],\n",
              " '163-60-81-789': [789],\n",
              " '163-60-81-790': [790],\n",
              " '163-60-81-791': [791],\n",
              " '163-60-81-792': [792],\n",
              " '163-60-81-793': [793],\n",
              " '163-60-81-794': [794],\n",
              " '163-60-81-795': [795],\n",
              " '163-60-81-796': [796],\n",
              " '163-60-81-797': [797],\n",
              " '163-60-81-798': [798],\n",
              " '163-60-81-799': [799],\n",
              " '163-60-81-800': [800],\n",
              " '163-60-81-801': [801],\n",
              " '163-60-81-802': [802],\n",
              " '163-60-81-803': [803],\n",
              " '163-60-81-804': [804],\n",
              " '163-60-81-805': [805],\n",
              " '163-60-81-806': [806],\n",
              " '163-60-81-807': [807],\n",
              " '163-60-81-808': [808],\n",
              " '163-60-81-809': [809],\n",
              " '163-60-81-810': [810],\n",
              " '163-60-81-811': [811],\n",
              " '163-60-81-812': [812],\n",
              " '163-60-81-813': [813],\n",
              " '163-60-81-814': [814],\n",
              " '163-60-81-815': [815],\n",
              " '163-60-81-816': [816],\n",
              " '163-60-81-817': [817],\n",
              " '163-60-81-818': [818],\n",
              " '163-60-81-819': [819],\n",
              " '163-60-81-820': [820],\n",
              " '163-60-81-821': [821],\n",
              " '163-60-81-822': [822],\n",
              " '163-60-81-823': [823],\n",
              " '163-60-81-824': [824],\n",
              " '163-60-81-825': [825],\n",
              " '163-60-81-826': [826],\n",
              " '163-60-81-827': [827],\n",
              " '163-60-81-828': [828],\n",
              " '163-60-81-829': [829],\n",
              " '163-60-81-830': [830],\n",
              " '163-60-81-831': [831],\n",
              " '163-60-81-832': [832],\n",
              " '163-60-81-833': [833],\n",
              " '163-60-81-834': [834],\n",
              " '163-60-81-835': [835],\n",
              " '163-60-81-836': [836],\n",
              " '163-60-81-837': [837],\n",
              " '163-60-81-838': [838],\n",
              " '163-60-81-839': [839],\n",
              " '163-60-81-840': [840],\n",
              " '163-60-81-841': [841],\n",
              " '163-60-81-842': [842],\n",
              " '163-60-81-843': [843],\n",
              " '163-60-81-844': [844],\n",
              " '163-60-81-845': [845],\n",
              " '163-60-81-846': [846],\n",
              " '163-60-81-847': [847],\n",
              " '163-60-81-848': [848],\n",
              " '163-60-81-849': [849],\n",
              " '163-60-81-850': [850],\n",
              " '163-60-81-851': [851],\n",
              " '163-60-81-852': [852],\n",
              " '163-60-81-853': [853],\n",
              " '163-60-81-854': [854],\n",
              " '163-60-81-855': [855],\n",
              " '163-60-81-856': [856],\n",
              " '163-60-81-857': [857],\n",
              " '163-60-81-858': [858],\n",
              " '163-60-81-859': [859],\n",
              " '163-60-81-860': [860],\n",
              " '163-60-81-861': [861],\n",
              " '163-60-81-862': [862],\n",
              " '163-60-81-863': [863],\n",
              " '163-60-81-864': [864],\n",
              " '163-60-81-865': [865],\n",
              " '163-60-81-866': [866],\n",
              " '163-60-81-867': [867],\n",
              " '163-60-81-868': [868],\n",
              " '163-60-81-869': [869],\n",
              " '163-60-81-870': [870],\n",
              " '163-60-81-871': [871],\n",
              " '163-60-81-872': [872],\n",
              " '163-60-81-873': [873],\n",
              " '163-60-81-874': [874],\n",
              " '163-60-81-875': [875],\n",
              " '163-60-81-876': [876],\n",
              " '163-60-81-877': [877],\n",
              " '163-60-81-878': [878],\n",
              " '163-60-81-879': [879],\n",
              " '163-60-81-880': [880],\n",
              " '163-60-81-881': [881],\n",
              " '163-60-81-882': [882],\n",
              " '163-60-81-883': [883],\n",
              " '163-60-81-884': [884],\n",
              " '163-60-81-885': [885],\n",
              " '163-60-81-886': [886],\n",
              " '163-60-81-887': [887],\n",
              " '163-60-81-888': [888],\n",
              " '163-60-81-889': [889],\n",
              " '163-60-81-890': [890],\n",
              " '163-60-81-891': [891],\n",
              " '163-60-81-892': [892],\n",
              " '163-60-81-893': [893],\n",
              " '163-60-81-894': [894],\n",
              " '163-60-81-895': [895],\n",
              " '163-60-81-896': [896],\n",
              " '163-60-81-897': [897],\n",
              " '163-60-81-898': [898],\n",
              " '163-60-81-899': [899],\n",
              " '163-60-81-900': [900],\n",
              " '163-60-81-901': [901],\n",
              " '163-60-81-902': [902],\n",
              " '163-60-81-903': [903],\n",
              " '163-60-81-904': [904],\n",
              " '163-60-81-905': [905],\n",
              " '163-60-81-906': [906],\n",
              " '163-60-81-907': [907],\n",
              " '163-60-81-908': [908],\n",
              " '163-60-81-909': [909],\n",
              " '163-60-81-910': [910],\n",
              " '163-60-81-911': [911],\n",
              " '163-60-81-912': [912],\n",
              " '163-60-81-913': [913],\n",
              " '163-60-81-914': [914],\n",
              " '163-60-81-915': [915],\n",
              " '163-60-81-916': [916],\n",
              " '163-60-81-917': [917],\n",
              " '163-60-81-918': [918],\n",
              " '163-60-81-919': [919],\n",
              " '163-60-81-920': [920],\n",
              " '163-60-81-921': [921],\n",
              " '163-60-81-922': [922],\n",
              " '163-60-81-923': [923],\n",
              " '163-60-81-924': [924],\n",
              " '163-60-81-925': [925],\n",
              " '163-60-81-926': [926],\n",
              " '163-60-81-927': [927],\n",
              " '163-60-81-928': [928],\n",
              " '163-60-81-929': [929],\n",
              " '163-60-81-930': [930],\n",
              " '163-60-81-931': [931],\n",
              " '163-60-81-932': [932],\n",
              " '163-60-81-933': [933],\n",
              " '163-60-81-934': [934],\n",
              " '163-60-81-935': [935],\n",
              " '163-60-81-936': [936],\n",
              " '163-60-81-937': [937],\n",
              " '163-60-81-938': [938],\n",
              " '163-60-81-939': [939],\n",
              " '163-60-81-940': [940],\n",
              " '163-60-81-941': [941],\n",
              " '163-60-81-942': [942],\n",
              " '163-60-81-943': [943],\n",
              " '163-60-81-944': [944],\n",
              " '163-60-81-945': [945],\n",
              " '163-60-81-946': [946],\n",
              " '163-60-81-947': [947],\n",
              " '163-60-81-948': [948],\n",
              " '163-60-81-949': [949],\n",
              " '163-60-81-950': [950],\n",
              " '163-60-81-951': [951],\n",
              " '163-60-81-952': [952],\n",
              " '163-60-81-953': [953],\n",
              " '163-60-81-954': [954],\n",
              " '163-60-81-955': [955],\n",
              " '163-60-81-956': [956],\n",
              " '163-60-81-957': [957],\n",
              " '163-60-81-958': [958],\n",
              " '163-60-81-959': [959],\n",
              " '163-60-81-960': [960],\n",
              " '163-60-81-961': [961],\n",
              " '163-60-81-962': [962],\n",
              " '163-60-81-963': [963],\n",
              " '163-60-81-964': [964],\n",
              " '163-60-81-965': [965],\n",
              " '163-60-81-966': [966],\n",
              " '163-60-81-967': [967],\n",
              " '163-60-81-968': [968],\n",
              " '163-60-81-969': [969],\n",
              " '163-60-81-970': [970],\n",
              " '163-60-81-971': [971],\n",
              " '163-60-81-972': [972],\n",
              " '163-60-81-973': [973],\n",
              " '163-60-81-974': [974],\n",
              " '163-60-81-975': [975],\n",
              " '163-60-81-976': [976],\n",
              " '163-60-81-977': [977],\n",
              " '163-60-81-978': [978],\n",
              " '163-60-81-979': [979],\n",
              " '163-60-81-980': [980],\n",
              " '163-60-81-981': [981],\n",
              " '163-60-81-982': [982],\n",
              " '163-60-81-983': [983],\n",
              " '163-60-81-984': [984],\n",
              " '163-60-81-985': [985],\n",
              " '163-60-81-986': [986],\n",
              " '163-60-81-987': [987],\n",
              " '163-60-81-988': [988],\n",
              " '163-60-81-989': [989],\n",
              " '163-60-81-990': [990],\n",
              " '163-60-81-991': [991],\n",
              " '163-60-81-992': [992],\n",
              " '163-60-81-993': [993],\n",
              " '163-60-81-994': [994],\n",
              " '163-60-81-995': [995],\n",
              " '163-60-81-996': [996],\n",
              " '163-60-81-997': [997],\n",
              " '163-60-81-998': [998],\n",
              " '163-60-81-999': [999],\n",
              " ...}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sid_to_items"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYnxcsuhJHeL"
      },
      "source": [
        "## 6. Evaluation\n",
        "\n",
        "Measure SID@K, Invalid-ID rate, and qualitative examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "id5dczimJI8y",
        "outputId": "566ef130-62b4-47e3-ec04-088ea362ee07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on 1000 examples...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000/1000 [04:25<00:00,  3.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluation Results ===\n",
            "Examples evaluated: 1000\n",
            "Invalid-ID@1: 100.00%\n",
            "N/A\n",
            "Unique SIDs generated: 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate on validation set\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load validation dialogs\n",
        "eval_size = min(1000, len(valid_dialogs))\n",
        "eval_dialogs = valid_dialogs[:eval_size]\n",
        "\n",
        "print(f\"Evaluating on {eval_size} examples...\")\n",
        "\n",
        "# Metrics\n",
        "invalid_count = 0\n",
        "sid_hits = []\n",
        "generated_sids = []\n",
        "\n",
        "for dialog in tqdm(eval_dialogs):\n",
        "    # Extract ground truth\n",
        "    assistant_msg = dialog['messages'][2]['content']\n",
        "    # Parse SID tokens from assistant message\n",
        "    tokens = assistant_msg.split()\n",
        "    if len(tokens) != 4:\n",
        "        continue\n",
        "\n",
        "    # Extract codes from tokens\n",
        "    try:\n",
        "        gt_codes = []\n",
        "        for i, token in enumerate(tokens):\n",
        "            code_num = int(token.split('_')[1].rstrip('>'))\n",
        "            level_offset = i * 256\n",
        "            code = code_num - level_offset\n",
        "            gt_codes.append(code)\n",
        "        gt_sid = tuple(gt_codes)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "    # Extract history from user message\n",
        "    user_msg = dialog['messages'][1]['content']\n",
        "    history_lines = user_msg.split('\\n')[1:-1]  # Skip \"History:\" and \"Recommend next:\"\n",
        "\n",
        "    history_sids = []\n",
        "    for line in history_lines:\n",
        "        tokens = line.split()\n",
        "        if len(tokens) != 4:\n",
        "            continue\n",
        "        try:\n",
        "            codes = []\n",
        "            for i, token in enumerate(tokens):\n",
        "                code_num = int(token.split('_')[1].rstrip('>'))\n",
        "                level_offset = i * 256\n",
        "                code = code_num - level_offset\n",
        "                codes.append(code)\n",
        "            history_sids.append(tuple(codes))\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    if not history_sids:\n",
        "        continue\n",
        "\n",
        "    # Generate SID\n",
        "    try:\n",
        "        generated_sid = recommender.generate_sid(history_sids=history_sids)\n",
        "\n",
        "        if generated_sid is None:\n",
        "            invalid_count += 1\n",
        "            continue\n",
        "\n",
        "        generated_sids.append(generated_sid)\n",
        "\n",
        "        # Check if valid (exists in catalog)\n",
        "        sid_key = ','.join(map(str, generated_sid))\n",
        "        if sid_key not in sid_to_items:\n",
        "            invalid_count += 1\n",
        "            continue\n",
        "\n",
        "        # Check if matches ground truth\n",
        "        sid_hits.append(1 if generated_sid == gt_sid else 0)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        invalid_count += 1\n",
        "        continue\n",
        "\n",
        "print(\"\\n=== Evaluation Results ===\")\n",
        "print(f\"Examples evaluated: {len(sid_hits) + invalid_count}\")\n",
        "print(f\"Invalid-ID@1: {invalid_count / (len(sid_hits) + invalid_count) * 100:.2f}%\")\n",
        "print(f\"SID@1 (exact match): {np.mean(sid_hits) * 100:.2f}%\" if sid_hits else \"N/A\")\n",
        "print(f\"Unique SIDs generated: {len(set(generated_sids))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ALyV8Ty9JKx6",
        "outputId": "6e3649ae-810f-4193-dc92-4f46ae363108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Qualitative Examples ===\n",
            "\n",
            "[Example 1]\n",
            "History: [(91, 54, 165, 0), (146, 204, 254, 0), (225, 239, 96, 0)]\n",
            "Generated SID: (163, 60, 81, 166)\n",
            "Mapped to 0 items\n",
            "\n",
            "[Example 2]\n",
            "History: [(229, 236, 102, 0), (225, 212, 226, 1)]\n",
            "Generated SID: (163, 60, 81, 166)\n",
            "Mapped to 0 items\n",
            "\n",
            "[Example 3]\n",
            "History: [(94, 233, 248, 0), (180, 191, 245, 0), (89, 141, 245, 0)]\n",
            "Generated SID: (163, 60, 81, 166)\n",
            "Mapped to 0 items\n"
          ]
        }
      ],
      "source": [
        "# Qualitative examples\n",
        "print(\"\\n=== Qualitative Examples ===\")\n",
        "\n",
        "test_histories = [\n",
        "    [(91, 54, 165, 0), (146, 204, 254, 0), (225, 239, 96, 0)],\n",
        "    [(229, 236, 102, 0), (225, 212, 226, 1)],\n",
        "    [(94, 233, 248, 0), (180, 191, 245, 0), (89, 141, 245, 0)],\n",
        "]\n",
        "\n",
        "for i, history in enumerate(test_histories, 1):\n",
        "    print(f\"\\n[Example {i}]\")\n",
        "    print(f\"History: {history}\")\n",
        "\n",
        "    generated_sid = recommender.generate_sid(history_sids=history)\n",
        "    print(f\"Generated SID: {generated_sid}\")\n",
        "\n",
        "    if generated_sid:\n",
        "        sid_key = ','.join(map(str, generated_sid))\n",
        "        items = sid_to_items.get(sid_key, [])\n",
        "        print(f\"Mapped to {len(items)} items\")\n",
        "        if items:\n",
        "            print(f\"  Top item: {items[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QopE5Nx3JMLF"
      },
      "source": [
        "## 7. Acceptance Criteria\n",
        "\n",
        "Check if all acceptance criteria pass:\n",
        "\n",
        "1. \u2705 Stage A completes and new tokens are learned\n",
        "2. \u2705 Stage B completes with validation loss decreasing\n",
        "3. \u2705 Invalid-ID@1 = 0% (or very close)\n",
        "4. \u2705 SID@10 \u2265 baseline\n",
        "5. \u2705 NL prompts produce valid SIDs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2rFywwK7JNWa",
        "outputId": "c1dbd606-0df6-4871-a3d2-7f0f1e3cc30b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ACCEPTANCE CRITERIA\n",
            "============================================================\n",
            "\n",
            "[1] Stage A: Vocabulary Extension\n",
            "  Status: \u274c FAIL\n",
            "\n",
            "[2] Stage B: Full Fine-tuning\n",
            "  Status: \u274c FAIL\n",
            "\n",
            "[3] Invalid-ID Rate\n",
            "  Invalid-ID@1: 100.00%\n",
            "  Status: \u274c FAIL (target: <5%)\n",
            "\n",
            "[4] SID@1 Exact Match\n",
            "  SID@1: 0.00%\n",
            "  Status: \u274c FAIL (target: >0%)\n",
            "\n",
            "[5] Qualitative Examples\n",
            "  Status: \u2705 PASS (see examples above)\n",
            "\n",
            "============================================================\n",
            "OVERALL: \u274c SOME CHECKS FAILED\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ACCEPTANCE CRITERIA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n[1] Stage A: Vocabulary Extension\")\n",
        "vocab_stage_path = f'{LLM_DIR}/qwen3_vocab_stage/pytorch_model.bin'\n",
        "print(f\"  Status: {'\u2705 PASS' if os.path.exists(vocab_stage_path) else '\u274c FAIL'}\")\n",
        "\n",
        "print(\"\\n[2] Stage B: Full Fine-tuning\")\n",
        "full_stage_path = f'{LLM_DIR}/qwen3_full_stage/pytorch_model.bin'\n",
        "print(f\"  Status: {'\u2705 PASS' if os.path.exists(full_stage_path) else '\u274c FAIL'}\")\n",
        "\n",
        "print(\"\\n[3] Invalid-ID Rate\")\n",
        "invalid_rate = invalid_count / (len(sid_hits) + invalid_count) * 100 if (len(sid_hits) + invalid_count) > 0 else 100\n",
        "print(f\"  Invalid-ID@1: {invalid_rate:.2f}%\")\n",
        "print(f\"  Status: {'\u2705 PASS' if invalid_rate < 5.0 else '\u274c FAIL'} (target: <5%)\")\n",
        "\n",
        "print(\"\\n[4] SID@1 Exact Match\")\n",
        "sid_acc = np.mean(sid_hits) * 100 if sid_hits else 0\n",
        "print(f\"  SID@1: {sid_acc:.2f}%\")\n",
        "print(f\"  Status: {'\u2705 PASS' if sid_acc > 0 else '\u274c FAIL'} (target: >0%)\")\n",
        "\n",
        "print(\"\\n[5] Qualitative Examples\")\n",
        "print(f\"  Status: \u2705 PASS (see examples above)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "all_pass = (\n",
        "    os.path.exists(vocab_stage_path) and\n",
        "    os.path.exists(full_stage_path) and\n",
        "    invalid_rate < 5.0 and\n",
        "    sid_acc > 0\n",
        ")\n",
        "print(f\"OVERALL: {'\ud83c\udf89 ALL CHECKS PASSED!' if all_pass else '\u274c SOME CHECKS FAILED'}\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyMaJTvJ9lALBFd9NH7S/0VD",
      "gpuType": "L4",
      "machine_shape": "hm",
      "mount_file_id": "1NA5VK4xZZGOIFC1MAvqh8mAUGzSPBNCy",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}